{"instance_id": "zylon-ai__private-gpt-10", "found_files": [["ingest.py", "constants.py", "privateGPT.py"], ["ingest.py", "constants.py", "privateGPT.py"]], "found_modules": [["privateGPT.py:main", "ingest.py:main", "ingest.py:load_documents", "ingest.py:load_single_document"], ["privateGPT.py:main", "ingest.py:main", "ingest.py:load_documents", "ingest.py:load_single_document"]], "found_entities": [["privateGPT.py:main", "ingest.py:main", "ingest.py:load_documents", "ingest.py:load_single_document"], ["privateGPT.py:main", "ingest.py:main", "ingest.py:load_documents", "ingest.py:load_single_document"]], "raw_output_loc": ["```\ningest.py\nline: 71-92\nfunction: main\n\ningest.py\nline: 61-69\nfunction: load_documents\n\ningest.py\nline: 51-58\nfunction: load_single_document\n\nconstants.py\nline: 11-15\nvariable: CHROMA_SETTINGS\n\nprivateGPT.py\nline: 20-34\nfunction: main\n```", "```\n/ingest.py\nline: 71-92\nfunction: main\n  - RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n  - LlamaCppEmbeddings(model_path=..., n_ctx=model_n_ctx)\n  - Chroma.from_documents(...).persist()\n\n/ingest.py\nline: 61-69\nfunction: load_documents\n  - sequential file discovery + list-comprehension loading (potential ingestion bottleneck)\n\n/ingest.py\nline: 51-58\nfunction: load_single_document\n  - return loader.load()[0] (only first returned Document used; impacts chunk count/behavior)\n\n/constants.py\nline: 7-15\nvariable: PERSIST_DIRECTORY, CHROMA_SETTINGS\n  - Chroma Settings(chroma_db_impl='duckdb+parquet', persist_directory=...)\n\n/privateGPT.py\nline: 20-35\nfunction: main\n  - reuses LlamaCppEmbeddings + Chroma(persist_directory=...) which must align with ingestion settings\n```"], "meta_data": {"repo": "zylon-ai/private-gpt", "base_commit": "fdb45741e521d606b028984dbc2f6ac57755bb88", "problem_statement": "Suggestions for speeding up ingestion?\n\nI presume I must be doing something wrong, as it is taking hours to ingest a 500kbyte text on an i9-12900 with 128GB.  In fact it's not even done yet.  Using models are recommended.\r\n\r\nHelp?\r\n\r\nThanks\r\n\r\nSome output:\r\n\r\nllama_print_timings:        load time =   674.34 ms\r\nllama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\r\nllama_print_timings: prompt eval time = 12526.78 ms /   152 tokens (   82.41 ms per token)\r\nllama_print_timings:        eval time =   157.46 ms /     1 runs   (  157.46 ms per run)\r\nllama_print_timings:       total time = 12715.48 ms", "patch": ""}}
{"instance_id": "zylon-ai__private-gpt-2", "found_files": [["gpt4all_j.py", "privateGPT.py", "ingest.py"], ["gpt4all_j.py", "privateGPT.py", "ingest.py"]], "found_modules": [["gpt4all_j.py:GPT4All_J", "privateGPT.py:main", "ingest.py:main"], ["privateGPT.py:main", "gpt4all_j.py:GPT4All_J", "ingest.py:main"]], "found_entities": [["gpt4all_j.py:GPT4All_J._call", "gpt4all_j.py:GPT4All_J.validate_environment", "privateGPT.py:main", "ingest.py:main"], ["privateGPT.py:main", "gpt4all_j.py:GPT4All_J._call", "gpt4all_j.py:GPT4All_J.validate_environment", "ingest.py:main"]], "raw_output_loc": ["```\ngpt4all_j.py\nline: 129-160\nclass: GPT4All_J\nfunction: GPT4All_J._call\n(note: passes `new_text_callback=` into `self.client.generate(...)` at lines ~152-155; root cause of `TypeError: generate() got an unexpected keyword argument 'new_text_callback'`)\n\ngpt4all_j.py\nline: 91-109\nclass: GPT4All_J\nfunction: GPT4All_J.validate_environment\n(note: constructs `pygpt4all.models.gpt4all_j.GPT4All_J` client; its `.generate()` signature determines whether `new_text_callback` is valid)\n\nprivateGPT.py\nline: 7-23\nfunction: main\n(note: instantiates `GPT4All_J(..., callbacks=callbacks, ...)`, which leads LangChain to call `GPT4All_J._call` with a `run_manager`, activating the streaming codepath that supplies `new_text_callback`)\n\nprivateGPT.py\nline: 1\n(note: imports `GPT4All_J` from local `gpt4all_j.py`, establishing the entry path to the failing wrapper)\n\ningest.py\nline: 6-17\nfunction: main\n(note: upstream setup for the vectorstore used by `privateGPT.py`; not the TypeError source but part of the repo\u2019s execution flow)\n```", "```\ngpt4all_j.py\nline: 129-160\nclass: GPT4All_J\nfunction: GPT4All_J._call\n(note: line 150-155 passes `new_text_callback=` into `self.client.generate(...)`; this is the direct source of `TypeError: generate() got an unexpected keyword argument 'new_text_callback'`)\n\ngpt4all_j.py\nline: 91-109\nclass: GPT4All_J\nfunction: GPT4All_J.validate_environment\n(note: imports/constructs `pygpt4all.models.gpt4all_j.GPT4All_J` client; its `generate()` signature determines which kwargs are valid)\n\nprivateGPT.py\nline: 14-16\nfunction: main\n(note: sets up callbacks and constructs `GPT4All_J(..., callbacks=callbacks, ...)`, which causes LangChain to provide a run_manager and enter the streaming branch in `_call`)\n\nprivateGPT.py\nline: 18-22\nfunction: main\n(note: `qa.run(query)` is the runtime trigger that eventually invokes `GPT4All_J._call`)\n\ningest.py\nline: 6-17\nfunction: main\n(note: builds the persisted Chroma DB used by `privateGPT.py`; part of the execution path, useful for reproducing end-to-end behavior though not the direct TypeError source)\n```"], "meta_data": {"repo": "zylon-ai/private-gpt", "base_commit": "77447e50c0b8143edcf34896af80dd58925582f9", "problem_statement": "TypeError: generate() got an unexpected keyword argument 'new_text_callback'\n\n/privateGPT/gpt4all_j.py\", line 152, in _call\r\n    text = self.client.generate(\r\nTypeError: generate() got an unexpected keyword argument 'new_text_callback'", "patch": ""}}
{"instance_id": "zylon-ai__private-gpt-1007", "found_files": [["ingest.py", "privateGPT.py", "constants.py"], ["ingest.py", "privateGPT.py", "constants.py"]], "found_modules": [["privateGPT.py:main", "ingest.py:main", "ingest.py:does_vectorstore_exist"], ["privateGPT.py:main", "ingest.py:main", "ingest.py:does_vectorstore_exist"]], "found_entities": [["privateGPT.py:main", "ingest.py:main", "ingest.py:does_vectorstore_exist"], ["privateGPT.py:main", "ingest.py:main", "ingest.py:does_vectorstore_exist"]], "raw_output_loc": ["```\ningest.py\nline: 142-163\nfunction: main\n  - chroma_client = chromadb.PersistentClient(settings=CHROMA_SETTINGS , path=persist_directory)  # AttributeError source\n  - Chroma(..., client_settings=CHROMA_SETTINGS, client=chroma_client)\n  - Chroma.from_documents(..., client_settings=CHROMA_SETTINGS, client=chroma_client)\n\nprivateGPT.py\nline: 28-34\nfunction: main\n  - chroma_client = chromadb.PersistentClient(settings=CHROMA_SETTINGS , path=persist_directory)  # same failing API usage\n  - Chroma(..., client_settings=CHROMA_SETTINGS, client=chroma_client)\n\nconstants.py\nline: 1-16\n  - from chromadb.config import Settings\n  - CHROMA_SETTINGS = Settings(persist_directory=PERSIST_DIRECTORY, anonymized_telemetry=False)\n\ningest.py\nline: 133-140\nfunction: does_vectorstore_exist\n  - Constructs Chroma(persist_directory=..., embedding_function=...) without client/client_settings (may need alignment with any fix)\n\ningest.py\nline: 28-40\n  - .env loading and persist_directory = os.environ.get('PERSIST_DIRECTORY') used by PersistentClient/path and Chroma persist_directory\n```", "```\ningest.py\nline: 142-163\nfunction: main  (direct cause: chromadb.PersistentClient(...) at ~146; client passed into Chroma constructors)\n\nprivateGPT.py\nline: 28-35\nfunction: main  (same chromadb.PersistentClient(...) usage at ~32; constructs Chroma with client)\n\nconstants.py\nline: 1-16\nvariable: CHROMA_SETTINGS  (chromadb.config.Settings usage; persist_directory configured here)\n\ningest.py\nline: 133-140\nfunction: does_vectorstore_exist  (creates Chroma without explicit client/client_settings; may need to match updated client init path)\n\ningest.py\nline: 32-40\nline: 168-169\nmodule-level env wiring + entrypoint  (persist_directory sourced from env; ingest entrypoint triggers main)\n```"], "meta_data": {"repo": "zylon-ai/private-gpt", "base_commit": "2940f987c0996fe083d1777bdc117fc28c576c08", "problem_statement": "running ingest throws attribute error module 'chromadb' has no attribute 'PersistentClient'\n\n```\r\n(privategpt-py3.11) (base) \u279c  privateGPT git:(main) \u2717 python ingest.py\r\nTraceback (most recent call last):\r\n  File \"/Volumes/Projects/privateGPT/ingest.py\", line 169, in <module>\r\n    main()\r\n  File \"/Volumes/Projects/privateGPT/ingest.py\", line 146, in main\r\n    chroma_client = chromadb.PersistentClient(settings=CHROMA_SETTINGS , path=persist_directory)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^\r\nAttributeError: module 'chromadb' has no attribute 'PersistentClient'\r\n\r\n```\r\n\r\n.env file:\r\n\r\n```\r\nPERSIST_DIRECTORY=db\r\nMODEL_TYPE=GPT4All\r\nMODEL_PATH=models/ggml-gpt4all-j-v1.3-groovy.bin\r\nEMBEDDINGS_MODEL_NAME=all-MiniLM-L6-v2\r\nMODEL_N_CTX=1000\r\nMODEL_N_BATCH=8\r\nTARGET_SOURCE_CHUNKS=4\r\n```\r\n\r\n**Environment (please complete the following information):**\r\n - OS / hardware: macOS 13.5.1\r\n - Python version 3.11.5\r\n\r\nany idea what's wrong here or how to solve it?", "patch": ""}}
{"instance_id": "zylon-ai__private-gpt-133", "found_files": [["constants.py", "ingest.py", "privateGPT.py"], ["constants.py", "ingest.py", "privateGPT.py"]], "found_modules": [["privateGPT.py:main", "ingest.py:main", "ingest.py:load_single_document"], ["privateGPT.py:main", "ingest.py:main", "ingest.py:load_single_document"]], "found_entities": [["privateGPT.py:main", "ingest.py:main", "ingest.py:load_single_document"], ["privateGPT.py:main", "ingest.py:main", "ingest.py:load_single_document"]], "raw_output_loc": ["```\nconstants.py\nline: 5-15\nvariable: PERSIST_DIRECTORY\nvariable: CHROMA_SETTINGS\n# Root cause of the reported ValidationError: Settings(persist_directory=PERSIST_DIRECTORY) where PERSIST_DIRECTORY may be None.\n\ningest.py\nline: 11-14\nimport: from constants import CHROMA_SETTINGS\n# Import-time evaluation triggers constants.py Settings validation before ingest runs.\n\ningest.py\nline: 37-43\nfunction: main\n# Reads PERSIST_DIRECTORY and other env vars; potential need to ensure persist_directory is set/non-empty before use.\n\ningest.py\nline: 56-57\nfunction: main\n# Chroma.from_documents(... persist_directory=persist_directory, client_settings=CHROMA_SETTINGS) uses both env persist_directory and constants CHROMA_SETTINGS.\n\nprivateGPT.py\nline: 11-18\nimport-time env reads: persist_directory = os.environ.get('PERSIST_DIRECTORY')\nimport: from constants import CHROMA_SETTINGS\n# Same failure mode as ingest.py: importing CHROMA_SETTINGS will validate persist_directory.\n\nprivateGPT.py\nline: 20-23\nfunction: main\n# Chroma(persist_directory=persist_directory, ..., client_settings=CHROMA_SETTINGS) depends on the same persist_directory/Settings config.\n```", "```\nconstants.py\nline: 5-15\nkey: load_dotenv()\nkey: PERSIST_DIRECTORY = os.environ.get('PERSIST_DIRECTORY')\nkey: CHROMA_SETTINGS = Settings(... persist_directory=PERSIST_DIRECTORY ...)\n(note: ValidationError is thrown here when PERSIST_DIRECTORY is None)\n\ningest.py\nline: 11-15\nkey: from constants import CHROMA_SETTINGS\nkey: load_dotenv()\n(note: import happens before ingest.py load_dotenv(), so constants.py may see env vars unset)\n\ningest.py\nline: 37-57\nfunction: main\nkey: persist_directory = os.environ.get('PERSIST_DIRECTORY')\nkey: Chroma.from_documents(... persist_directory=persist_directory, client_settings=CHROMA_SETTINGS)\n\nprivateGPT.py\nline: 9-23\nfunction: main (uses module-level config too)\nkey: load_dotenv()\nkey: persist_directory = os.environ.get('PERSIST_DIRECTORY')\nkey: from constants import CHROMA_SETTINGS\n(note: same dependency on constants.py / PERSIST_DIRECTORY)\n\nprivateGPT.py\nline: 20-23\nfunction: main\nkey: db = Chroma(persist_directory=persist_directory, ..., client_settings=CHROMA_SETTINGS)\n(secondary place affected by any change to constants.py/PERSIST_DIRECTORY handling)\n```"], "meta_data": {"repo": "zylon-ai/private-gpt", "base_commit": "5a695e9767e24778ffd725ab195bf72916e27ba5", "problem_statement": "Need help with ingest.py\n\nRunning into this error -  python ingest.py\r\n\r\n-Traceback (most recent call last):\r\n  File \"C:\\Users\\krstr\\OneDrive\\Desktop\\privategpt\\privateGPT\\privateGPT\\ingest.py\", line 11, in <module>\r\n    from constants import CHROMA_SETTINGS\r\n  File \"C:\\Users\\krstr\\OneDrive\\Desktop\\privategpt\\privateGPT\\privateGPT\\constants.py\", line 11, in <module>\r\n    CHROMA_SETTINGS = Settings(\r\n  File \"pydantic\\env_settings.py\", line 39, in pydantic.env_settings.BaseSettings.__init__\r\n  File \"pydantic\\main.py\", line 341, in pydantic.main.BaseModel.__init__\r\npydantic.error_wrappers.ValidationError: 1 validation error for Settings\r\npersist_directory\r\n  none is not an allowed value (type=type_error.none.not_allowed) -\r\n\r\nI've installed the requirements and changed the .env file and followed the readme up to this point. Seeing some people solve but not answer what fixed the above errors. Help?", "patch": ""}}
{"instance_id": "zylon-ai__private-gpt-448", "found_files": [["ingest.py", "privateGPT.py"], ["ingest.py"]], "found_modules": [["ingest.py:load_single_document", "ingest.py:load_documents", "ingest.py:process_documents", "ingest.py:main", "privateGPT.py:main", "privateGPT.py:parse_arguments", "ingest.py:MyElmLoader"], ["ingest.py:load_single_document", "ingest.py:load_documents", "ingest.py:process_documents", "ingest.py:main", "privateGPT.py:parse_arguments"]], "found_entities": [["ingest.py:load_single_document", "ingest.py:load_documents", "ingest.py:process_documents", "ingest.py:main", "privateGPT.py:main", "privateGPT.py:parse_arguments", "ingest.py:MyElmLoader.load"], ["ingest.py:load_single_document", "ingest.py:load_documents", "ingest.py:process_documents", "ingest.py:main", "privateGPT.py:parse_arguments"]], "raw_output_loc": ["```\ningest.py\nline: 84-91\nfunction: load_single_document\n  - root cause: returns loader.load()[0] (drops all but the first Document/row from CSVLoader and any multi-document loaders)\n\ningest.py\nline: 94-112\nfunction: load_documents\n  - must be updated if load_single_document returns List[Document] (extend vs append; Pool iteration variable doc->docs)\n\ningest.py\nline: 114-127\nfunction: process_documents\n  - downstream dependency: assumes load_documents returns List[Document] and passes it to split_documents()\n\ningest.py\nline: 64-81\nvariable: LOADER_MAPPING\n  - confirms \u201c.csv\u201d: (CSVLoader, {}) and other loaders that may return multiple Documents\n\ningest.py\nline: 142-160\nfunction: main\n  - ingestion pipeline entry point; downstream impact of document count change on db.add_documents()/Chroma.from_documents()\n\nprivateGPT.py\nline: 23-61\nfunction: main\n  - verification path: reads persisted Chroma DB; impacted by what ingest.py stored (useful to confirm more CSV rows become retrievable)\n```", "```\ningest.py\nline: 84-91\nfunction: load_single_document\n# BUG SOURCE: returns loader.load()[0], which truncates multi-document loaders (notably CSVLoader -> one Document per row).\n\ningest.py\nline: 94-112\nfunction: load_documents\n# NEEDS CHANGE: currently appends a single Document per file from pool results.\n# Must aggregate lists returned from load_single_document (e.g., results.extend(docs)) and update loop variable accordingly.\n\ningest.py\nline: 64-81\nvariable: LOADER_MAPPING\n# KEY CONTEXT: \".csv\": (CSVLoader, {}), so CSV ingestion relies on CSVLoader behavior and is impacted by [0] truncation.\n\ningest.py\nline: 114-127\nfunction: process_documents\n# DOWNSTREAM DEPENDENCY: consumes load_documents output and splits; will naturally ingest all rows once load_documents returns full doc list.\n\ningest.py\nline: 142-160\nfunction: main\n# ENTRYPOINT: calls process_documents and persists to Chroma; confirms execution flow where the bug manifests during ingestion.\n```"], "meta_data": {"repo": "zylon-ai/private-gpt", "base_commit": "60e6bd25eb7e54a6d62ab0a9642c09170c1729e3", "problem_statement": "ingest.py extracts only the first row from the CSV files\n\nMy suggestion for fixing the bug:\r\n\r\n1. Modify the load_single_document function as follows:\r\n\r\ndef load_single_document(file_path: str) -> List[Document]:\r\n    ext = \".\" + file_path.rsplit(\".\", 1)[-1]\r\n    if ext in LOADER_MAPPING:\r\n        loader_class, loader_args = LOADER_MAPPING[ext]\r\n        loader = loader_class(file_path, **loader_args)\r\n        return loader.load()\r\n\r\n    raise ValueError(f\"Unsupported file extension '{ext}'\")\r\n    \r\n2. Modify the load_documents function as follows:    \r\n \r\n def load_documents(source_dir: str, ignored_files: List[str] = []) -> List[Document]:\r\n    \"\"\"\r\n    Loads all documents from the source documents directory, ignoring specified files\r\n    \"\"\"\r\n    all_files = []\r\n    for ext in LOADER_MAPPING:\r\n        all_files.extend(\r\n            glob.glob(os.path.join(source_dir, f\"**/*{ext}\"), recursive=True)\r\n        )\r\n    filtered_files = [file_path for file_path in all_files if file_path not in ignored_files]\r\n\r\n    with Pool(processes=os.cpu_count()) as pool:\r\n        results = []\r\n        with tqdm(total=len(filtered_files), desc='Loading new documents', ncols=80) as pbar:\r\n            for i, docs in enumerate(pool.imap_unordered(load_single_document, filtered_files)):\r\n                results.extend(docs)\r\n                pbar.update()\r\n\r\n    return results", "patch": ""}}
{"instance_id": "zylon-ai__private-gpt-1258", "found_files": [["private_gpt/server/ingest/ingest_service.py", "private_gpt/server/ingest/ingest_router.py", "private_gpt/paths.py", "private_gpt/launcher.py"], ["private_gpt/server/ingest/ingest_service.py", "private_gpt/server/ingest/ingest_router.py", "private_gpt/ui/ui.py", "private_gpt/paths.py", "private_gpt/launcher.py"]], "found_modules": [["private_gpt/server/ingest/ingest_service.py:IngestService", "private_gpt/settings/settings_loader.py:merge_settings", "private_gpt/server/embeddings/embeddings_router.py:embeddings_generation", "private_gpt/launcher.py:create_app", "private_gpt/server/health/health_router.py:health", "private_gpt/ui/ui.py:PrivateGptUi._chat", "private_gpt/server/chunks/chunks_service.py:ChunksService", "private_gpt/open_ai/openai_models.py:OpenAICompletion", "private_gpt/components/vector_store/vector_store_component.py:VectorStoreComponent", "private_gpt/server/chat/chat_service.py:ChatService", "private_gpt/open_ai/openai_models.py:to_openai_sse_stream", "private_gpt/components/embedding/custom/sagemaker.py:SagemakerEmbedding", "private_gpt/server/ingest/ingest_watcher.py:IngestWatcher", "private_gpt/server/ingest/ingest_router.py:ingest", "private_gpt/paths.py:_absolute_or_from_project_root"], ["private_gpt/settings/settings_loader.py:merge_settings", "private_gpt/server/ingest/ingest_service.py:IngestService", "private_gpt/launcher.py:create_app", "private_gpt/ui/ui.py:PrivateGptUi", "private_gpt/server/embeddings/embeddings_router.py:embeddings_generation", "private_gpt/ui/ui.py:PrivateGptUi._chat", "private_gpt/server/chunks/chunks_service.py:ChunksService", "private_gpt/open_ai/openai_models.py:OpenAICompletion", "private_gpt/components/vector_store/vector_store_component.py:VectorStoreComponent", "private_gpt/server/chat/chat_service.py:ChatService", "private_gpt/open_ai/openai_models.py:to_openai_sse_stream", "private_gpt/server/ingest/ingest_router.py:ingest", "private_gpt/server/health/health_router.py:health"]], "found_entities": [["private_gpt/server/ingest/ingest_service.py:IngestService.ingest", "private_gpt/settings/settings_loader.py:merge_settings", "private_gpt/server/embeddings/embeddings_router.py:embeddings_generation", "private_gpt/launcher.py:create_app", "private_gpt/server/health/health_router.py:health", "private_gpt/ui/ui.py:PrivateGptUi._chat.yield_deltas", "private_gpt/server/chunks/chunks_service.py:ChunksService._get_sibling_nodes_text", "private_gpt/open_ai/openai_models.py:OpenAICompletion.json_from_delta", "private_gpt/components/vector_store/vector_store_component.py:VectorStoreComponent.get_retriever", "private_gpt/ui/ui.py:PrivateGptUi._chat.build_history", "private_gpt/server/chat/chat_service.py:ChatService.chat", "private_gpt/open_ai/openai_models.py:to_openai_sse_stream", "private_gpt/server/chunks/chunks_service.py:ChunksService.retrieve_relevant", "private_gpt/components/embedding/custom/sagemaker.py:SagemakerEmbedding.class_name", "private_gpt/server/ingest/ingest_watcher.py:IngestWatcher.start", "private_gpt/server/ingest/ingest_router.py:ingest", "private_gpt/paths.py:_absolute_or_from_project_root"], ["private_gpt/settings/settings_loader.py:merge_settings", "private_gpt/server/ingest/ingest_service.py:IngestService.ingest", "private_gpt/launcher.py:create_app", "private_gpt/ui/ui.py:PrivateGptUi._build_ui_blocks", "private_gpt/server/embeddings/embeddings_router.py:embeddings_generation", "private_gpt/ui/ui.py:PrivateGptUi._chat.yield_deltas", "private_gpt/server/chunks/chunks_service.py:ChunksService._get_sibling_nodes_text", "private_gpt/open_ai/openai_models.py:OpenAICompletion.json_from_delta", "private_gpt/components/vector_store/vector_store_component.py:VectorStoreComponent.get_retriever", "private_gpt/ui/ui.py:PrivateGptUi._chat.build_history", "private_gpt/server/chat/chat_service.py:ChatService.chat", "private_gpt/open_ai/openai_models.py:to_openai_sse_stream", "private_gpt/server/chunks/chunks_service.py:ChunksService.retrieve_relevant", "private_gpt/server/ingest/ingest_router.py:ingest", "private_gpt/server/health/health_router.py:health"]], "raw_output_loc": ["```\nprivate_gpt/server/ingest/ingest_service.py\nline: 31-37\nfunction: IngestService.ingest (uses FILE_READER_CLS / DEFAULT_FILE_READER_CLS to determine supported extensions)\n\nprivate_gpt/server/ingest/ingest_service.py\nline: 85-131\nfunction: IngestService.ingest (extension detection + reader selection fallback logic)\n\nprivate_gpt/server/ingest/ingest_router.py\nline: 19-39\nfunction: ingest (API docstring claims \u201cMost common document formats are supported\u201d \u2014 good place to link/mention explicit list)\n\nprivate_gpt/paths.py\nline: 13-18\nfunction: _absolute_or_from_project_root\nline: 15\nvariable: docs_path (points to docs/ used by the FastAPI OpenAPI description)\n\nprivate_gpt/launcher.py\nline: 22-27\nfunction: create_app (loads docs_path/\"description.md\" for API docs; add/update supported file types info in docs content referenced here)\n```", "```\nprivate_gpt/server/ingest/ingest_service.py\nline: 31-37  # FILE_READER_CLS / DEFAULT_FILE_READER_CLS mapping (source of truth for supported extensions)\nfunction: IngestService.ingest\nline: 85-131\n\nprivate_gpt/server/ingest/ingest_router.py\nfunction: ingest\nline: 19-34  # mentions \u201cMost common document formats are supported...\u201d; good spot to reference/point to the supported list\n\nprivate_gpt/ui/ui.py\nfunction: PrivateGptUi._build_ui_blocks\nline: 165-214  # Upload UI; optional place to hint supported file types in the UI\n\nprivate_gpt/paths.py\nline: 13-16  # docs_path points to docs/ (where docs content lives)\n\nprivate_gpt/launcher.py\nfunction: create_app\nline: 25-27  # loads docs/description.md into API description (docs entrypoint to add \u201csupported file types\u201d section)\n```"], "meta_data": {"repo": "zylon-ai/private-gpt", "base_commit": "57a829a8e8cf5c31410c256ae59e0eda9f129a41", "problem_statement": "Add a list of supported file types to README and Docs\n\nMaybe I'm blind, but I couldn't find a list of the file types supported by privateGPT.\r\n\r\nOne might add a list with the supported file types to the [README.md](https://github.com/imartinez/privateGPT/blob/main/README.md) and [PrivateGPT Docs](https://docs.privategpt.dev/).\r\n\r\nKinda related https://github.com/imartinez/privateGPT/issues/451 and apologize at this place, I haven't had the time yet to look further into a first implementation proposal.", "patch": ""}}
{"instance_id": "zylon-ai__private-gpt-875", "found_files": [["privateGPT.py", "constants.py", "ingest.py"], ["privateGPT.py", "constants.py", "ingest.py"]], "found_modules": [["privateGPT.py:main", "privateGPT.py:parse_arguments", "ingest.py:main"], ["privateGPT.py:main", "privateGPT.py:parse_arguments"]], "found_entities": [["privateGPT.py:main", "privateGPT.py:parse_arguments", "ingest.py:main"], ["privateGPT.py:main", "privateGPT.py:parse_arguments"]], "raw_output_loc": ["```\nprivateGPT.py\nline: 19\n# model_n_ctx = os.environ.get('MODEL_N_CTX') (string/None handling); root of bad/extra param propagation\n\nprivateGPT.py\nline: 25-43\nfunction: main\nline: 36\n# LlamaCpp(..., n_ctx=model_n_ctx, ...) (same ctx handling; should be cast/validated)\nline: 38\n# GPT4All(model=..., n_ctx=model_n_ctx, backend='gptj', ...) (direct source of pydantic ValidationError: \u201cn_ctx extra fields not permitted\u201d)\n\nprivateGPT.py\nline: 69-79\nfunction: parse_arguments\n# CLI entry point; potential place to add/validate ctx-related args and/or select correct GPT4All kwargs based on installed langchain version\n\nconstants.py\nline: 1-16\n# env loading pattern used across repo; confirms reliance on .env and motivates consistent type casting/validation approach\n\ningest.py\nline: 33-38\n# env loading pattern (PERSIST_DIRECTORY / EMBEDDINGS_MODEL_NAME); ensures no other hidden MODEL_N_CTX usage elsewhere in repo\n\ningest.py\nline: 141-159\nfunction: main\n# downstream dependency path (Chroma/embeddings); not causing the traceback, but part of overall execution and env usage consistency\n```", "```\nprivateGPT.py\nline: 17-21\n(note: env config; model_n_ctx is read as string via os.environ.get('MODEL_N_CTX') and then passed through to LLM constructors)\n\nprivateGPT.py\nline: 25-43\nfunction: main\n(line: 38 is the failing call site)\n(note: GPT4All(model=model_path, n_ctx=model_n_ctx, backend='gptj', n_batch=model_n_batch, callbacks=callbacks, verbose=False) triggers pydantic ValidationError \u201cn_ctx extra fields not permitted\u201d)\n\nprivateGPT.py\nline: 34-41\nfunction: main\n(note: model_type dispatch; both branches pass n_ctx=model_n_ctx, so any parameter rename/compat shim must be applied here)\n\nprivateGPT.py\nline: 69-79\nfunction: parse_arguments\n(context: could be extended to accept/validate context-related args; currently only hide-source/mute-stream)\n\nconstants.py\nline: 7-15\nvariable: CHROMA_SETTINGS\n(context/dependency: imported into privateGPT.py and ingest.py; verifies runtime config loading pattern in repo)\n\ningest.py\nline: 33-37\n(note: shows same env-loading approach used across repo; useful to keep behavior consistent if adding validation/casting for env vars like MODEL_N_CTX)\n```"], "meta_data": {"repo": "zylon-ai/private-gpt", "base_commit": "86c2dcfe1b33ac467558487a1df408abee0d2321", "problem_statement": "I got a Traceback error while running privateGPT on Ubuntu 22.04\n\nWhile running privateGPT.py, the error started after \"gptj_model_load: model size = 3609.38 MB / num tensors = 285\". The error reads as follows:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dennis/privateGPT/privateGPT.py\", line 83, in <module>\r\n    main()\r\n  File \"/home/dennis/privateGPT/privateGPT.py\", line 38, in main\r\n    llm = GPT4All(model=model_path, n_ctx=model_n_ctx, backend='gptj', n_batch=model_n_batch, callbacks=callbacks, verbose=False)\r\n  File \"/home/dennis/.local/lib/python3.10/site-packages/langchain/load/serializable.py\", line 74, in __init__\r\n    super().__init__(**kwargs)\r\n  File \"pydantic/main.py\", line 341, in pydantic.main.BaseModel.__init__\r\npydantic.error_wrappers.ValidationError: 1 validation error for GPT4All\r\nn_ctx\r\n  extra fields not permitted (type=value_error.extra)\r\n\r\nI have no idea what's happening here. Could anyone be able to fix it so that I can try privateGPT on my Ubuntu 22.04 on an old iMac late 2012?", "patch": ""}}
{"instance_id": "zylon-ai__private-gpt-375", "found_files": [["privateGPT.py", "ingest.py", "constants.py"], ["privateGPT.py", "ingest.py", "constants.py"]], "found_modules": [["privateGPT.py:main", "ingest.py:process_documents"], ["privateGPT.py:main", "ingest.py:process_documents", "ingest.py:MyElmLoader"]], "found_entities": [["privateGPT.py:main", "ingest.py:process_documents"], ["privateGPT.py:main", "ingest.py:process_documents", "ingest.py:MyElmLoader.load"]], "raw_output_loc": ["```\nprivateGPT.py\nline: 13-19\nline: 22-39\nfunction: main\n  - LlamaCpp/GPT4All initialization (n_ctx from MODEL_N_CTX; likely string vs int)\n  - RetrievalQA.from_chain_type(chain_type=\"stuff\") (can overflow context window)\n\nprivateGPT.py\nline: 27-28\nfunction: main\n  - db.as_retriever() (place to set retriever search_kwargs like k / fetch_k to reduce returned context)\n\ningest.py\nline: 37-39\n  - chunk_size / chunk_overlap defaults (affects amount of text stuffed into prompt)\n\ningest.py\nline: 114-127\nfunction: process_documents\n  - RecursiveCharacterTextSplitter(chunk_size, chunk_overlap) (key for controlling chunking vs model context)\n\nconstants.py\nline: 11-15\n  - CHROMA_SETTINGS (Chroma client settings; upstream dependency used by both ingest.py and privateGPT.py)\n```", "```\nprivateGPT.py\nline: 13-19\n  - embeddings_model_name / persist_directory / model_type / model_path / model_n_ctx read from env (MODEL_N_CTX is critical for the 1000-token context window)\n\nprivateGPT.py\nline: 22-60\nfunction: main\n  - retriever = db.as_retriever() (line: 27; no retrieval limits like k/search_kwargs set)\n  - LlamaCpp(..., n_ctx=model_n_ctx, ...) / GPT4All(..., n_ctx=model_n_ctx, ...) (line: 31-36; likely needs int conversion + max_tokens and/or correct n_ctx)\n  - RetrievalQA.from_chain_type(... chain_type=\"stuff\" ...) (line: 39; \u201cstuff\u201d concatenates retrieved docs into one prompt, causing context overflow)\n  - res = qa(query) (line: 47; call site where overflow manifests)\n\ningest.py\nline: 37-39\n  - chunk_size = 500, chunk_overlap = 50 (chunking affects how large retrieved context can get)\n\ningest.py\nline: 114-127\nfunction: process_documents\n  - RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n  - texts = text_splitter.split_documents(documents) (creates chunks later used in the \u201cstuff\u201d prompt)\n\nconstants.py\nline: 11-15\n  - CHROMA_SETTINGS used by both ingest.py and privateGPT.py when creating/loading Chroma (central place if retrieval/vectorstore settings need adjustment)\n```"], "meta_data": {"repo": "zylon-ai/private-gpt", "base_commit": "2f3aab9cfdc139f399387dbb90300d5a8bf8d2f1", "problem_statement": "ValueError: Requested tokens exceed context window of 1000\n\nAfter I ingest a file, run privateGPT and try to ask anything, I get following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Stable_Diffusion\\privateGPT\\privateGPT.py\", line 75, in <module>\r\n    main()\r\n  File \"C:\\Stable_Diffusion\\privateGPT\\privateGPT.py\", line 47, in main\r\n    res = qa(query)\r\n  File \"C:\\Users\\olegt\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\chains\\base.py\", line 140, in __call__\r\n    raise e\r\n  File \"C:\\Users\\olegt\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\chains\\base.py\", line 134, in __call__\r\n    self._call(inputs, run_manager=run_manager)\r\n  File \"C:\\Users\\olegt\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\chains\\retrieval_qa\\base.py\", line 120, in _call\r\n    answer = self.combine_documents_chain.run(\r\n  File \"C:\\Users\\olegt\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\chains\\base.py\", line 239, in run\r\n    return self(kwargs, callbacks=callbacks)[self.output_keys[0]]\r\n  File \"C:\\Users\\olegt\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\chains\\base.py\", line 140, in __call__\r\n    raise e\r\n  File \"C:\\Users\\olegt\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\chains\\base.py\", line 134, in __call__\r\n    self._call(inputs, run_manager=run_manager)\r\n  File \"C:\\Users\\olegt\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\chains\\combine_documents\\base.py\", line 84, in _call\r\n    output, extra_return_dict = self.combine_docs(\r\n  File \"C:\\Users\\olegt\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\chains\\combine_documents\\stuff.py\", line 87, in combine_docs\r\n    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}\r\n  File \"C:\\Users\\olegt\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\chains\\llm.py\", line 213, in predict\r\n    return self(kwargs, callbacks=callbacks)[self.output_key]\r\n  File \"C:\\Users\\olegt\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\chains\\base.py\", line 140, in __call__\r\n    raise e\r\n  File \"C:\\Users\\olegt\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\chains\\base.py\", line 134, in __call__\r\n    self._call(inputs, run_manager=run_manager)\r\n  File \"C:\\Users\\olegt\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\chains\\llm.py\", line 69, in _call\r\n    response = self.generate([inputs], run_manager=run_manager)\r\n  File \"C:\\Users\\olegt\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\chains\\llm.py\", line 79, in generate\r\n    return self.llm.generate_prompt(\r\n  File \"C:\\Users\\olegt\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\llms\\base.py\", line 134, in generate_prompt\r\n    return self.generate(prompt_strings, stop=stop, callbacks=callbacks)\r\n  File \"C:\\Users\\olegt\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\llms\\base.py\", line 191, in generate\r\n    raise e\r\n  File \"C:\\Users\\olegt\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\llms\\base.py\", line 185, in generate\r\n    self._generate(prompts, stop=stop, run_manager=run_manager)\r\n  File \"C:\\Users\\olegt\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\llms\\base.py\", line 405, in _generate\r\n    self._call(prompt, stop=stop, run_manager=run_manager)\r\n  File \"C:\\Users\\olegt\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\llms\\llamacpp.py\", line 225, in _call\r\n    for token in self.stream(prompt=prompt, stop=stop, run_manager=run_manager):\r\n  File \"C:\\Users\\olegt\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\llms\\llamacpp.py\", line 274, in stream\r\n    for chunk in result:\r\n  File \"C:\\Users\\olegt\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\llama_cpp\\llama.py\", line 618, in _create_completion\r\n    raise ValueError(\r\nValueError: Requested tokens exceed context window of 1000\r\n```\r\n\r\nI tried it with docx and pdf, used models are ggml-vic13b-q5_1.bin and stable-vicuna-13B.ggml.q4_0.bin.\r\nDuring ingestion or loading privateGPT I get no error.\r\n\r\nOS: Windows 10\r\nCPU: Ryzen 7 3700\r\nRAM: 32gb", "patch": ""}}
{"instance_id": "zylon-ai__private-gpt-15", "found_files": [["privateGPT.py", "ingest.py", "constants.py"], ["privateGPT.py", "ingest.py", "constants.py"]], "found_modules": [["privateGPT.py:main", "ingest.py:main"], ["privateGPT.py:main", "ingest.py:main"]], "found_entities": [["privateGPT.py:main", "ingest.py:main"], ["privateGPT.py:main", "ingest.py:main"]], "raw_output_loc": ["```\nprivateGPT.py\nline: 11-17\nline: 20-34\nfunction: main\n# Reads MODEL_TYPE/MODEL_PATH/LLAMA_EMBEDDINGS_MODEL/MODEL_N_CTX and constructs LlamaCpp + LlamaCppEmbeddings.\n# Primary place to validate model file type (.gguf vs old .bin/.ggml), add user-facing warning, or switch defaults.\n\nprivateGPT.py\nline: 21\nfunction: main (callsite)\n# LlamaCppEmbeddings(model_path=llama_embeddings_model, n_ctx=model_n_ctx)\n# Embeddings model may also be an old GGML .bin, triggering the same llama.cpp mmap/alignment warning.\n\nprivateGPT.py\nline: 28\nfunction: main (callsite)\n# LlamaCpp(model_path=model_path, n_ctx=model_n_ctx, callbacks=..., verbose=False)\n# Main LLM load path producing the reported llama.cpp message; key point to enforce GGUF or conversion guidance.\n\ningest.py\nline: 71-91\nfunction: main\n# Ingestion pipeline constructs LlamaCppEmbeddings(...) used to build Chroma vectorstore; same model-format constraints apply.\n\nconstants.py\nline: 7-15\n# Central configuration imported by both scripts; suitable place to add shared validation/config (e.g., allowed model extensions)\n# if you want consistent enforcement across ingest.py and privateGPT.py.\n```", "```\nprivateGPT.py\nline: 14-16  (reads MODEL_TYPE / MODEL_PATH / MODEL_N_CTX; model_path points to ggml *.bin that triggers llama.cpp \u201cold format / no mmap\u201d warning)\nline: 20     function: main  (entry point that instantiates the LLM/embeddings)\nline: 21     function: main  (LlamaCppEmbeddings(model_path=..., n_ctx=...))\nline: 26-33  function: main  (match model_type; LlamaCpp(model_path=..., n_ctx=...) is the direct llama.cpp caller)\n\ningest.py\nline: 71     function: main  (entry point for ingestion run)\nline: 73-77  function: main  (reads LLAMA_EMBEDDINGS_MODEL / MODEL_N_CTX; controls which llama.cpp-backed embeddings model is used)\nline: 87-88  function: main  (LlamaCppEmbeddings(model_path=..., n_ctx=...) -> also triggers llama.cpp loading warnings)\n\nconstants.py\nline: 8-15   (PERSIST_DIRECTORY + CHROMA_SETTINGS; used by both privateGPT.py and ingest.py, impacts runtime pathing and persistence but not the mmap warning itself)\n```"], "meta_data": {"repo": "zylon-ai/private-gpt", "base_commit": "fdb45741e521d606b028984dbc2f6ac57755bb88", "problem_statement": "llama.cpp: can't use mmap because tensors are not aligned; convert to new format to avoid this\n\nllama.cpp: loading model from ./models/ggml-model-q4_0.bin\r\nllama.cpp: can't use mmap because tensors are not aligned; convert to new format to avoid this\r\nllama_model_load_internal: format     = 'ggml' (old version with low tokenizer quality and no mmap support)\r\nllama_model_load_internal: n_vocab    = 32000\r\nllama_model_load_internal: n_ctx      = 512\r\nllama_model_load_internal: n_embd     = 4096\r\nllama_model_load_internal: n_mult     = 256\r\nllama_model_load_internal: n_head     = 32\r\nllama_model_load_internal: n_layer    = 32\r\nllama_model_load_internal: n_rot      = 128\r\nllama_model_load_internal: ftype      = 2 (mostly Q4_0)\r\nllama_model_load_internal: n_ff       = 11008\r\nllama_model_load_internal: n_parts    = 1\r\nllama_model_load_internal: model size = 7B\r\nllama_model_load_internal: ggml ctx size = 4113739.11 KB\r\nllama_model_load_internal: mem required  = 5809.32 MB (+ 2052.00 MB per state)\r\n...................................................................................................\r\nI am using a recommended model, but I get this error message. How do you think I could solve it?", "patch": ""}}
{"instance_id": "zylon-ai__private-gpt-850", "found_files": [["privateGPT.py", "ingest.py", "constants.py"], ["privateGPT.py", "ingest.py", "constants.py"]], "found_modules": [["privateGPT.py:main", "ingest.py:process_documents"], ["privateGPT.py:main", "ingest.py:process_documents", "privateGPT.py:parse_arguments"]], "found_entities": [["privateGPT.py:main", "ingest.py:process_documents"], ["privateGPT.py:main", "ingest.py:process_documents", "privateGPT.py:parse_arguments"]], "raw_output_loc": ["```\nprivateGPT.py\nline: 14-23  (ENV vars: EMBEDDINGS_MODEL_NAME, MODEL_TYPE, MODEL_PATH, MODEL_N_CTX, MODEL_N_BATCH, TARGET_SOURCE_CHUNKS)\nline: 25-55\nfunction: main   (creates LlamaCpp/GPT4All with n_ctx; builds RetrievalQA; calls qa(query) where token-limit errors surface)\n\nprivateGPT.py\nline: 34-43\nfunction: main   (LLM init + RetrievalQA.from_chain_type(chain_type=\"stuff\"); prompt/context assembly likely overflowing with Chinese input)\n\ningest.py\nline: 33-39\nvariable: chunk_size, chunk_overlap   (currently fixed; printed as \u201ctokens\u201d but RecursiveCharacterTextSplitter defaults are character-based -> misleading token math esp. \u4e2d\u6587)\n\ningest.py\nline: 113-126\nfunction: process_documents   (RecursiveCharacterTextSplitter(chunk_size=..., chunk_overlap=...); primary place to adjust token-based splitting)\n\nconstants.py\nline: 7-15\nvariable: CHROMA_SETTINGS (used by ingest.py/privateGPT.py; affects vectorstore config but is an upstream dependency of the query flow)\n```", "```\nprivateGPT.py\nline: 14-22\n(note: reads env vars impacting context/token limits: MODEL_N_CTX, TARGET_SOURCE_CHUNKS, etc.)\n\nprivateGPT.py\nline: 25-56\nfunction: main\n(note: builds LLM with n_ctx (lines 36-38) and runs qa(query) (line 54) where \u201ctoken\u8d85\u51fa\u9650\u5236\u201d will surface)\n\nprivateGPT.py\nline: 33-43\n(note: LlamaCpp/GPT4All n_ctx wiring; RetrievalQA.from_chain_type(... chain_type=\"stuff\") constructs the full prompt and can overflow context)\n\ningest.py\nline: 37-39\n(note: chunk_size/chunk_overlap are defined; currently assumes \u201ctokens\u201d but is not token-based)\n\ningest.py\nline: 113-126\nfunction: process_documents\n(note: RecursiveCharacterTextSplitter is character-based; prints \u201cmax. {chunk_size} tokens each\u201d (line 125) which is misleading and can relate to \u201ctoken\u8ba1\u7b97\u65b9\u5f0f\u5f88\u5947\u602a\u201d esp. for Chinese)\n\ningest.py\nline: 64-81\nvariable: LOADER_MAPPING\n(note: input document encoding handling; .txt loader uses utf8 (line 79), relevant to Chinese ingestion/representation)\n\nconstants.py\nline: 11-15\nvariable: CHROMA_SETTINGS\n(note: affects Chroma retrieval; larger k (TARGET_SOURCE_CHUNKS) + \u201cstuff\u201d chain can increase prompt size and trigger context overflow)\n```"], "meta_data": {"repo": "zylon-ai/private-gpt", "base_commit": "dd1100202881a01b6b013b7bc1faad8b5c63fec9", "problem_statement": "privateGPT\u4e2d\u6587\u63d0\u95ee\u663e\u793atoken\u8d85\u51fa\u9650\u5236\uff0c\u82f1\u6587\u63d0\u95ee\u4e0d\u5b58\u5728\u8fd9\u4e2a\u95ee\u9898\n\ntoken\u7684\u8ba1\u7b97\u65b9\u5f0f\u5f88\u5947\u602a\u4e94\u4e2a\u5b57\u6307\u4ee4\u7684token\u6bd4\u4e03\u4e2a\u5b57\u591a\r\n![\u5fae\u4fe1\u56fe\u7247_20230713094810](https://github.com/imartinez/privateGPT/assets/139415035/6346ae1f-9c65-4721-b7dd-a176fc9be4e1)\r\n![\u5fae\u4fe1\u56fe\u7247_20230713094822](https://github.com/imartinez/privateGPT/assets/139415035/60f2d272-8a80-48d7-9032-4d915a83aa7d)", "patch": ""}}
{"instance_id": "zylon-ai__private-gpt-13", "found_files": [["privateGPT.py", "ingest.py"], ["privateGPT.py", "ingest.py"]], "found_modules": [["ingest.py:main", "privateGPT.py:main"], ["ingest.py:main", "privateGPT.py:main"]], "found_entities": [["ingest.py:main", "privateGPT.py:main"], ["ingest.py:main", "privateGPT.py:main"]], "raw_output_loc": ["```\nprivateGPT.py\nline: 9\nfunction: main\nnote: LlamaCppEmbeddings(model_path=\"./models/ggml-model-q4_0.bin\") \u2014 embedding tokenizer/model choice can mismatch corpus/query encoding and propagate bad inputs.\n\nprivateGPT.py\nline: 15\nfunction: main\nnote: GPT4All(model='./models/ggml-gpt4all-j-v1.3-groovy.bin', backend='gptj', ...) \u2014 most likely source of \u201cgpt_tokenize: unknown token '?'\u201d due to tokenizer/model/backend mismatch.\n\nprivateGPT.py\nline: 11-13\nfunction: main\nnote: Chroma(..., embedding_function=llama) + db.as_retriever() \u2014 retrieval pipeline feeding prompts into the LLM; impacts frequency/size of tokenization calls.\n\nprivateGPT.py\nline: 16, 24\nfunction: main\nnote: RetrievalQA.from_chain_type(...) and res = qa(query) \u2014 main execution path that triggers repeated tokenization.\n\ningest.py\nline: 9, 14-18\nfunction: main\nnote: TextLoader(..., encoding=\"utf8\") + LlamaCppEmbeddings(...) + Chroma.from_documents(...persist...) \u2014 upstream producer of the vector DB used by privateGPT.py; embedding model mismatch can indirectly trigger problematic prompts/behavior.\n```", "```\nprivateGPT.py\nline: 1-16\nfunction: main\nnote: GPT4All(model=..., backend='gptj', ...) initialization is the most likely origin of \u201cgpt_tokenize: unknown token '?'\u201d.\n\nprivateGPT.py\nline: 18-25\nfunction: main\nnote: qa(query) call path (RetrievalQA.from_chain_type) where tokenization happens repeatedly during inference.\n\nprivateGPT.py\nline: 9-12\nfunction: main\nnote: LlamaCppEmbeddings + Chroma retriever setup; retrieval size/contents can increase prompt size and contribute to the process being killed.\n\ningest.py\nline: 14-18\nfunction: main\nnote: LlamaCppEmbeddings(model_path=...) + Chroma.from_documents(...); embedding/tokenization + persistence used by privateGPT.py.\n\ningest.py\nline: 11-12\nfunction: main\nnote: RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50); chunking impacts downstream context size (prompt length) and memory pressure.\n```"], "meta_data": {"repo": "zylon-ai/private-gpt", "base_commit": "026b9f895cfb727da523a20c59773146801236ba", "problem_statement": "gpt_tokenize: unknown token '?'\n\ngpt_tokenize: unknown token '?'\r\ngpt_tokenize: unknown token '?'\r\ngpt_tokenize: unknown token '?'\r\ngpt_tokenize: unknown token '?'\r\ngpt_tokenize: unknown token '?'\r\ngpt_tokenize: unknown token '?'\r\ngpt_tokenize: unknown token '?'\r\ngpt_tokenize: unknown token '?'\r\ngpt_tokenize: unknown token '?'\r\ngpt_tokenize: unknown token '?'\r\ngpt_tokenize: unknown token '?'\r\ngpt_tokenize: unknown token '?'\r\ngpt_tokenize: unknown token '?'\r\ngpt_tokenize: unknown token '?'\r\ngpt_tokenize: unknown token '?'\r\ngpt_tokenize: unknown token '?'\r\ngpt_tokenize: unknown token '?'\r\ngpt_tokenize: unknown token '?'\r\n[1]    32658 killed     python3 privateGPT.py", "patch": ""}}
{"instance_id": "zylon-ai__private-gpt-685", "found_files": [["privateGPT.py", "ingest.py", "constants.py"], ["privateGPT.py", "ingest.py", "constants.py"]], "found_modules": [["privateGPT.py:main", "privateGPT.py:parse_arguments", "ingest.py:load_documents", "ingest.py:process_documents", "ingest.py:main"], ["privateGPT.py:main", "privateGPT.py:parse_arguments", "ingest.py:load_documents", "ingest.py:process_documents", "ingest.py:main"]], "found_entities": [["privateGPT.py:main", "privateGPT.py:parse_arguments", "ingest.py:load_documents", "ingest.py:process_documents", "ingest.py:main"], ["privateGPT.py:main", "privateGPT.py:parse_arguments", "ingest.py:load_documents", "ingest.py:process_documents", "ingest.py:main"]], "raw_output_loc": ["```\nprivateGPT.py\nline: 13-20  (reads env vars; no CPU/threads tuning surfaced here)\nline: 23-40\nfunction: main  (constructs LlamaCpp/GPT4All; missing common CPU-parallelism knobs such as n_threads/n_batch)\nline: 62-72\nfunction: parse_arguments  (no CLI flags for threads/performance)\n\ningest.py\nline: 93-111\nfunction: load_documents  (multiprocessing Pool(processes=os.cpu_count()); key place to adjust/parameterize CPU usage and avoid oversubscription)\nline: 113-126\nfunction: process_documents  (calls load_documents; part of ingestion pipeline)\nline: 141-160\nfunction: main  (creates HuggingFaceEmbeddings + Chroma; embedding generation performance depends on runtime thread/env settings)\n\nconstants.py\nline: 11-15\nvariable: CHROMA_SETTINGS  (Chroma backend config; impacts performance/IO, but no explicit CPU utilization knobs)\n```", "```\nprivateGPT.py\nline: 23-40\nfunction: main\n# LLM creation happens here (LlamaCpp / GPT4All). This is the primary place to add/configure CPU thread controls (e.g., n_threads / n_batch) that directly affect CPU utilization.\n\nprivateGPT.py\nline: 62-72\nfunction: parse_arguments\n# Add CLI flags to control CPU/threads (e.g., --threads/--n-batch) and pass them into main.\n\ningest.py\nline: 93-111\nfunction: load_documents\n# Multiprocessing Pool(processes=os.cpu_count()) controls parallel document loading; make process count configurable if ingestion CPU usage is capped.\n\ningest.py\nline: 113-126\nfunction: process_documents\n# Calls load_documents and then splits text; part of ingestion flow affected by load parallelism and can be a performance hotspot.\n\ningest.py\nline: 141-159\nfunction: main\n# Embeddings + Chroma write path; place to apply embedding-related CPU thread/env configuration (common source of CPU underutilization).\n\nconstants.py\nline: 11-15\nvariable: CHROMA_SETTINGS\n# Chroma/duckdb settings used by both ingest.py and privateGPT.py; relevant if DB settings/telemetry affect performance.\n```"], "meta_data": {"repo": "zylon-ai/private-gpt", "base_commit": "c4b247d696c727c1da6d993ce4f6c3a557e91b42", "problem_statement": "CPU utilization\n\nCPU utilization appears to be capped at 20%\r\nIs there a way to increase CPU utilization and thereby enhance performance?", "patch": ""}}
{"instance_id": "zylon-ai__private-gpt-181", "found_files": [["privateGPT.py", "ingest.py", "constants.py"], ["privateGPT.py", "ingest.py", "constants.py"]], "found_modules": [["privateGPT.py:main", "ingest.py:load_documents", "ingest.py:main", "ingest.py:load_single_document"], ["privateGPT.py:main", "ingest.py:main", "ingest.py:load_single_document", "ingest.py:load_documents"]], "found_entities": [["privateGPT.py:main", "ingest.py:load_documents", "ingest.py:main", "ingest.py:load_single_document"], ["privateGPT.py:main", "ingest.py:main", "ingest.py:load_single_document", "ingest.py:load_documents"]], "raw_output_loc": ["```\nprivateGPT.py\nline: 11-17   # reads env vars: LLAMA_EMBEDDINGS_MODEL, MODEL_TYPE, MODEL_PATH, MODEL_N_CTX\nline: 20-34\nfunction: main\nline: 21       # LlamaCppEmbeddings(model_path=llama_embeddings_model, n_ctx=model_n_ctx)\nline: 28       # LlamaCpp(model_path=model_path, n_ctx=model_n_ctx, callbacks=..., verbose=...)\nline: 30       # GPT4All(model=model_path, n_ctx=model_n_ctx, backend='gptj', ...)\n\ningest.py\nline: 37-57\nfunction: main\nline: 41-43    # reads LLAMA_EMBEDDINGS_MODEL and MODEL_N_CTX from env\nline: 53       # LlamaCppEmbeddings(model_path=llama_embeddings_model, n_ctx=model_n_ctx)\n\nconstants.py\nline: 8-15     # CHROMA_SETTINGS / persist_directory wiring (startup context; not ggml but part of execution flow)\n\nprivateGPT.py:main\nline: 21, 28, 30   # primary call sites likely needing changes to configure llama-cpp context/memory parameters\n\ningest.py:main\nline: 53           # embedding-time ggml context allocation; also likely needs same configuration/validation\n```", "```\nprivateGPT.py\nline: 11-17   # env vars: LLAMA_EMBEDDINGS_MODEL, MODEL_TYPE, MODEL_PATH, MODEL_N_CTX (key for context/memory sizing)\nline: 20-34\nfunction: main\nline: 21      # LlamaCppEmbeddings(model_path=..., n_ctx=model_n_ctx) -> embeddings-side llama.cpp/ggml context allocation\nline: 26-33   # model_type switch\nline: 28      # LlamaCpp(model_path=..., n_ctx=model_n_ctx, ...) -> main llama.cpp/ggml context allocation + where extra params should be added\nline: 30      # GPT4All(model=..., n_ctx=model_n_ctx, ...) -> alternative backend path\n\ningest.py\nline: 39-43   # reads MODEL_N_CTX (key input to ggml context sizing)\nline: 37-57\nfunction: main\nline: 53      # LlamaCppEmbeddings(model_path=..., n_ctx=model_n_ctx) -> embeddings-side ggml allocation (can also trigger the memory-pool error)\n\nconstants.py\nline: 8-15    # PERSIST_DIRECTORY + CHROMA_SETTINGS (execution-path dependency when Chroma client is created in both scripts; not ggml itself but part of init flow)\n\nprivateGPT.py\nline: 18      # imports CHROMA_SETTINGS (ties execution path to constants.py)\nline: 22-23   # Chroma(..., client_settings=CHROMA_SETTINGS) and retriever creation (downstream of embedding creation)\n\ningest.py\nline: 11-12   # imports CHROMA_SETTINGS (ties execution path to constants.py)\nline: 56-57   # Chroma.from_documents(..., client_settings=CHROMA_SETTINGS) (downstream of embedding creation)\n```"], "meta_data": {"repo": "zylon-ai/private-gpt", "base_commit": "7ae80e662936bd946a231d1327bde476556c5d61", "problem_statement": "Segfault : not enough space in the context's memory pool\n\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 3779301744, available 3745676000)\r\nzsh: segmentation fault  python3.11 privateGPT.py\r\n\r\nWhats context memory pool? can i configure it? i actually have a lot of excess memory", "patch": ""}}
