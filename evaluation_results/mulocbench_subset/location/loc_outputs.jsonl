{"instance_id": "scikit-learn__scikit-learn-25525", "found_files": [["sklearn/feature_selection/_sequential.py", "sklearn/utils/_param_validation.py", "sklearn/base.py", "sklearn/exceptions.py"]], "found_modules": [["sklearn/utils/_param_validation.py:validate_parameter_constraints", "benchmarks/bench_glmnet.py:rmse", "asv_benchmarks/benchmarks/model_selection.py:CrossValidationBenchmark", "examples/linear_model/plot_bayesian_ridge_curvefit.py:func", "benchmarks/bench_plot_parallel_pairwise.py:euclidean_distances", "asv_benchmarks/benchmarks/decomposition.py:DictionaryLearningBenchmark", "benchmarks/bench_plot_parallel_pairwise.py:rbf_kernels", "examples/neural_networks/plot_rbm_logistic_classification.py:nudge_dataset", "sklearn/utils/_mocking.py:MockDataFrame", "examples/text/plot_document_classification_20newsgroups.py:size_mb", "benchmarks/bench_sparsify.py:sparsity_ratio", "examples/linear_model/plot_logistic_multinomial.py:plot_hyperplane", "examples/applications/svm_gui.py:Model", "examples/mixture/plot_gmm_selection.py:gmm_bic_score", "examples/linear_model/plot_sgd_iris.py:plot_hyperplane", "examples/cluster/plot_adjusted_for_chance_measures.py:random_labels", "sklearn/externals/_packaging/_structures.py:NegativeInfinityType", "doc/sphinxext/sphinx_issues.py:IssueRole", "examples/gaussian_process/plot_gpr_on_structured_data.py:SequenceKernel", "examples/svm/plot_rbf_parameters.py:MidpointNormalize", "asv_benchmarks/benchmarks/decomposition.py:MiniBatchDictionaryLearningBenchmark", "sklearn/utils/parallel.py:delayed", "sklearn/linear_model/_ridge.py:_solve_sparse_cg.create_mv", "examples/applications/plot_out_of_core_classification.py:ReutersParser", "sklearn/base.py:BaseEstimator", "sklearn/cluster/_bicluster.py:SpectralBiclustering._fit_best_piecewise"]], "found_entities": [["sklearn/utils/_param_validation.py:validate_parameter_constraints", "benchmarks/bench_glmnet.py:rmse", "asv_benchmarks/benchmarks/model_selection.py:CrossValidationBenchmark.time_crossval", "asv_benchmarks/benchmarks/model_selection.py:CrossValidationBenchmark.peakmem_crossval", "examples/linear_model/plot_bayesian_ridge_curvefit.py:func", "benchmarks/bench_plot_parallel_pairwise.py:euclidean_distances", "asv_benchmarks/benchmarks/decomposition.py:DictionaryLearningBenchmark.setup_cache", "benchmarks/bench_plot_parallel_pairwise.py:rbf_kernels", "examples/neural_networks/plot_rbm_logistic_classification.py:nudge_dataset.shift", "sklearn/utils/_mocking.py:MockDataFrame.__eq__", "examples/text/plot_document_classification_20newsgroups.py:size_mb", "sklearn/utils/_mocking.py:MockDataFrame.__ne__", "benchmarks/bench_sparsify.py:sparsity_ratio", "examples/linear_model/plot_logistic_multinomial.py:plot_hyperplane.line", "examples/applications/svm_gui.py:Model.set_surface", "examples/mixture/plot_gmm_selection.py:gmm_bic_score", "examples/linear_model/plot_sgd_iris.py:plot_hyperplane.line", "examples/cluster/plot_adjusted_for_chance_measures.py:random_labels", "sklearn/externals/_packaging/_structures.py:NegativeInfinityType.__ge__", "doc/sphinxext/sphinx_issues.py:IssueRole.default_format_text", "examples/gaussian_process/plot_gpr_on_structured_data.py:SequenceKernel.diag", "examples/svm/plot_rbf_parameters.py:MidpointNormalize.__call__", "examples/gaussian_process/plot_gpr_on_structured_data.py:SequenceKernel.is_stationary", "asv_benchmarks/benchmarks/decomposition.py:MiniBatchDictionaryLearningBenchmark.make_scorers", "sklearn/utils/parallel.py:delayed.delayed_function", "sklearn/linear_model/_ridge.py:_solve_sparse_cg.create_mv._mv", "examples/applications/plot_out_of_core_classification.py:ReutersParser.start_reuters", "sklearn/base.py:BaseEstimator._validate_params", "sklearn/cluster/_bicluster.py:SpectralBiclustering._fit_best_piecewise.make_piecewise"]], "raw_output_loc": ["```\nsklearn/feature_selection/_sequential.py\nclass: SequentialFeatureSelector\nlines: Top (class definition and parameter_constraints), __init__, fit\n\nsklearn/utils/_param_validation.py\nfunction: validate_parameter_constraints\nlines: 28-100\n\nsklearn/base.py\nfunction: BaseEstimator._validate_params\nlines: 614-626\n\nsklearn/exceptions.py\nclass: InvalidParameterError\n\nexamples/feature_selection\n(location to add or update an example demonstrating negative tol)\n```"], "meta_data": {"repo": "scikit-learn/scikit-learn", "base_commit": "559609fe98ec2145788133687e64a6e87766bc77", "problem_statement": "Extend SequentialFeatureSelector example to demonstrate how to use negative tol\n\n### Describe the bug\r\n\r\nI utilized the **SequentialFeatureSelector** for feature selection in my code, with the direction set to \"backward.\" The tolerance value is negative and the selection process stops when the decrease in the metric, AUC in this case, is less than the specified tolerance. Generally, increasing the number of features results in a higher AUC, but sacrificing some features, especially correlated ones that offer little contribution, can produce a pessimistic model with a lower AUC. The code worked as expected in **sklearn 1.1.1**, but when I updated to **sklearn 1.2.1**, I encountered the following error.\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```python\r\nfrom sklearn.datasets import load_breast_cancer\r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn.feature_selection import SequentialFeatureSelector\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom sklearn.pipeline import Pipeline\r\n\r\nX, y = load_breast_cancer(return_X_y=True)\r\n\r\nTOL = -0.001\r\nfeature_selector = SequentialFeatureSelector(\r\n                    LogisticRegression(max_iter=1000),\r\n                    n_features_to_select=\"auto\",\r\n                    direction=\"backward\",\r\n                    scoring=\"roc_auc\",\r\n                    tol=TOL\r\n                )\r\n\r\n\r\npipe = Pipeline(\r\n    [('scaler', StandardScaler()), \r\n    ('feature_selector', feature_selector), \r\n    ('log_reg', LogisticRegression(max_iter=1000))]\r\n    )\r\n\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    pipe.fit(X, y)\r\n    print(pipe['log_reg'].coef_[0])\r\n\r\n```\r\n\r\n### Expected Results\r\n\r\n```\r\n$ python sfs_tol.py \r\n[-2.0429818   0.5364346  -1.35765488 -2.85009904 -2.84603016]\r\n```\r\n\r\n### Actual Results\r\n\r\n```python-traceback\r\n$ python sfs_tol.py \r\nTraceback (most recent call last):\r\n  File \"/home/modelling/users-workspace/nsofinij/lab/open-source/sfs_tol.py\", line 28, in <module>\r\n    pipe.fit(X, y)\r\n  File \"/home/modelling/opt/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/pipeline.py\", line 401, in fit\r\n    Xt = self._fit(X, y, **fit_params_steps)\r\n  File \"/home/modelling/opt/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/pipeline.py\", line 359, in _fit\r\n    X, fitted_transformer = fit_transform_one_cached(\r\n  File \"/home/modelling/opt/anaconda3/envs/py310/lib/python3.10/site-packages/joblib/memory.py\", line 349, in __call__\r\n    return self.func(*args, **kwargs)\r\n  File \"/home/modelling/opt/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\r\n    res = transformer.fit_transform(X, y, **fit_params)\r\n  File \"/home/modelling/opt/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 142, in wrapped\r\n    data_to_wrap = f(self, X, *args, **kwargs)\r\n  File \"/home/modelling/opt/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/base.py\", line 862, in fit_transform\r\n    return self.fit(X, y, **fit_params).transform(X)\r\n  File \"/home/modelling/opt/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/feature_selection/_sequential.py\", line 201, in fit\r\n    self._validate_params()\r\n  File \"/home/modelling/opt/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/base.py\", line 581, in _validate_params\r\n    validate_parameter_constraints(\r\n  File \"/home/modelling/opt/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 97, in validate_parameter_constraints\r\n    raise InvalidParameterError(\r\nsklearn.utils._param_validation.InvalidParameterError: The 'tol' parameter of SequentialFeatureSelector must be None or a float in the range (0, inf). Got -0.001 instead.\r\n\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:26:04) [GCC 10.4.0]\r\nexecutable: /home/modelling/opt/anaconda3/envs/py310/bin/python\r\n   machine: Linux-4.14.301-224.520.amzn2.x86_64-x86_64-with-glibc2.26\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.1\r\n          pip: 23.0\r\n   setuptools: 66.1.1\r\n        numpy: 1.24.1\r\n        scipy: 1.10.0\r\n       Cython: None\r\n       pandas: 1.5.3\r\n   matplotlib: 3.6.3\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libgomp\r\n       filepath: /home/modelling/opt/anaconda3/envs/py310/lib/python3.10/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n    num_threads: 64\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/modelling/opt/anaconda3/envs/py310/lib/python3.10/site-packages/numpy.libs/libopenblas64_p-r0-15028c96.3.21.so\r\n        version: 0.3.21\r\nthreading_layer: pthreads\r\n   architecture: SkylakeX\r\n    num_threads: 64\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/modelling/opt/anaconda3/envs/py310/lib/python3.10/site-packages/scipy.libs/libopenblasp-r0-41284840.3.18.so\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: SkylakeX\r\n    num_threads: 64\r\n```", "patch": ""}}
{"instance_id": "pallets__flask-2264", "found_files": [[]], "found_modules": [[]], "found_entities": [[]], "raw_output_loc": [], "meta_data": {"repo": "pallets/flask", "base_commit": "cb94f4c5d3d4e1797207fd03d20d06c7bc0d05b4", "problem_statement": "Handle app factory in FLASK_APP\n\n`FLASK_APP=myproject.app:create_app('dev')`\r\n[\r\nGunicorn does this with `eval`](https://github.com/benoitc/gunicorn/blob/fbd151e9841e2c87a18512d71475bcff863a5171/gunicorn/util.py#L364), which I'm not super happy with. Instead, we could use `literal_eval` to allow a simple list of arguments. The line should never be so complicated that `eval` would be necessary anyway.\r\n\r\n~~~python\r\n# might need to fix this regex\r\nm = re.search(r'(\\w+)(\\(.*\\))', app_obj)\r\n\r\nif m:\r\n    app = getattr(mod, m.group(1))(*literal_eval(m.group(2)))\r\n~~~", "patch": ""}}
{"instance_id": "localstack__localstack-894", "found_files": [["localstack/services/generic_proxy.py", "localstack/services/infra.py", "localstack/config.py", "localstack/utils/common.py"]], "found_modules": [["localstack/config.py:service_port", "localstack/utils/aws/aws_stack.py:iam_resource_arn", "localstack/services/infra.py:start_local_api", "localstack/utils/aws/aws_stack.py:dynamodb_table_arn", "localstack/utils/analytics/event_publisher.py:AnalyticsEvent", "localstack/utils/kinesis/kinesis_util.py:EventFileReaderThread", "localstack/utils/common.py:wait_for_port_open", "localstack/services/infra.py:start_secretsmanager", "localstack/utils/aws/aws_models.py:LambdaFunction", "localstack/services/install.py:log_install_msg", "localstack/utils/common.py:JsonObject", "localstack/utils/cloudformation/template_deployer.py:retrieve_topic_arn", "localstack/services/sns/sns_listener.py:do_delete_topic", "localstack/utils/cloudformation/template_deployer.py:get_role_arn", "localstack/utils/aws/aws_stack.py:VelocityUtil", "localstack/utils/bootstrap.py:start_infra_locally", "localstack/services/sns/sns_listener.py:do_subscribe", "localstack/services/sns/sns_listener.py:_get_tags", "localstack/dashboard/infra.py:get_lambda_event_sources", "localstack/services/infra.py:restore_persisted_data", "localstack/services/sns/sns_listener.py:do_untag_resource", "localstack/utils/common.py:CaptureOutput", "localstack/services/firehose/firehose_api.py:serve", "localstack/services/sns/sns_listener.py:get_topic_by_arn", "localstack/services/infra.py:register_signal_handlers", "localstack/services/sns/sns_listener.py:get_subscription_by_arn", "localstack/utils/aws/aws_stack.py:get_s3_client", "localstack/services/infra.py:start_proxy_for_service", "localstack/services/s3/s3_listener.py:get_replication", "localstack/services/dynamodb/dynamodb_listener.py:ProxyListenerDynamoDB", "localstack/utils/aws/aws_models.py:EventSource", "localstack/services/infra.py:start_moto_server_separate", "localstack/services/generic_proxy.py:GenericProxyHandler", "localstack/utils/aws/aws_stack.py:Environment", "localstack/dashboard/infra.py:cmd_sqs", "localstack/services/awslambda/lambda_executors.py:LambdaExecutor", "localstack/services/generic_proxy.py:ProxyListener", "localstack/utils/cloudwatch/cloudwatch_util.py:publish_lambda_result", "localstack/utils/kinesis/kinesis_connector.py:KinesisProcessor", "localstack/dashboard/infra.py:cmd_es", "localstack/utils/cloudwatch/cloudwatch_util.py:publish_event", "localstack/services/infra.py:Plugin", "localstack/dashboard/infra.py:cmd_kinesis", "localstack/dashboard/infra.py:get_dynamo_dbs", "localstack/utils/common.py:is_port_open"]], "found_entities": [["localstack/config.py:service_port", "localstack/utils/aws/aws_stack.py:iam_resource_arn", "localstack/services/infra.py:start_local_api", "localstack/utils/aws/aws_stack.py:dynamodb_table_arn", "localstack/utils/analytics/event_publisher.py:AnalyticsEvent.process_id", "localstack/utils/kinesis/kinesis_util.py:EventFileReaderThread.stop", "localstack/utils/analytics/event_publisher.py:AnalyticsEvent.payload", "localstack/utils/common.py:wait_for_port_open", "localstack/services/infra.py:start_secretsmanager", "localstack/utils/aws/aws_models.py:LambdaFunction.get_qualifier_version", "localstack/services/install.py:log_install_msg", "localstack/utils/common.py:JsonObject.__str__", "localstack/utils/cloudformation/template_deployer.py:retrieve_topic_arn", "localstack/services/sns/sns_listener.py:do_delete_topic", "localstack/utils/cloudformation/template_deployer.py:get_role_arn", "localstack/utils/aws/aws_stack.py:VelocityUtil.base64Decode", "localstack/utils/bootstrap.py:start_infra_locally", "localstack/services/sns/sns_listener.py:do_subscribe", "localstack/services/sns/sns_listener.py:_get_tags", "localstack/dashboard/infra.py:get_lambda_event_sources", "localstack/services/infra.py:restore_persisted_data", "localstack/services/sns/sns_listener.py:do_untag_resource", "localstack/utils/common.py:CaptureOutput._set", "localstack/services/firehose/firehose_api.py:serve", "localstack/services/sns/sns_listener.py:get_topic_by_arn", "localstack/services/infra.py:register_signal_handlers.signal_handler", "localstack/services/sns/sns_listener.py:get_subscription_by_arn", "localstack/utils/common.py:CaptureOutput.stdout", "localstack/utils/common.py:CaptureOutput.stderr", "localstack/utils/aws/aws_stack.py:get_s3_client", "localstack/services/infra.py:start_proxy_for_service", "localstack/services/s3/s3_listener.py:get_replication", "localstack/services/dynamodb/dynamodb_listener.py:ProxyListenerDynamoDB._thread_local", "localstack/utils/aws/aws_models.py:EventSource.filter_type", "localstack/services/infra.py:start_moto_server_separate", "localstack/services/generic_proxy.py:GenericProxyHandler.forward", "localstack/utils/aws/aws_stack.py:Environment.__str__", "localstack/dashboard/infra.py:cmd_sqs", "localstack/services/awslambda/lambda_executors.py:LambdaExecutor._execute", "localstack/services/generic_proxy.py:ProxyListener.get_forward_url", "localstack/utils/cloudwatch/cloudwatch_util.py:publish_lambda_result", "localstack/utils/analytics/event_publisher.py:AnalyticsEvent.timestamp", "localstack/utils/kinesis/kinesis_connector.py:KinesisProcessor.initialize", "localstack/dashboard/infra.py:cmd_es", "localstack/utils/cloudwatch/cloudwatch_util.py:publish_event", "localstack/services/infra.py:Plugin.check", "localstack/dashboard/infra.py:cmd_kinesis", "localstack/services/infra.py:Plugin.start", "localstack/dashboard/infra.py:get_dynamo_dbs.handle", "localstack/utils/common.py:is_port_open"]], "raw_output_loc": ["```\nlocalstack/services/generic_proxy.py\nline: 198-336\nclass: GenericProxyHandler\nfunction: forward\n\nlocalstack/services/generic_proxy.py\nline: 82-86\nclass: ProxyListener\nfunction: get_forward_url\n\nlocalstack/services/infra.py\nline: 49-71\nclass: Plugin\n\nlocalstack/services/infra.py\nline: 57-63\nclass: Plugin\nfunction: start\n\nlocalstack/config.py\nline: 257-258\nfunction: service_port\n\nlocalstack/utils/common.py\nline: 320-341\nfunction: is_port_open\n\nlocalstack/utils/common.py\nline: 344-351\nfunction: wait_for_port_open\n```\n"], "meta_data": {"repo": "localstack/localstack", "base_commit": "737ca72b7bce6e377dd6876eacee63338fa8c30c", "problem_statement": "ERROR:localstack.services.generic_proxy: Error forwarding request:\n\nStarting local dev environment. CTRL-C to quit.\r\nStarting mock API Gateway (http port 4567)...\r\nStarting mock DynamoDB (http port 4569)...\r\nStarting mock SES (http port 4579)...\r\nStarting mock Kinesis (http port 4568)...\r\nStarting mock Redshift (http port 4577)...\r\nStarting mock S3 (http port 4572)...\r\nStarting mock CloudWatch (http port 4582)...\r\nStarting mock CloudFormation (http port 4581)...\r\nStarting mock SSM (http port 4583)...\r\nStarting mock SQS (http port 4576)...\r\nStarting local Elasticsearch (http port 4571)...\r\nStarting mock SNS (http port 4575)...\r\nStarting mock DynamoDB Streams service (http port 4570)...\r\nStarting mock Firehose service (http port 4573)...\r\nStarting mock Route53 (http port 4580)...\r\nStarting mock ES service (http port 4578)...\r\nStarting mock Lambda service (http port 4574)...\r\n2018-08-11T13:33:08:ERROR:localstack.services.generic_proxy: Error forwarding request: HTTPConnectionPool(host='127.0.0.1', port=4564): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4d442415d0>: Failed to establish a new connection: [Errno 111] Connection refused',)) Traceback (most recent call last):\r\n  File \"/home/maruf/.local/lib/python2.7/site-packages/localstack/services/generic_proxy.py\", line 201, in forward\r\n    headers=forward_headers)\r\n  File \"/home/maruf/.local/lib/python2.7/site-packages/requests/api.py\", line 112, in post\r\n    return request('post', url, data=data, json=json, **kwargs)\r\n  File \"/home/maruf/.local/lib/python2.7/site-packages/requests/api.py\", line 58, in request\r\n    return session.request(method=method, url=url, **kwargs)\r\n  File \"/home/maruf/.local/lib/python2.7/site-packages/requests/sessions.py\", line 508, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"/home/maruf/.local/lib/python2.7/site-packages/requests/sessions.py\", line 618, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"/home/maruf/.local/lib/python2.7/site-packages/requests/adapters.py\", line 508, in send\r\n    raise ConnectionError(e, request=request)\r\nConnectionError: HTTPConnectionPool(host='127.0.0.1', port=4564): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4d442415d0>: Failed to establish a new connection: [Errno 111] Connection refused',))\r\n\r\n2018-08-11T13:34:08:ERROR:localstack.services.generic_proxy: Error forwarding request: HTTPConnectionPool(host='127.0.0.1', port=4564): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4d4425fa10>: Failed to establish a new connection: [Errno 111] Connection refused',)) Traceback (most recent call last):\r\n  File \"/home/maruf/.local/lib/python2.7/site-packages/localstack/services/generic_proxy.py\", line 201, in forward\r\n    headers=forward_headers)\r\n  File \"/home/maruf/.local/lib/python2.7/site-packages/requests/api.py\", line 112, in post\r\n    return request('post', url, data=data, json=json, **kwargs)\r\n  File \"/home/maruf/.local/lib/python2.7/site-packages/requests/api.py\", line 58, in request\r\n    return session.request(method=method, url=url, **kwargs)\r\n  File \"/home/maruf/.local/lib/python2.7/site-packages/requests/sessions.py\", line 508, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"/home/maruf/.local/lib/python2.7/site-packages/requests/sessions.py\", line 618, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"/home/maruf/.local/lib/python2.7/site-packages/requests/adapters.py\", line 508, in send\r\n    raise ConnectionError(e, request=request)\r\nConnectionError: HTTPConnectionPool(host='127.0.0.1', port=4564): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4d4425fa10>: Failed to establish a new connection: [Errno 111] Connection refused',))\r\n\r\n2018-08-11T13:35:09:ERROR:localstack.services.generic_proxy: Error forwarding request: HTTPConnectionPool(host='127.0.0.1', port=4564): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4d4429c3d0>: Failed to establish a new connection: [Errno 111] Connection refused',)) Traceback (most recent call last):\r\n  File \"/home/maruf/.local/lib/python2.7/site-packages/localstack/services/generic_proxy.py\", line 201, in forward\r\n    headers=forward_headers)\r\n  File \"/home/maruf/.local/lib/python2.7/site-packages/requests/api.py\", line 112, in post\r\n    return request('post', url, data=data, json=json, **kwargs)\r\n  File \"/home/maruf/.local/lib/python2.7/site-packages/requests/api.py\", line 58, in request\r\n    return session.request(method=method, url=url, **kwargs)\r\n  File \"/home/maruf/.local/lib/python2.7/site-packages/requests/sessions.py\", line 508, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"/home/maruf/.local/lib/python2.7/site-packages/requests/sessions.py\", line 618, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"/home/maruf/.local/lib/python2.7/site-packages/requests/adapters.py\", line 508, in send\r\n    raise ConnectionError(e, request=request)\r\nConnectionError: HTTPConnectionPool(host='127.0.0.1', port=4564): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4d4429c3d0>: Failed to establish a new connection: [Errno 111] Connection refused',))\r\n\r\n2018-08-11T13:36:09:ERROR:localstack.services.generic_proxy: Error forwarding request: HTTPConnectionPool(host='127.0.0.1', port=4564): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4d4425f910>: Failed to establish a new connection: [Errno 111] Connection refused',)) Traceback (most recent call last):\r\n  File \"/home/maruf/.local/lib/python2.7/site-packages/localstack/services/generic_proxy.py\", line 201, in forward\r\n    headers=forward_headers)\r\n  File \"/home/maruf/.local/lib/python2.7/site-packages/requests/api.py\", line 112, in post\r\n    return request('post', url, data=data, json=json, **kwargs)\r\n  File \"/home/maruf/.local/lib/python2.7/site-packages/requests/api.py\", line 58, in request\r\n    return session.request(method=method, url=url, **kwargs)\r\n  File \"/home/maruf/.local/lib/python2.7/site-packages/requests/sessions.py\", line 508, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"/home/maruf/.local/lib/python2.7/site-packages/requests/sessions.py\", line 618, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"/home/maruf/.local/lib/python2.7/site-packages/requests/adapters.py\", line 508, in send\r\n    raise ConnectionError(e, request=request)\r\nConnectionError: HTTPConnectionPool(host='127.0.0.1', port=4564): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4d4425f910>: Failed to establish a new connection: [Errno 111] Connection refused',))", "patch": ""}}
{"instance_id": "huggingface__transformers-30", "found_files": [[]], "found_modules": [[]], "found_entities": [[]], "raw_output_loc": [], "meta_data": {"repo": "huggingface/transformers", "base_commit": "d2871b29754abd0f72cf42c299bb1c041519f7bc", "problem_statement": "[Feature request] Add example of finetuning the pretrained models on custom corpus", "patch": ""}}
{"instance_id": "pandas-dev__pandas-11080", "found_files": [["pandas/core/frame.py", "pandas/core/index.py", "pandas/core/series.py"]], "found_modules": [["pandas/core/index.py:MultiIndex", "pandas/core/frame.py:DataFrame", "pandas/core/index.py:CategoricalIndex", "pandas/core/generic.py:NDFrame", "pandas/core/index.py:Index", "pandas/tseries/tdi.py:TimedeltaIndex", "asv_bench/benchmarks/frame_ctor.py:frame_ctor_dtindex_FY5253Quarterx1__variation_nearest", "pandas/stats/ols.py:MovingOLS", "pandas/tools/plotting.py:autocorrelation_plot", "asv_bench/benchmarks/frame_ctor.py:frame_ctor_dtindex_FY5253Quarterx2__variation_last", "pandas/core/series.py:Series", "pandas/io/pytables.py:GenericIndexCol", "pandas/io/stata.py:StataReader", "pandas/tools/plotting.py:LinePlot"]], "found_entities": [["pandas/core/index.py:MultiIndex.is_lexsorted", "pandas/core/frame.py:DataFrame.sort_index", "pandas/core/index.py:CategoricalIndex._format_attrs", "pandas/core/index.py:CategoricalIndex.inferred_type", "pandas/core/index.py:CategoricalIndex.values", "pandas/core/generic.py:NDFrame.notnull", "pandas/core/index.py:CategoricalIndex.categories", "pandas/core/index.py:CategoricalIndex.ordered", "pandas/core/index.py:Index.is_monotonic_increasing", "pandas/tseries/tdi.py:TimedeltaIndex.is_all_dates", "asv_bench/benchmarks/frame_ctor.py:frame_ctor_dtindex_FY5253Quarterx1__variation_nearest.get_index_for_offset", "pandas/stats/ols.py:MovingOLS._valid_indices", "pandas/core/index.py:Index.is_monotonic_decreasing", "pandas/tools/plotting.py:autocorrelation_plot.r", "asv_bench/benchmarks/frame_ctor.py:frame_ctor_dtindex_FY5253Quarterx2__variation_last.setup", "pandas/core/index.py:MultiIndex.sortlevel", "pandas/core/frame.py:DataFrame.combineAdd", "pandas/core/series.py:Series.sort_index", "pandas/io/pytables.py:GenericIndexCol.set_attr", "pandas/io/stata.py:StataReader.value_labels", "pandas/tools/plotting.py:LinePlot._plot"]], "raw_output_loc": ["```\npandas/core/frame.py\nclass: DataFrame\nfunction: sort_index\nlines: 3126-3169\n\npandas/core/index.py\nclass: Index\nfunction: is_monotonic_increasing\nlines: 784-789\n\npandas/core/index.py\nclass: Index\nfunction: is_monotonic_decreasing\nlines: 792-797\n\npandas/core/index.py\nclass: MultiIndex\nfunction: is_lexsorted\nlines: 4561-4565\n\npandas/core/index.py\nclass: MultiIndex\nfunction: sortlevel\nlines: 4991-5055\n\npandas/core/series.py\nclass: Series\nfunction: sort_index\nlines: 1661-1680\n```\n"], "meta_data": {"repo": "pandas-dev/pandas", "base_commit": "51a70dcb7133bc7cb8e6bea5da39a2cf58fa8319", "problem_statement": "PERF: checking is_monotonic_increasing/decreasing before sorting on an index\n\nWe don't keep the sortedness state in an index per-se, but it is rather cheap to check\n- `is_monotonic_increasing` or `is_monotonic_decreasing` on a reg-index \n- MultiIndex should check `is_lexsorted` (this might be done already)\n\n```\nIn [8]: df = DataFrame(np.random.randn(1000000,2),columns=list('AB'))\n\nIn [9]: %timeit df.sort_index()\n10 loops, best of 3: 37.1 ms per loop\n\nIn [10]: %timeit -n 1 -r 1 df.index.is_monotonic_increasing\n1 loops, best of 1: 2.01 ms per loop\n\nIn [11]: %timeit -n 1 -r 1 df.index.is_monotonic_increasin^C\nKeyboardInterrupt\n\nIn [11]: %timeit df.set_index('A').sort_index()\n10 loops, best of 3: 175 ms per loop\n\nIn [12]: %timeit -n 1 -r 1 df.set_index('A').index.is_monotonic_increasing\n1 loops, best of 1: 9.54 ms per loop\n```", "patch": ""}}
{"instance_id": "huggingface__transformers-9", "found_files": [[]], "found_modules": [[]], "found_entities": [[]], "raw_output_loc": [], "meta_data": {"repo": "huggingface/transformers", "base_commit": "9fef668338b15e508bac99598dd139546fece00b", "problem_statement": "Crash at the end of training\n\nHi, I tried running the Squad model this morning (on a single GPU with gradient accumulation over 3 steps) but after 3 hours of training, my job failed with the following output:\r\n\r\nI was running the code, unmodified, from commit 3bfbc21376af691b912f3b6256bbeaf8e0046ba8\r\n\r\nIs this an issue you know about?\r\n```\r\n11/08/2018 17:50:03 - INFO - __main__ -   device cuda n_gpu 1 distributed training False\r\n11/08/2018 17:50:18 - INFO - __main__ -   *** Example ***\r\n11/08/2018 17:50:18 - INFO - __main__ -   unique_id: 1000000000\r\n11/08/2018 17:50:18 - INFO - __main__ -   example_index: 0\r\n11/08/2018 17:50:18 - INFO - __main__ -   doc_span_index: 0\r\n11/08/2018 17:50:18 - INFO - __main__ -   tokens: [CLS] to whom did the virgin mary allegedly appear in 1858 in lou ##rdes france ? [SEP] architectural ##ly , the school has a catholic character . atop the main building ' s gold dome is a golden statue of the virgin mary . immediately in front of the main building and facing it , is a copper statue of christ with arms up ##rai ##sed with the legend \" ve ##ni ##te ad me om ##nes \" . next to the main building is the basilica of the sacred heart . immediately behind the basilica is the gr ##otto , a marian place of prayer and reflection . it is a replica of the gr ##otto at lou ##rdes , france where the virgin mary reputed ##ly appeared to saint bern ##ade ##tte so ##ub ##iro ##us in 1858 . at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ) , is a simple , modern stone statue of mary . [SEP]\r\n11/08/2018 17:50:18 - INFO - __main__ -   token_to_orig_map: 17:0 18:0 19:0 20:1 21:2 22:3 23:4 24:5 25:6 26:6 27:7 28:8 29:9 30:10 31:10 32:10 33:11 34:12 35:13 36:14 37:15 38:16 39:17 40:18 41:19 42:20 43:20 44:21 45:22 46:23 47:24 48:25 49:26 50:27 51:28 52:29 53:30 54:30 55:31 56:32 57:33 58:34 59:35 60:36 61:37 62:38 63:39 64:39 65:39 66:40 67:41 68:42 69:43 70:43 71:43 72:43 73:44 74:45 75:46 76:46 77:46 78:46 79:47 80:48 81:49 82:50 83:51 84:52 85:53 86:54 87:55 88:56 89:57 90:58 91:58 92:59 93:60 94:61 95:62 96:63 97:64 98:65 99:65 100:65 101:66 102:67 103:68 104:69 105:70 106:71 107:72 108:72 109:73 110:74 111:75 112:76 113:77 114:78 115:79 116:79 117:80 118:81 119:81 120:81 121:82 122:83 123:84 124:85 125:86 126:87 127:87 128:88 129:89 130:90 131:91 132:91 133:91 134:92 135:92 136:92 137:92 138:93 139:94 140:94 141:95 142:96 143:97 144:98 145:99 146:100 147:101 148:102 149:102 150:103 151:104 152:105 153:106 154:107 155:108 156:109 157:110 158:111 159:112 160:113 161:114 162:115 163:115 164:115 165:116 166:117 167:118 168:118 169:119 170:120 171:121 172:122 173:123 174:123\r\n11/08/2018 17:50:18 - INFO - __main__ -   token_is_max_context: 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True\r\n11/08/2018 17:50:18 - INFO - __main__ -   input_ids: 101 2000 3183 2106 1996 6261 2984 9382 3711 1999 8517 1999 10223 26371 2605 1029 102 6549 2135 1010 1996 2082 2038 1037 3234 2839 1012 10234 1996 2364 2311 1005 1055 2751 8514 2003 1037 3585 6231 1997 1996 6261 2984 1012 3202 1999 2392 1997 1996 2364 2311 1998 5307 2009 1010 2003 1037 6967 6231 1997 4828 2007 2608 2039 14995 6924 2007 1996 5722 1000 2310 3490 2618 4748 2033 18168 5267 1000 1012 2279 2000 1996 2364 2311 2003 1996 13546 1997 1996 6730 2540 1012 3202 2369 1996 13546 2003 1996 24665 23052 1010 1037 14042 2173 1997 7083 1998 9185 1012 2009 2003 1037 15059 1997 1996 24665 23052 2012 10223 26371 1010 2605 2073 1996 6261 2984 22353 2135 2596 2000 3002 16595 9648 4674 2061 12083 9711 2271 1999 8517 1012 2012 1996 2203 1997 1996 2364 3298 1006 1998 1999 1037 3622 2240 2008 8539 2083 1017 11342 1998 1996 2751 8514 1007 1010 2003 1037 3722 1010 2715 2962 6231 1997 2984 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n11/08/2018 17:50:18 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n\r\n... [truncated] ...\r\n\r\nIteration: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 29314/29324 [3:27:55<00:04,  2.36it/s]\u001b[A\r\n\r\nIteration: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 29315/29324 [3:27:55<00:03,  2.44it/s]\u001b[A\r\n\r\nIteration: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 29316/29324 [3:27:56<00:03,  2.26it/s]\u001b[A\r\n\r\nIteration: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 29317/29324 [3:27:56<00:02,  2.35it/s]\u001b[A\r\n\r\nIteration: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 29318/29324 [3:27:56<00:02,  2.44it/s]\u001b[A\r\n\r\nIteration: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 29319/29324 [3:27:57<00:02,  2.25it/s]\u001b[A\r\n\r\nIteration: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 29320/29324 [3:27:57<00:01,  2.35it/s]\u001b[A\r\n\r\nIteration: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 29321/29324 [3:27:58<00:01,  2.41it/s]\u001b[A\r\n\r\nIteration: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 29322/29324 [3:27:58<00:00,  2.25it/s]\u001b[A\r\n\r\nIteration: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 29323/29324 [3:27:59<00:00,  2.36it/s]\u001b[ATraceback (most recent call last):\r\n  File \"code/run_squad.py\", line 929, in <module>\r\n    main()\r\n  File \"code/run_squad.py\", line 862, in main\r\n    loss = model(input_ids, segment_ids, input_mask, start_positions, end_positions)\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 477, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/0x0d4ff90d01fa4168983197b17d73bb0c_dependencies/code/modeling.py\", line 467, in forward\r\n    start_loss = loss_fct(start_logits, start_positions)\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 477, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\", line 862, in forward\r\n    ignore_index=self.ignore_index, reduction=self.reduction)\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\", line 1550, in cross_entropy\r\n    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\", line 1403, in nll_loss\r\n    if input.size(0) != target.size(0):\r\nRuntimeError: dimension specified as 0 but tensor has no dimensions\r\n\r\nException ignored in: <bound method tqdm.__del__ of Iteration: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 29323/29324 [3:27:59<00:00,  2.36it/s]>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tqdm/_tqdm.py\", line 931, in __del__\r\n    self.close()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tqdm/_tqdm.py\", line 1133, in close\r\n    self._decr_instances(self)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tqdm/_tqdm.py\", line 496, in _decr_instances\r\n    cls.monitor.exit()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tqdm/_monitor.py\", line 52, in exit\r\n    self.join()\r\n  File \"/usr/lib/python3.6/threading.py\", line 1053, in join\r\n    raise RuntimeError(\"cannot join current thread\")\r\nRuntimeError: cannot join current thread\r\n```", "patch": ""}}
{"instance_id": "pallets__flask-593", "found_files": [[]], "found_modules": [[]], "found_entities": [[]], "raw_output_loc": [], "meta_data": {"repo": "pallets/flask", "base_commit": "85dce2c836fe03aefc07b7f4e0aec575e170f1cd", "problem_statement": "Nestable blueprints\n\nI'd like to be able to register \"sub-blueprints\" using `Blueprint.register_blueprint(*args, **kwargs)`. This would register the nested blueprints with an app when the \"parent\" is registered with it. All parameters are preserved, other than `url_prefix`, which is handled similarly to in `add_url_rule`. A na\u00edve implementation could look like this:\n\n``` python\nclass Blueprint(object):\n    ...\n\n    def register_blueprint(self, blueprint, **options):\n        def deferred(state):\n            url_prefix = options.get('url_prefix')\n            if url_prefix is None:\n                url_prefix = blueprint.url_prefix\n            if 'url_prefix' in options:\n                del options['url_prefix']\n\n            state.app.register_blueprint(blueprint, url_prefix, **options)\n        self.record(deferred)\n```", "patch": ""}}
{"instance_id": "scikit-learn__scikit-learn-26948", "found_files": [["doc/conf.py", "doc/sphinxext/github_link.py", "doc/sphinxext/add_toctree_functions.py", "doc/tutorial/machine_learning_map/pyparsing.py", "doc/conftest.py"]], "found_modules": [["sklearn/_loss/loss.py:HalfPoissonLoss", "doc/tutorial/machine_learning_map/pyparsing.py:ParseResults"]], "found_entities": [["sklearn/_loss/loss.py:HalfPoissonLoss.constant_to_optimal_zero", "doc/tutorial/machine_learning_map/pyparsing.py:ParseResults.copy"]], "raw_output_loc": ["```\ndoc/conf.py\n# Critical configuration file for building HTML docs and likely contains logic for custom buttons and links in the documentation UI\n\ndoc/sphinxext/github_link.py\n# Sphinx extension likely involved in generating GitHub links, often includes logic for copy buttons or user actions on doc pages\n\ndoc/sphinxext/add_toctree_functions.py\n# Manages custom UI elements added to Sphinx-built HTML, might touch install instructions or interface with copy actions\n\ndoc/tutorial/machine_learning_map/pyparsing.py\nline: 733\nfunction: ParseResults.copy\n# Contains copy logic, though likely unrelated to the copy button on the install page, but relevant for any document-wide copy utilities\n\ndoc/conftest.py\n# May be involved in setting up documentation tests and UI elements including plugins handling copy actions or install instruction demos\n```\n"], "meta_data": {"repo": "scikit-learn/scikit-learn", "base_commit": "96b5814de70ad2435b6db5f49b607b136921f701", "problem_statement": "The copy button on install copies an extensive comman including env activation\n\n### Describe the issue linked to the documentation\n\nhttps://scikit-learn.org/stable/install.html\r\n\r\nAbove link will lead you to the sklearn downlanding for link . \r\nwhen you link copy link button it will copy \r\n`python3 -m venv sklearn-venvpython -m venv sklearn-venvpython -m venv sklearn-venvsource sklearn-venv/bin/activatesource sklearn-venv/bin/activatesklearn-venv\\Scripts\\activatepip install -U scikit-learnpip install -U scikit-learnpip install -U scikit-learnpip3 install -U scikit-learnconda create -n sklearn-env -c conda-forge scikit-learnconda activate sklearn-env`\r\n\r\ninstead of  `pip3 install -U scikit-learn`\r\n\r\nif this is the issue so please issue i want to create a pull request for it and tell in which file this issue reside\r\nThanks\n\n### Suggest a potential alternative/fix\n\nBy resoving above issue", "patch": ""}}
{"instance_id": "scikit-learn__scikit-learn-26590", "found_files": [["sklearn/impute/_knn.py", "sklearn/impute/_base.py"]], "found_modules": [["sklearn/ensemble/_base.py:BaseEnsemble", "sklearn/_loss/link.py:HalfLogitLink", "asv_benchmarks/benchmarks/common.py:Predictor", "sklearn/feature_selection/_rfe.py:RFE", "sklearn/utils/_array_api.py:_ArrayAPIWrapper", "doc/tutorial/machine_learning_map/pyparsing.py:ParseBaseException", "asv_benchmarks/benchmarks/linear_model.py:LassoBenchmark", "asv_benchmarks/benchmarks/common.py:Transformer", "sklearn/impute/_knn.py:KNNImputer", "benchmarks/bench_plot_nmf.py:_PGNMF", "sklearn/_loss/link.py:MultinomialLogit", "sklearn/utils/_metadata_requests.py:MethodMetadataRequest", "sklearn/metrics/_scorer.py:_BaseScorer", "sklearn/utils/validation.py:_is_arraylike", "sklearn/utils/_param_validation.py:_Constraint", "sklearn/utils/_array_api.py:_NumPyAPIWrapper", "doc/tutorial/machine_learning_map/pyparsing.py:RecursiveGrammarException", "doc/tutorial/machine_learning_map/pyparsing.py:_ParseResultsWithOffset", "sklearn/utils/_param_validation.py:_InstancesOf", "sklearn/utils/_param_validation.py:_NoneConstraint", "sklearn/mixture/_base.py:BaseMixture", "sklearn/utils/_param_validation.py:_PandasNAConstraint", "sklearn/utils/_encode.py:_NaNCounter", "sklearn/gaussian_process/kernels.py:Kernel", "sklearn/utils/_mocking.py:NoSampleWeightWrapper", "examples/miscellaneous/plot_metadata_routing.py:RouterConsumerClassifier", "examples/applications/plot_cyclical_feature_engineering.py:sin_transformer", "sklearn/base.py:BaseEstimator", "sklearn/linear_model/_base.py:LinearModel", "sklearn/externals/_packaging/version.py:Version", "sklearn/model_selection/_search_successive_halving.py:BaseSuccessiveHalving", "sklearn/utils/_mocking.py:_MockEstimatorOnOffPrediction", "sklearn/ensemble/_voting.py:VotingClassifier", "sklearn/neighbors/_classification.py:KNeighborsClassifier", "doc/sphinxext/sphinx_issues.py:IssueRole", "examples/gaussian_process/plot_gpr_on_structured_data.py:SequenceKernel", "asv_benchmarks/benchmarks/ensemble.py:GradientBoostingClassifierBenchmark", "asv_benchmarks/benchmarks/decomposition.py:MiniBatchDictionaryLearningBenchmark", "sklearn/utils/parallel.py:delayed", "examples/ensemble/plot_bias_variance.py:f", "asv_benchmarks/benchmarks/ensemble.py:HistGradientBoostingClassifierBenchmark", "examples/applications/plot_out_of_core_classification.py:ReutersParser", "examples/compose/plot_column_transformer.py:text_stats", "sklearn/cluster/_bicluster.py:BaseSpectral", "asv_benchmarks/benchmarks/linear_model.py:LinearRegressionBenchmark", "sklearn/cluster/_kmeans.py:_BaseKMeans", "sklearn/calibration.py:_SigmoidCalibration", "sklearn/preprocessing/_label.py:MultiLabelBinarizer", "sklearn/ensemble/_gb_losses.py:ExponentialLoss", "doc/tutorial/machine_learning_map/pyparsing.py:_defaultStartDebugAction", "doc/tutorial/machine_learning_map/pyparsing.py:_defaultSuccessDebugAction", "doc/tutorial/machine_learning_map/pyparsing.py:_defaultExceptionDebugAction", "sklearn/pipeline.py:Pipeline", "doc/tutorial/machine_learning_map/pyparsing.py:_trim_arity", "sklearn/naive_bayes.py:ComplementNB", "sklearn/utils/_metadata_requests.py:MetadataRouter"]], "found_entities": [["sklearn/ensemble/_base.py:BaseEnsemble.__getitem__", "sklearn/_loss/link.py:HalfLogitLink.inverse", "asv_benchmarks/benchmarks/common.py:Predictor.peakmem_predict", "sklearn/feature_selection/_rfe.py:RFE._estimator_type", "sklearn/utils/_array_api.py:_ArrayAPIWrapper.isdtype", "doc/tutorial/machine_learning_map/pyparsing.py:ParseBaseException.__repr__", "asv_benchmarks/benchmarks/common.py:Predictor.params", "asv_benchmarks/benchmarks/linear_model.py:LassoBenchmark.setup_cache", "asv_benchmarks/benchmarks/common.py:Transformer.time_transform", "doc/tutorial/machine_learning_map/pyparsing.py:ParseBaseException.__dir__", "sklearn/impute/_knn.py:KNNImputer.fit", "benchmarks/bench_plot_nmf.py:_PGNMF.fit_transform", "asv_benchmarks/benchmarks/linear_model.py:LassoBenchmark.make_scorers", "asv_benchmarks/benchmarks/common.py:Transformer.params", "sklearn/_loss/link.py:MultinomialLogit.symmetrize_raw_prediction", "sklearn/utils/_metadata_requests.py:MethodMetadataRequest.requests", "sklearn/_loss/link.py:MultinomialLogit.link", "sklearn/metrics/_scorer.py:_BaseScorer._factory_args", "sklearn/utils/validation.py:_is_arraylike", "sklearn/utils/_param_validation.py:_Constraint.__str__", "sklearn/utils/_array_api.py:_NumPyAPIWrapper.bool", "doc/tutorial/machine_learning_map/pyparsing.py:RecursiveGrammarException.__str__", "doc/tutorial/machine_learning_map/pyparsing.py:_ParseResultsWithOffset.__getitem__", "sklearn/utils/_param_validation.py:_InstancesOf.__str__", "doc/tutorial/machine_learning_map/pyparsing.py:_ParseResultsWithOffset.__repr__", "doc/tutorial/machine_learning_map/pyparsing.py:_ParseResultsWithOffset.setOffset", "sklearn/utils/_array_api.py:_NumPyAPIWrapper.unique_inverse", "sklearn/utils/_array_api.py:_NumPyAPIWrapper.unique_counts", "sklearn/utils/_param_validation.py:_NoneConstraint.__str__", "sklearn/utils/_array_api.py:_NumPyAPIWrapper.unique_values", "sklearn/mixture/_base.py:BaseMixture._get_parameters", "sklearn/mixture/_base.py:BaseMixture._set_parameters", "sklearn/utils/_array_api.py:_NumPyAPIWrapper.isdtype", "sklearn/utils/_param_validation.py:_PandasNAConstraint.__str__", "sklearn/utils/_encode.py:_NaNCounter.__missing__", "sklearn/gaussian_process/kernels.py:Kernel.__mul__", "sklearn/utils/_mocking.py:NoSampleWeightWrapper.predict_proba", "sklearn/utils/_mocking.py:NoSampleWeightWrapper._more_tags", "sklearn/gaussian_process/kernels.py:Kernel.__rmul__", "examples/miscellaneous/plot_metadata_routing.py:RouterConsumerClassifier.predict", "examples/applications/plot_cyclical_feature_engineering.py:sin_transformer", "sklearn/base.py:BaseEstimator._more_tags", "sklearn/linear_model/_base.py:LinearModel.fit", "sklearn/impute/_knn.py:KNNImputer.transform", "sklearn/externals/_packaging/version.py:Version.dev", "sklearn/gaussian_process/kernels.py:Kernel.__repr__", "sklearn/model_selection/_search_successive_halving.py:BaseSuccessiveHalving._generate_candidate_params", "sklearn/utils/_mocking.py:_MockEstimatorOnOffPrediction.predict", "sklearn/ensemble/_voting.py:VotingClassifier._check_voting", "sklearn/neighbors/_classification.py:KNeighborsClassifier._more_tags", "sklearn/utils/_mocking.py:_MockEstimatorOnOffPrediction.predict_proba", "sklearn/impute/_knn.py:KNNImputer.get_feature_names_out", "doc/sphinxext/sphinx_issues.py:IssueRole.default_format_text", "examples/gaussian_process/plot_gpr_on_structured_data.py:SequenceKernel.diag", "asv_benchmarks/benchmarks/ensemble.py:GradientBoostingClassifierBenchmark.make_scorers", "examples/gaussian_process/plot_gpr_on_structured_data.py:SequenceKernel.is_stationary", "asv_benchmarks/benchmarks/decomposition.py:MiniBatchDictionaryLearningBenchmark.make_scorers", "sklearn/utils/parallel.py:delayed.delayed_function", "examples/ensemble/plot_bias_variance.py:f", "asv_benchmarks/benchmarks/ensemble.py:HistGradientBoostingClassifierBenchmark.setup_cache", "examples/applications/plot_out_of_core_classification.py:ReutersParser.start_title", "examples/compose/plot_column_transformer.py:text_stats", "sklearn/cluster/_bicluster.py:BaseSpectral._check_parameters", "examples/applications/plot_out_of_core_classification.py:ReutersParser.start_body", "examples/applications/plot_out_of_core_classification.py:ReutersParser.end_body", "asv_benchmarks/benchmarks/linear_model.py:LinearRegressionBenchmark.setup_cache", "examples/applications/plot_out_of_core_classification.py:ReutersParser.start_topics", "sklearn/cluster/_kmeans.py:_BaseKMeans._validate_center_shape", "sklearn/calibration.py:_SigmoidCalibration.predict", "sklearn/preprocessing/_label.py:MultiLabelBinarizer._more_tags", "sklearn/ensemble/_gb_losses.py:ExponentialLoss._raw_prediction_to_decision", "doc/tutorial/machine_learning_map/pyparsing.py:_defaultStartDebugAction", "doc/tutorial/machine_learning_map/pyparsing.py:_defaultSuccessDebugAction", "doc/tutorial/machine_learning_map/pyparsing.py:_defaultExceptionDebugAction", "sklearn/pipeline.py:Pipeline._more_tags", "doc/tutorial/machine_learning_map/pyparsing.py:_trim_arity.extract_stack", "doc/tutorial/machine_learning_map/pyparsing.py:_trim_arity.extract_tb", "sklearn/naive_bayes.py:ComplementNB._count", "sklearn/utils/_metadata_requests.py:MetadataRouter.__repr__"]], "raw_output_loc": ["```\nsklearn/impute/_knn.py\nline: 203-240\nclass: KNNImputer\nfunction: fit\n\nsklearn/impute/_knn.py\nline: 242-367\nclass: KNNImputer\nfunction: transform\n\nsklearn/impute/_knn.py\nline: 369-392\nclass: KNNImputer\nfunction: get_feature_names_out\n\nsklearn/impute/_base.py\nline: 90-98, 100-111, 113-126\nclass: _BaseImputer\nfunctions: _fit_indicator, _transform_indicator, _concatenate_indicator\n\nsklearn/impute/_base.py\nline: 929-948, 950-989, 1017-1046\nclass: MissingIndicator\nfunctions: fit, transform, get_feature_names_out\n```\n"], "meta_data": {"repo": "scikit-learn/scikit-learn", "base_commit": "e04b8e70e60df88751af5cd667cafb66dc32b397", "problem_statement": "KNNImputer add_indicator fails to persist where missing data had been present in training\n\n### Describe the bug\r\n\r\nHello, I've encountered an issue where the KNNImputer fails to record the fields where there were missing data at the time when `.fit` is called, but not recognised if `.transform` is called on a dense matrix. I would have expected it to return a 2x3 matrix rather than 2x2, with `missingindicator_A = False` for all cases.\r\n\r\nReproduction steps below. Any help much appreciated :)\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```python\r\n>>> import pandas as pd\r\n>>> from sklearn.impute import KNNImputer\r\n>>> knn = KNNImputer(add_indicator=True)\r\n>>> df = pd.DataFrame({'A': [0, None], 'B': [1, 2]})\r\n>>> df\r\n     A  B\r\n0  0.0  1\r\n1  NaN  2\r\n>>> knn.fit(df)\r\nKNNImputer(add_indicator=True)\r\n>>> pd.DataFrame(knn.transform(df), columns=knn.get_feature_names_out())\r\n     A    B  missingindicator_A\r\n0  0.0  1.0                 0.0\r\n1  0.0  2.0                 1.0\r\n>>> df['A'] = 0\r\n>>> pd.DataFrame(knn.transform(df), columns=knn.get_feature_names_out())\r\n```\r\n\r\n### Expected Results\r\n\r\n```\r\n     A    B  missingindicator_A\r\n0  0.0  1.0                 0.0\r\n1  0.0  2.0                 0.0\r\n```\r\n\r\n### Actual Results\r\n\r\n```pytb\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nCell In[30], line 1\r\n----> 1 pd.DataFrame(knn.transform(df), columns=knn.get_feature_names_out())\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:694, in DataFrame.__init__(self, data, index, columns, dtype, copy)\r\n    684         mgr = dict_to_mgr(\r\n    685             # error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\r\n    686             # attribute \"name\"\r\n   (...)\r\n    691             typ=manager,\r\n    692         )\r\n    693     else:\r\n--> 694         mgr = ndarray_to_mgr(\r\n    695             data,\r\n    696             index,\r\n    697             columns,\r\n    698             dtype=dtype,\r\n    699             copy=copy,\r\n    700             typ=manager,\r\n    701         )\r\n    703 # For data is list-like, or Iterable (will consume into list)\r\n    704 elif is_list_like(data):\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/pandas/core/internals/construction.py:351, in ndarray_to_mgr(values, index, columns, dtype, copy, typ)\r\n    346 # _prep_ndarray ensures that values.ndim == 2 at this point\r\n    347 index, columns = _get_axes(\r\n    348     values.shape[0], values.shape[1], index=index, columns=columns\r\n    349 )\r\n--> 351 _check_values_indices_shape_match(values, index, columns)\r\n    353 if typ == \"array\":\r\n    355     if issubclass(values.dtype.type, str):\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/pandas/core/internals/construction.py:422, in _check_values_indices_shape_match(values, index, columns)\r\n    420 passed = values.shape\r\n    421 implied = (len(index), len(columns))\r\n--> 422 raise ValueError(f\"Shape of passed values is {passed}, indices imply {implied}\")\r\n\r\nValueError: Shape of passed values is (2, 2), indices imply (2, 3)\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\npython3, sklearn = 1.2.1\r\n```", "patch": ""}}
{"instance_id": "scikit-learn__scikit-learn-29294", "found_files": [[]], "found_modules": [[]], "found_entities": [[]], "raw_output_loc": [], "meta_data": {"repo": "scikit-learn/scikit-learn", "base_commit": "2707099b23a0a8580731553629566c1182d26f48", "problem_statement": "ConvergenceWarnings cannot be turned off\n\nHi, I'm unable to turn off convergence warnings from `GraphicalLassoCV`.\r\n\r\nI've tried most of the solutions from, and none of them worked (see below for actual implementations):\r\nhttps://stackoverflow.com/questions/879173/how-to-ignore-deprecation-warnings-in-python\r\nhttps://stackoverflow.com/questions/32612180/eliminating-warnings-from-scikit-learn/33812427#33812427\r\nhttps://stackoverflow.com/questions/53968004/how-to-silence-all-sklearn-warning\r\nhttps://stackoverflow.com/questions/14463277/how-to-disable-python-warnings\r\n\r\nContrary to what the designers of the sklearn's exceptions must have thought when it was implemented, some of us actually use stdout to log important information of the host program for diagnostics purposes.  Flooding it with garbage that cannot be turned off, as is in the case with cross-validation, is not ok. \r\n\r\nTo briefly speak to the severity of the issue, the above sklearn-specific questions relating to suppressing warnings have been viewed ~500K times with combined ~400 upvotes, and dates back 7 years. \r\n\r\nI've tried the following (`n_jobs` parameter does not appear to affect the result):\r\n\r\n```py\r\nfrom sklearn.covariance import GraphicalLassoCV\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\r\n\r\nmodel = GraphicalLassoCV(n_jobs=4)\r\nmodel = model.fit(data)\r\n```\r\n\r\n```py\r\nfrom sklearn.covariance import GraphicalLassoCV\r\nimport warnings\r\nwarnings.filterwarnings(action='ignore')\r\n\r\nmodel = GraphicalLassoCV(n_jobs=4)\r\nmodel = model.fit(data)\r\n```\r\n\r\n```py\r\nimport warnings\r\nwith warnings.catch_warnings():\r\n    warnings.simplefilter(\"ignore\", ConvergenceWarning)\r\n\r\n    model = GraphicalLassoCV(n_jobs=4)\r\n    model = model.fit(data)\r\n```\r\n\r\n```py\r\nfrom sklearn.covariance import GraphicalLassoCV\r\ndef warn(*args, **kwargs):\r\n    pass\r\nimport warnings\r\nwarnings.warn = warn\r\n\r\nmodel = GraphicalLassoCV(n_jobs=4)\r\nmodel = model.fit(data)\r\n```\r\n\r\n```py\r\nimport contextlib\r\nimport os, sys\r\n\r\n@contextlib.contextmanager\r\ndef suppress_stdout():\r\n    with open(os.devnull, 'w') as fnull:\r\n        old_stdout = sys.stdout\r\n        sys.stdout = fnull\r\n        try:\r\n            yield\r\n        finally:\r\n            sys.stdout = old_stdout\r\n\r\nwith suppress_stdout():\r\n    model = GraphicalLassoCV(n_jobs=4)\r\n    model = model.fit(data)\r\n```\r\n\r\n```py\r\nimport logging\r\nlogging.captureWarnings(True)\r\n\r\nlogging.getLogger(\"py.warnings\").setLevel(logging.ERROR)\r\n\r\nmodel = GraphicalLassoCV(n_jobs=4)\r\nmodel = model.fit(data)\r\n```", "patch": ""}}
{"instance_id": "scikit-learn__scikit-learn-19248", "found_files": [[]], "found_modules": [[]], "found_entities": [[]], "raw_output_loc": [], "meta_data": {"repo": "scikit-learn/scikit-learn", "base_commit": "23d8761615d0417eef5f52cc796518e44d41ca2a", "problem_statement": "Birch should be called BIRCH\n\nC.f. the original paper.\r\nZhang, T.; Ramakrishnan, R.; Livny, M. (1996). \"BIRCH: an efficient data clustering method for very large databases\". Proceedings of the 1996 ACM SIGMOD international conference on Management of data - SIGMOD '96. pp. 103\u2013114. doi:10.1145/233269.233324", "patch": ""}}
{"instance_id": "localstack__localstack-402", "found_files": [["localstack/services/dynamodbstreams/dynamodbstreams_api.py", "localstack/services/dynamodbstreams/__init__.py", "localstack/services/dynamodb/dynamodb_listener.py", "localstack/utils/aws/aws_models.py", "localstack/dashboard/infra.py"]], "found_modules": [["localstack/utils/aws/aws_models.py:KinesisStream", "localstack/dashboard/infra.py:get_kinesis_shards", "localstack/dashboard/infra.py:get_kinesis_streams"]], "found_entities": [["localstack/utils/aws/aws_models.py:KinesisStream.describe", "localstack/dashboard/infra.py:get_kinesis_shards", "localstack/dashboard/infra.py:get_kinesis_streams"]], "raw_output_loc": ["```\nlocalstack/services/dynamodbstreams/dynamodbstreams_api.py\nfunction: (any handling ShardId or describe_stream)\n\nlocalstack/services/dynamodbstreams/__init__.py\nfunction: (any reference to ShardId)\n\nlocalstack/services/dynamodb/dynamodb_listener.py\nfunction: (any reference to ShardId)\n\nlocalstack/utils/aws/aws_models.py\nclass: KinesisShard\nfunction: KinesisStream.describe\n\nlocalstack/dashboard/infra.py\nfunction: get_kinesis_shards\nfunction: get_kinesis_streams\n```\n"], "meta_data": {"repo": "localstack/localstack", "base_commit": "65b807e4e95fe6da3e30f13e4271dc9dcfaa334e", "problem_statement": "Dynamodbstreams Use Kinesis Shard Identifiers\n\n<!-- Love localstack? Please consider supporting our collective:\r\n\ud83d\udc49  https://opencollective.com/localstack/donate -->\r\n\r\nDynamodbstreams seem to be making use of Kinesis shard identifiers which are considered invalid by botocore request validators.\r\n\r\nError response from boto3 when attempting to `get_shard_iterator` from shard ids returned from `describe_stream`:\r\n\r\n```\r\n[test-integration:L51:27s] exception = ParamValidationError(u'Parameter validation failed:\\nInvalid length for parameter ShardId, value: 20, valid range: 28-inf',)\r\n[test-integration:L52:27s]\r\n[test-integration:L53:27s]     def _reraise_exception(self, exception):\r\n[test-integration:L54:27s]         if hasattr(exception, 'response'):\r\n[test-integration:L55:27s]             code = exception.response['Error']['Code']\r\n[test-integration:L56:27s]\r\n[test-integration:L57:27s]             if code == 'TrimmedDataAccessException':\r\n[test-integration:L58:27s]                 raise TrimmedRecordsException()\r\n[test-integration:L59:27s]             elif code == 'ResourceNotFoundException':\r\n[test-integration:L60:27s]                 raise ResourceDNEException()\r\n[test-integration:L61:27s]\r\n[test-integration:L62:27s] >       raise exception\r\n[test-integration:L63:27s] E       ParamValidationError: Parameter validation failed:\r\n[test-integration:L64:27s] E       Invalid length for parameter ShardId, value: 20, valid range: 28-inf\r\n[test-integration:L65:27s]\r\n[test-integration:L66:27s] .tox/py27/lib/python2.7/site-packages/pyrokinesis/dynamodbstreams_ingress_backend.py:111: ParamValidationError\r\n```\r\n\r\nThe following is the response object I am getting back when I `describe_stream` on the stream's ARN:\r\n\r\n```\r\n[test-integration:L68:27s] {'ResponseMetadata': {'HTTPStatusCode': 200, 'RetryAttempts': 0, 'HTTPHeaders': {'content-length': '692', 'access-control-allow-origin': '*', 'date': 'Fri, 13 Oct 2017 12:47:00 GMT', 'server': 'Werkzeug/0.12.2 Python/2.7.13', 'content-type': 'application/json'}}, u'StreamDescription': {u'StreamLabel': u'TODO', u'StreamArn': u'arn:aws:dynamodb:us-east-1:000000000000:table/DynamoTest/stream/2017-10-13T12:47:00', u'Shards': [{u'ShardId': u'shardId-000000000000', u'SequenceNumberRange': {u'StartingSequenceNumber': u'49577893583130519883135457518096755974321873497073123330'}}], u'KeySchema': [{u'KeyType': u'HASH', u'AttributeName': u'ID'}], u'TableName': u'DynamoTest', u'StreamStatus': u'ENABLED'}}\r\n```\r\n\r\nMy localstack setup:\r\n\r\n```\r\nlocalstack 0.7.3\r\n\r\n[localstack:L2:1s] 2017-10-13 15:10:35,915 INFO spawned: 'dashboard' with pid 13\r\n[localstack:L3:1s] 2017-10-13 15:10:35,917 INFO spawned: 'infra' with pid 14\r\n[localstack:L4:1s] (. .venv/bin/activate; bin/localstack web --port=8080)\r\n[localstack:L5:1s] (. .venv/bin/activate; exec bin/localstack start)\r\n[localstack:L6:1s] Starting local dev environment. CTRL-C to quit.\r\n[localstack:L7:1s]  * Running on http://0.0.0.0:8080/ (Press CTRL+C to quit)\r\n[localstack:L8:1s]  * Restarting with stat\r\n[localstack:L9:1s] Starting mock Kinesis (http port 4568)...\r\n[localstack:L10:1s] Starting mock S3 (http port 4572)...\r\n[localstack:L11:1s] Starting mock DynamoDB (http port 4569)...\r\n[localstack:L12:1s]  * Debugger is active!\r\n[localstack:L13:2s]  * Debugger PIN: 281-540-735\r\n[localstack:L14:2s] 2017-10-13 15:10:37,123 INFO success: dashboard entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)\r\n[localstack:L15:2s] 2017-10-13 15:10:37,123 INFO success: infra entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)\r\n[localstack:L16:2s] Starting mock DynamoDB Streams service (http port 4570)...\r\n[localstack:L17:2s] Listening at http://:::4565\r\n[localstack:L18:2s] Initializing DynamoDB Local with the following configuration:\r\n[localstack:L19:2s] Port:\t4564\r\n[localstack:L20:2s] InMemory:\tfalse\r\n[localstack:L21:2s] DbPath:\t/tmp/localstack/dynamodb\r\n[localstack:L22:2s] SharedDb:\ttrue\r\n[localstack:L23:2s] shouldDelayTransientStatuses:\tfalse\r\n[localstack:L24:2s] CorsParams:\t*\r\n[localstack:L25:2s]\r\n[localstack:L26:2s] * Running on http://0.0.0.0:4563/ (Press CTRL+C to quit)\r\n```", "patch": ""}}
{"instance_id": "pallets__flask-602", "found_files": [[]], "found_modules": [[]], "found_entities": [[]], "raw_output_loc": [], "meta_data": {"repo": "pallets/flask", "base_commit": "ee76129812419d473eb62434051e81d5855255b6", "problem_statement": "Misspelling in docs @ flask.Flask.handle_exception\n\n`Default exception handling that kicks in when an exception occours that is not caught. In debug mode the exception will be re-raised immediately, otherwise it is logged and the handler for a 500 internal server error is used. If no such handler exists, a default 500 internal server error message is displayed.`\n\nOccours should be occurs.\n\nI looked around in the project code to see if i could update this, but it looks like the docs subdir is no longer used? I could be wrong, if you let me know where this is at I'll update it and send a PR :)", "patch": ""}}
{"instance_id": "pandas-dev__pandas-37494", "found_files": [[]], "found_modules": [[]], "found_entities": [[]], "raw_output_loc": [], "meta_data": {"repo": "pandas-dev/pandas", "base_commit": "862cd05df4452592a99dd1a4fa10ce8cfb3766f7", "problem_statement": "ENH: improve the resulting dtype for groupby operations on nullable dtypes\n\nFollow-up on https://github.com/pandas-dev/pandas/pull/37433, and partly related to https://github.com/pandas-dev/pandas/issues/37493\r\n\r\nCurrently, after groupby operations we try to cast back to the original dtype when possible (at least in case of extension arrays). But this is not always correct, and also not done consistently. Some examples using the test case from the mentioned PR using a nullable Int64 column as input:\r\n\r\n```\r\nIn [1]: df = DataFrame(\r\n   ...:     {\r\n   ...:         \"A\": [\"A\", \"B\"] * 5,\r\n   ...:         \"B\": pd.array([1, 2, 3, 4, 5, 6, 7, 8, 9, pd.NA], dtype=\"Int64\"),\r\n   ...:     }\r\n   ...: )\r\n\r\nIn [2]: df.groupby(\"A\")[\"B\"].sum()\r\nOut[2]: \r\nA\r\nA    25\r\nB    20\r\nName: B, dtype: Int64\r\n\r\nIn [3]: df.groupby(\"A\")[\"B\"].std()\r\nOut[3]: \r\nA\r\nA    3.162278\r\nB    2.581989\r\nName: B, dtype: float64\r\n\r\nIn [4]: df.groupby(\"A\")[\"B\"].mean()\r\nOut[4]: \r\nA\r\nA    5\r\nB    5\r\nName: B, dtype: Int64\r\n\r\nIn [5]: df.groupby(\"A\")[\"B\"].count()\r\nOut[5]: \r\nA\r\nA    5\r\nB    4\r\nName: B, dtype: int64\r\n```\r\n\r\nSo some observations:\r\n\r\n* For `sum()`, we correctly have Int64 for the result\r\n* For `std()`, we could use the nullable Float64 instead of float64 dtype\r\n* For `mean()`, we incorrectly cast back to Int64 dtype, as the result of mean should always be floating (in this case the casting just happened to work because the means were rounded numbers)\r\n* For `count()`, we did not create a nullable Int64 dtype for the result, while this could be done in the input is nullable", "patch": ""}}
{"instance_id": "scikit-learn__scikit-learn-16730", "found_files": [[]], "found_modules": [[]], "found_entities": [[]], "raw_output_loc": [], "meta_data": {"repo": "scikit-learn/scikit-learn", "base_commit": "eaf0a044fdc084ebeeb9bbfbcf42e6df2b1491bb", "problem_statement": "BUG: MLE for PCA mis-estimates rank\n\nAfter #16224 it looks like this code no longer produces the correct result:\r\n```\r\nimport numpy as np\r\nfrom sklearn.decomposition import PCA\r\nn_samples, n_dim = 1000, 10\r\nX = np.random.RandomState(0).randn(n_samples, n_dim)\r\nX[:, -1] = np.mean(X[:, :-1], axis=-1)  # true X dim is ndim - 1\r\npca_skl = PCA('mle', svd_solver='full')\r\npca_skl.fit(X)\r\nassert pca_skl.n_components_ == n_dim - 1\r\n```\r\nBefore #16224 this passed (`n_components_ == 9`) but after #16224 it gives 8. Not sure why this would happen given the singular value spectrum looks good:\r\n```\r\nimport matplotlib.pyplot as plt\r\ns = np.linalg.svdvals(X)\r\nplt.stem(s)\r\n```\r\n![Figure_1](https://user-images.githubusercontent.com/2365790/77180767-c4f62a00-6aa0-11ea-8dc8-99c6dc137a71.png)\r\n\r\nMaybe an off-by-one error somewhere?\r\n\r\ncc'ing @lschwetlick since it was your PR", "patch": ""}}
{"instance_id": "pallets__flask-2813", "found_files": [["src/flask/app.py", "src/flask/helpers.py", "src/flask/wrappers.py", "src/flask/ctx.py"]], "found_modules": [["src/flask/app.py:Flask", "src/flask/sansio/app.py:App", "src/flask/ctx.py:RequestContext", "src/flask/sansio/blueprints.py:Blueprint.app_template_filter", "src/flask/cli.py:shell_command", "src/flask/json/tag.py:TagMarkup", "src/flask/json/tag.py:TagDateTime", "src/flask/sansio/scaffold.py:Scaffold", "src/flask/sansio/blueprints.py:Blueprint", "src/flask/helpers.py:url_for"]], "found_entities": [["src/flask/app.py:Flask.url_for", "src/flask/sansio/app.py:App._check_setup_finished", "src/flask/ctx.py:RequestContext.__exit__", "src/flask/sansio/blueprints.py:Blueprint.app_template_filter.decorator", "src/flask/app.py:Flask.create_url_adapter", "src/flask/cli.py:shell_command", "src/flask/json/tag.py:TagMarkup.to_python", "src/flask/json/tag.py:TagDateTime.check", "src/flask/json/tag.py:TagDateTime.to_json", "src/flask/json/tag.py:TagDateTime.to_python", "src/flask/sansio/scaffold.py:Scaffold.__repr__", "src/flask/sansio/scaffold.py:Scaffold._check_setup_finished", "src/flask/sansio/blueprints.py:Blueprint.record", "src/flask/helpers.py:url_for"]], "raw_output_loc": ["```\nsrc/flask/app.py\nline: 423-461\nclass: Flask\nfunction: create_url_adapter\n\nsrc/flask/app.py\nline: 988-1112\nclass: Flask\nfunction: url_for\n\nsrc/flask/helpers.py\nline: 188-239\nfunction: url_for\n\nsrc/flask/wrappers.py\nclass: Request\n\nsrc/flask/ctx.py\nline: 238-284\nclass: AppContext\n```\n"], "meta_data": {"repo": "pallets/flask", "base_commit": "07c7d5730a2685ef2281cc635e289685e5c3d478", "problem_statement": "Allow flexible routing with SERVER_NAME config\n\n### Expected Behavior\r\n\r\nDeployed a flask application which is reachable over multiple domains and ports:\r\n- external via load balancer: `client - Host: example.org -> LB -> flask app`\r\n- internal via DNS service discovery without load balancer: `client - Host: instance-1231.example.org -> flask app` \r\n\r\nIf the client connects directly (`Host: instance-1231.example.org`) the app should be able to return absolute and stable URLs like `http://example.org/path/to/my/view` as the URL (`http://instance-1231.example.org/path/to/my/view`) with the internal DNS name is ephemeral.\r\nTherefore I configured the `SERVER_NAME` config key and `url_for` generates the intended absolute URL by using `_external=True` within and without request context. But the app should be still able to route requests coming with `Host: instance-1231.example.org`.\r\n\r\n### Actual Behavior\r\n\r\nFlasks creates the `werkzeug.routing.MapAdapter` with `server_name=app.config['SERVER_NAME']` and therefore no view method will match to incoming requests with `Host: instance-1231.example.org`.\r\n\r\n### Environment\r\n\r\n* Python version: 2.7.13 (I'm sorry)\r\n* Flask version: 1.0.2\r\n* Werkzeug version: 0.14.1\r\n\r\n### Applied workaround:\r\n\r\nOverwrite `Flask.create_url_adapter` and create `MapAdapter` for request context without `server_name` parameter. Routing and URL generation works fine.", "patch": ""}}
{"instance_id": "pandas-dev__pandas-46804", "found_files": [[]], "found_modules": [[]], "found_entities": [[]], "raw_output_loc": [], "meta_data": {"repo": "pandas-dev/pandas", "base_commit": "a8968bfa696d51f73769c54f2630a9530488236a", "problem_statement": "DOC: building page for nested methods doesn't work\n\nThe following\r\n```\r\npython make.py --single pandas.Series.str.rsplit\r\n```\r\nfails to produce the docs:\r\n```\r\n(pandas-dev) marcogorelli@OVMG025 doc % python make.py clean && python make.py --single pandas.Series.str.rsplit\r\nRunning Sphinx v4.4.0\r\nloading translations [en]... done\r\nmaking output directory... done\r\n[autosummary] generating autosummary for: index.rst\r\n[autosummary] generating autosummary for: /Users/marcogorelli/pandas-dev/doc/source/reference/api/pandas.Series.str.rsplit.rst\r\nbuilding [mo]: targets for 0 po files that are out of date\r\nbuilding [html]: targets for 1 source files that are out of date\r\nupdating environment: [new config] 2 added, 0 changed, 0 removed\r\nreading sources... [100%] reference/api/pandas.Series.str.rsplit                                                                        \r\nWARNING: autodoc: failed to import method 'str.rsplit' from module 'Series'; the following exception was raised:\r\nNo module named 'Series'\r\nlooking for now-outdated files... none found\r\npickling environment... done\r\nchecking consistency... done\r\npreparing documents... done\r\n/Users/marcogorelli/pandas-dev/doc/source/index.rst:44: WARNING: 'any' reference target not found: getting_started\r\n/Users/marcogorelli/pandas-dev/doc/source/index.rst:60: WARNING: 'any' reference target not found: user_guide\r\n/Users/marcogorelli/pandas-dev/doc/source/index.rst:77: WARNING: 'any' reference target not found: api\r\n/Users/marcogorelli/pandas-dev/doc/source/index.rst:94: WARNING: 'any' reference target not found: development\r\nwriting output... [100%] reference/api/pandas.Series.str.rsplit                                                                         \r\nwaiting for workers...\r\ngenerating indices... genindex py-modindex done\r\nwriting additional pages... search done\r\ncopying images... [100%] _static/index_contribute.svg                                                                                   \r\ncopying static files... done\r\ncopying extra files... done\r\ndumping search index in English (code: en)... done\r\ndumping object inventory... done\r\nbuild succeeded, 5 warnings.\r\n```\r\n\r\nHowever, it works just fine to do\r\n```\r\npython make.py --single pandas.Series.value_counts\r\n```\r\n\r\nI haven't figured out how to address this, so opening an issue for now", "patch": ""}}
{"instance_id": "pandas-dev__pandas-33428", "found_files": [["pandas/plotting/_misc.py", "doc/source/conf.py", "pandas/plotting/_matplotlib/misc.py", "pandas/conftest.py"]], "found_modules": [["pandas/conftest.py:iris", "asv_bench/benchmarks/timeseries.py:ToDatetimeCache", "pandas/plotting/_misc.py:parallel_coordinates", "pandas/io/clipboard/__init__.py:CheckedCall", "pandas/plotting/_matplotlib/misc.py:parallel_coordinates"]], "found_entities": [["pandas/conftest.py:iris", "asv_bench/benchmarks/timeseries.py:ToDatetimeCache.time_unique_seconds_and_unit", "pandas/plotting/_misc.py:parallel_coordinates", "pandas/io/clipboard/__init__.py:CheckedCall.__setattr__", "pandas/plotting/_matplotlib/misc.py:parallel_coordinates"]], "raw_output_loc": ["```\npandas/plotting/_misc.py\nline: 361\nfunction: parallel_coordinates\n\ndoc/source/reference/api/pandas.plotting.parallel_coordinates.rst\n(all lines referencing: \"https://raw.github.com/pandas-dev/pandas/master/pandas/tests/data/csv/iris.csv\")\n\ndoc/source/conf.py\n(all possible doc-link/static-link rewriting logic)\n\npandas/plotting/_matplotlib/misc.py\nline: 307\nfunction: parallel_coordinates\n\npandas/conftest.py\nline: 734\nfunction: iris\n```"], "meta_data": {"repo": "pandas-dev/pandas", "base_commit": "e88c39225ef545123860c679822f1b567fe65c27", "problem_statement": "DOC: Data links in Pandas API Reference are broken 404\n\n#### Location of the documentation\r\n\r\nhttps://pandas.pydata.org/docs/reference/api/pandas.plotting.parallel_coordinates.html\r\n...probably many examples in other sections\r\n\r\n#### Documentation problem\r\n\r\nResults in 404 not found error\r\ndf = pd.read_csv('https://raw.github.com/pandas-dev/pandas/master'\r\n                    '/pandas/tests/data/csv/iris.csv')\r\n\r\n#### Suggested fix for documentation\r\n\r\nThe GitHub site should be \"raw.githubusercontent.com\"", "patch": ""}}
{"instance_id": "pandas-dev__pandas-17200", "found_files": [["pandas/io/json/json.py", "pandas/io/common.py", "pandas/io/s3.py"]], "found_modules": [["asv_bench/benchmarks/groupby.py:groupby_nth", "pandas/core/window.py:_Window", "asv_bench/benchmarks/stat_ops.py:stats_rolling_mean", "pandas/core/indexes/accessors.py:TimedeltaProperties", "asv_bench/benchmarks/offset.py:SemiMonthOffset", "pandas/compat/__init__.py:iteritems", "asv_bench/benchmarks/timeseries.py:ResampleSeries", "pandas/core/config.py:DictWrapper", "asv_bench/benchmarks/sparse.py:sparse_arithmetic_block", "pandas/plotting/_converter.py:PandasAutoDateLocator", "asv_bench/benchmarks/indexing.py:IndexerLookup", "pandas/core/sparse/series.py:SparseSeries", "pandas/core/indexes/category.py:CategoricalIndex", "pandas/plotting/_converter.py:MilliSecondLocator", "pandas/core/indexes/interval.py:IntervalIndex", "pandas/core/computation/pytables.py:ConditionBinOp", "pandas/core/internals.py:Block", "pandas/tseries/offsets.py:DateOffset", "pandas/core/indexes/period.py:PeriodIndex", "asv_bench/benchmarks/timeseries.py:TimeSeries", "asv_bench/benchmarks/frame_methods.py:Apply", "asv_bench/benchmarks/gil.py:nogil_rolling_algos_fast.time_nogil_rolling_skew", "pandas/core/groupby.py:Grouper", "pandas/core/sparse/array.py:SparseArray", "pandas/core/common.py:groupby", "asv_bench/benchmarks/groupby.py:groupby_multi", "asv_bench/benchmarks/frame_methods.py:frame_dtypes", "pandas/core/groupby.py:GroupByPlot.__call__", "pandas/core/dtypes/dtypes.py:CategoricalDtype", "asv_bench/benchmarks/timeseries.py:ToDatetime", "asv_bench/benchmarks/gil.py:nogil_read_csv", "pandas/core/computation/ops.py:Op", "asv_bench/benchmarks/packers.py:HDF", "pandas/core/computation/pytables.py:BinOp", "pandas/core/config.py:CallableDynamicDoc", "pandas/core/base.py:SelectionMixin", "pandas/core/indexes/numeric.py:UInt64Index", "asv_bench/benchmarks/indexing.py:MultiIndexing", "pandas/core/panel.py:Panel", "pandas/core/indexes/range.py:RangeIndex", "pandas/plotting/_core.py:MPLPlot", "asv_bench/benchmarks/timeseries.py:AsOf", "asv_bench/benchmarks/groupby.py:groupby_multi_count", "asv_bench/benchmarks/gil.py:nogil_datetime_fields.time_datetime_field_normalize", "asv_bench/benchmarks/packers.py:MsgPack", "asv_bench/benchmarks/gil.py:nogil_datetime_fields.time_datetime_to_period", "asv_bench/benchmarks/groupby.py:groupby_int_count", "asv_bench/benchmarks/gil.py:nogil_datetime_fields.time_period_to_datetime", "asv_bench/benchmarks/packers.py:Pickle", "asv_bench/benchmarks/indexing.py:IntervalIndexing", "asv_bench/benchmarks/timeseries.py:AsOfDataFrame", "asv_bench/benchmarks/packers.py:SQL", "asv_bench/benchmarks/indexing.py:PanelIndexing", "asv_bench/benchmarks/frame_methods.py:Dropna", "pandas/io/json/json.py:read_json", "pandas/io/json/json.py:JsonReader", "asv_bench/benchmarks/groupby.py:GroupBySuite", "pandas/core/categorical.py:Categorical", "setup.py:pxd", "pandas/io/common.py:get_filepath_or_buffer", "asv_bench/benchmarks/timeseries.py:ResampleDataFrame", "asv_bench/benchmarks/packers.py:packers_read_sas", "asv_bench/benchmarks/rolling.py:SeriesRolling", "pandas/io/common.py:_get_handle", "asv_bench/benchmarks/groupby.py:groupby_size", "asv_bench/benchmarks/frame_methods.py:Equals", "doc/sphinxext/numpydoc/compiler_unparse.py:UnparseCompilerAst", "asv_bench/benchmarks/gil.py:nogil_factorize", "pandas/core/groupby.py:_GroupBy", "pandas/io/html.py:_BeautifulSoupHtml5LibFrameParser", "setup.py:DummyBuildSrc", "asv_bench/benchmarks/frame_methods.py:Interpolate", "doc/sphinxext/numpydoc/docscrape.py:header", "pandas/io/s3.py:get_filepath_or_buffer", "asv_bench/benchmarks/strings.py:StringMethods"]], "found_entities": [["asv_bench/benchmarks/groupby.py:groupby_nth.time_groupby_frame_nth_none", "pandas/core/window.py:_Window._get_window", "asv_bench/benchmarks/stat_ops.py:stats_rolling_mean.time_rolling_max", "pandas/core/indexes/accessors.py:TimedeltaProperties.to_pytimedelta", "asv_bench/benchmarks/offset.py:SemiMonthOffset.time_end_apply", "asv_bench/benchmarks/offset.py:SemiMonthOffset.time_end_incr", "pandas/compat/__init__.py:iteritems", "asv_bench/benchmarks/timeseries.py:ResampleSeries.time_timestamp_downsample_mean", "pandas/core/config.py:DictWrapper.__dir__", "asv_bench/benchmarks/sparse.py:sparse_arithmetic_block.time_sparse_division_zero", "pandas/plotting/_converter.py:PandasAutoDateLocator._get_unit", "asv_bench/benchmarks/indexing.py:IndexerLookup.setup", "pandas/core/sparse/series.py:SparseSeries.__len__", "pandas/core/indexes/category.py:CategoricalIndex.tolist", "pandas/plotting/_converter.py:MilliSecondLocator._get_unit", "pandas/core/indexes/interval.py:IntervalIndex._engine", "pandas/core/computation/pytables.py:ConditionBinOp.format", "pandas/core/internals.py:Block.ftype", "pandas/tseries/offsets.py:DateOffset.__ne__", "pandas/core/indexes/period.py:PeriodIndex._engine", "pandas/tseries/offsets.py:DateOffset.__hash__", "asv_bench/benchmarks/timeseries.py:TimeSeries.time_add_irregular", "asv_bench/benchmarks/frame_methods.py:Apply.time_apply_pass_thru", "asv_bench/benchmarks/gil.py:nogil_rolling_algos_fast.time_nogil_rolling_skew.run", "asv_bench/benchmarks/frame_methods.py:Apply.time_apply_ref_by_name", "pandas/core/groupby.py:Grouper.groups", "pandas/core/sparse/array.py:SparseArray.__unicode__", "pandas/core/common.py:groupby.__iter__", "asv_bench/benchmarks/groupby.py:groupby_multi.time_groupby_multi_cython", "asv_bench/benchmarks/frame_methods.py:frame_dtypes.time_frame_dtypes", "pandas/core/groupby.py:GroupByPlot.__call__.f", "asv_bench/benchmarks/groupby.py:groupby_multi.time_groupby_multi_series_op", "pandas/core/dtypes/dtypes.py:CategoricalDtype.ordered", "asv_bench/benchmarks/timeseries.py:ToDatetime.time_format_YYYYMMDD", "asv_bench/benchmarks/groupby.py:groupby_multi.time_groupby_series_simple_rank", "pandas/core/indexes/category.py:CategoricalIndex.argsort", "asv_bench/benchmarks/timeseries.py:ToDatetime.time_iso8601_nosep", "asv_bench/benchmarks/timeseries.py:ToDatetime.time_iso8601_format", "asv_bench/benchmarks/gil.py:nogil_read_csv.create_cols", "asv_bench/benchmarks/offset.py:SemiMonthOffset.time_end_decr_rng", "pandas/core/computation/ops.py:Op.isscalar", "asv_bench/benchmarks/packers.py:HDF.time_write_hdf_table", "pandas/core/computation/pytables.py:BinOp.convert_values", "pandas/core/config.py:CallableDynamicDoc.__call__", "pandas/core/base.py:SelectionMixin.ndim", "pandas/core/indexes/numeric.py:UInt64Index.asi8", "pandas/core/sparse/series.py:SparseSeries.block", "asv_bench/benchmarks/indexing.py:MultiIndexing.time_multiindex_large_get_loc", "pandas/core/panel.py:Panel._init_arrays", "pandas/core/indexes/range.py:RangeIndex.is_monotonic_decreasing", "pandas/plotting/_core.py:MPLPlot.draw", "pandas/core/sparse/series.py:SparseSeries.sp_values", "pandas/core/indexes/range.py:RangeIndex.tolist", "asv_bench/benchmarks/timeseries.py:AsOf.time_asof_nan_single", "pandas/core/sparse/series.py:SparseSeries.npoints", "asv_bench/benchmarks/groupby.py:groupby_multi_count.time_groupby_multi_count", "pandas/core/groupby.py:Grouper.ax", "asv_bench/benchmarks/gil.py:nogil_datetime_fields.time_datetime_field_normalize.run", "asv_bench/benchmarks/packers.py:MsgPack.setup", "asv_bench/benchmarks/gil.py:nogil_datetime_fields.time_datetime_to_period.run", "asv_bench/benchmarks/groupby.py:groupby_int_count.time_groupby_int_count", "asv_bench/benchmarks/gil.py:nogil_datetime_fields.time_period_to_datetime.run", "asv_bench/benchmarks/packers.py:Pickle.setup", "asv_bench/benchmarks/indexing.py:IntervalIndexing.time_getitem_list", "asv_bench/benchmarks/packers.py:Pickle.time_write_pickle", "asv_bench/benchmarks/indexing.py:IntervalIndexing.time_loc_list", "pandas/core/base.py:SelectionMixin.aggregate", "asv_bench/benchmarks/timeseries.py:AsOfDataFrame.time_asof_nan_single", "asv_bench/benchmarks/packers.py:SQL.time_write_sql", "asv_bench/benchmarks/indexing.py:PanelIndexing.time_subset", "asv_bench/benchmarks/frame_methods.py:Dropna.time_dropna_axis1_any_mixed_dtypes", "pandas/io/json/json.py:read_json", "pandas/io/json/json.py:JsonReader.read", "asv_bench/benchmarks/groupby.py:GroupBySuite.time_max", "pandas/core/categorical.py:Categorical.itemsize", "asv_bench/benchmarks/groupby.py:GroupBySuite.time_mean", "asv_bench/benchmarks/groupby.py:GroupBySuite.time_median", "setup.py:pxd", "pandas/io/common.py:get_filepath_or_buffer", "asv_bench/benchmarks/timeseries.py:ResampleDataFrame.time_mean_numpy", "asv_bench/benchmarks/packers.py:packers_read_sas.time_read_xport", "asv_bench/benchmarks/timeseries.py:ResampleDataFrame.time_mean_string", "asv_bench/benchmarks/rolling.py:SeriesRolling.time_rolling_count_l", "asv_bench/benchmarks/timeseries.py:ResampleDataFrame.time_min_numpy", "pandas/core/window.py:_Window._dir_additions", "pandas/io/common.py:_get_handle", "asv_bench/benchmarks/timeseries.py:ToDatetime.time_iso8601_format_no_sep", "pandas/core/indexes/category.py:CategoricalIndex.is_unique", "asv_bench/benchmarks/timeseries.py:ToDatetime.time_format_exact", "pandas/core/indexes/period.py:PeriodIndex._shallow_copy_with_infer", "pandas/tseries/offsets.py:DateOffset.rule_code", "asv_bench/benchmarks/groupby.py:groupby_size.time_groupby_multi_size", "pandas/plotting/_converter.py:MilliSecondLocator._get_interval", "asv_bench/benchmarks/groupby.py:groupby_size.time_groupby_dt_timegrouper_size", "asv_bench/benchmarks/timeseries.py:ToDatetime.time_cache_false_with_dup_seconds_and_unit", "asv_bench/benchmarks/timeseries.py:ToDatetime.time_cache_true_with_dup_string_dates", "asv_bench/benchmarks/frame_methods.py:Equals.time_frame_object_unequal", "asv_bench/benchmarks/timeseries.py:ToDatetime.time_cache_false_with_dup_string_dates", "doc/sphinxext/numpydoc/compiler_unparse.py:UnparseCompilerAst._Power", "pandas/tseries/offsets.py:DateOffset._offset_str", "asv_bench/benchmarks/gil.py:nogil_factorize.factorize_strings", "pandas/core/groupby.py:_GroupBy.__len__", "pandas/io/html.py:_BeautifulSoupHtml5LibFrameParser._text_getter", "asv_bench/benchmarks/timeseries.py:ToDatetime.time_cache_false_with_dup_string_tzoffset_dates", "setup.py:DummyBuildSrc.initialize_options", "setup.py:DummyBuildSrc.finalize_options", "pandas/io/html.py:_BeautifulSoupHtml5LibFrameParser._parse_th", "setup.py:DummyBuildSrc.run", "asv_bench/benchmarks/frame_methods.py:Interpolate.time_interpolate_some_good_infer", "pandas/io/html.py:_BeautifulSoupHtml5LibFrameParser._parse_thead", "doc/sphinxext/numpydoc/docscrape.py:header", "pandas/io/s3.py:get_filepath_or_buffer", "asv_bench/benchmarks/strings.py:StringMethods.time_center"]], "raw_output_loc": ["```\npandas/io/json/json.py\nline: 182-366\nfunction: read_json\n\npandas/io/json/json.py\nclass: JsonReader\nfunction: JsonReader.read\nline: 456-467\n\npandas/io/common.py\nfunction: get_filepath_or_buffer\nline: 171-213\n\npandas/io/common.py\nfunction: _get_handle\nline: 291-418\n\npandas/io/s3.py\nfunction: get_filepath_or_buffer\nline: 21-35\n```\n"], "meta_data": {"repo": "pandas-dev/pandas", "base_commit": "674fb96b33c07c680844f674fcdf0767b6e3c2f9", "problem_statement": "read_json(lines=True) broken for s3 urls in Python 3 (v0.20.3)\n\n#### Code Sample, a copy-pastable example if possible\r\n\r\nUsing Python\r\n```python\r\nimport pandas as pd\r\ninputdf = pd.read_json(path_or_buf=\"s3://path/to/python-lines/file.json\", lines=True)\r\n```\r\n\r\nThe file is similar to:\r\n```\r\n{\"url\": \"blah\", \"other\": \"blah\"}\r\n{\"url\": \"blah\", \"other\": \"blah\"}\r\n{\"url\": \"blah\", \"other\": \"blah\"}\r\n```\r\n\r\n#### Problem description\r\n\r\nWhen attempting to read a python lines file into a DataFrame using the s3 protocol, the above code will error with:\r\n\r\n```\r\n2017-08-08 11:06:14,225 - image_rank_csv - ERROR - initial_value must be str or None, not bytes\r\nTraceback (most recent call last):\r\n  File \"image_rank_csv.py\", line 62, in run\r\n    inputdf = pd.read_json(path_or_buf=\"s3://path/to/python-lines/file.json\", lines=True)\r\n  File \"...env/lib/python3.6/site-packages/pandas/io/json/json.py\", line 347, in read_json\r\n    lines = list(StringIO(json.strip()))\r\nTypeError: initial_value must be str or None, not bytes\r\n```\r\n\r\nThis works fine if the file is local, e.g.:\r\n```python\r\nimport pandas as pd\r\ninputdf = pd.read_json(path_or_buf=\"/local/path/to/python-lines/file.json\", lines=True)\r\n```\r\n\r\n#### Expected Output\r\n\r\nExpect to successfully read the file and error above not to occur.\r\n\r\nMy current thinking is that when we get the file handle: https://github.com/pandas-dev/pandas/blob/v0.20.3/pandas/io/json/json.py#L333 , you delegate to `s3fs`, which documents that [it only operates in Binary mode](http://s3fs.readthedocs.io/en/latest/#limitations). Therefore when you `read()`: https://github.com/pandas-dev/pandas/blob/v0.20.3/pandas/io/json/json.py#L335, Therefore passing to `StringIO` will fail here: https://github.com/pandas-dev/pandas/blob/v0.20.3/pandas/io/json/json.py#L347 . Maybe it needs a different handler for `BytesIO`?\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n[paste the output of ``pd.show_versions()`` here below this line]\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.1.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 16.6.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.20.3\r\npytest: None\r\npip: 9.0.1\r\nsetuptools: 36.2.7\r\nCython: None\r\nnumpy: 1.12.0\r\nscipy: 0.19.1\r\nxarray: None\r\nIPython: None\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: 2.6.2 (dt dec pq3 ext lo64)\r\njinja2: None\r\ns3fs: 0.1.2\r\npandas_gbq: None\r\npandas_datareader: None\r\n```\r\n</details>", "patch": ""}}
{"instance_id": "pandas-dev__pandas-39636", "found_files": [["pandas/core/series.py", "pandas/core/aggregation.py"]], "found_modules": [["pandas/core/series.py:Series", "pandas/io/formats/latex.py:GenericTableBuilder", "pandas/core/internals/array_manager.py:ArrayManager", "pandas/plotting/_matplotlib/converter.py:MilliSecondLocator", "pandas/io/stata.py:_datetime_to_stata_elapsed_vec.parse_dates_safe", "pandas/core/strings/object_array.py:ObjectStringArrayMixin", "pandas/tseries/frequencies.py:_is_multiple", "pandas/core/arrays/categorical.py:Categorical", "pandas/io/excel/_base.py:BaseExcelReader", "asv_bench/benchmarks/frame_methods.py:Dtypes", "pandas/io/formats/info.py:TableBuilderAbstract", "pandas/core/apply.py:FrameApply", "asv_bench/benchmarks/timeseries.py:DatetimeAccessor", "pandas/plotting/_matplotlib/core.py:MPLPlot", "asv_bench/benchmarks/groupby.py:GroupByMethods", "asv_bench/benchmarks/frame_methods.py:Equals", "pandas/core/groupby/groupby.py:GroupByPlot.__call__", "pandas/core/indexes/category.py:CategoricalIndex", "pandas/core/aggregation.py:transform", "pandas/core/internals/array_manager.py:ArrayManager.copy", "asv_bench/benchmarks/groupby.py:Categories", "pandas/io/formats/info.py:DataFrameTableBuilderNonVerbose", "pandas/core/indexes/base.py:Index", "pandas/core/aggregation.py:transform_str_or_callable", "pandas/io/formats/info.py:DataFrameTableBuilder", "pandas/core/arrays/sparse/array.py:SparseArray", "asv_bench/benchmarks/groupby.py:Float32", "pandas/io/excel/_openpyxl.py:OpenpyxlReader", "pandas/core/frame.py:DataFrame", "pandas/core/arrays/period.py:PeriodArray", "asv_bench/benchmarks/frame_methods.py:Interpolate", "pandas/core/frame.py:DataFrame._constructor_expanddim", "pandas/core/computation/ops.py:isnumeric", "asv_bench/benchmarks/frame_methods.py:Shift", "pandas/core/aggregation.py:transform_dict_like", "pandas/core/generic.py:NDFrame"]], "found_entities": [["pandas/core/series.py:Series.transform", "pandas/io/formats/latex.py:GenericTableBuilder.top_separator", "pandas/core/internals/array_manager.py:ArrayManager.astype", "pandas/plotting/_matplotlib/converter.py:MilliSecondLocator._get_interval", "pandas/io/stata.py:_datetime_to_stata_elapsed_vec.parse_dates_safe.f", "pandas/core/strings/object_array.py:ObjectStringArrayMixin._str_title", "pandas/tseries/frequencies.py:_is_multiple", "pandas/core/strings/object_array.py:ObjectStringArrayMixin._str_lower", "pandas/io/stata.py:_datetime_to_stata_elapsed_vec.parse_dates_safe.g", "pandas/core/arrays/categorical.py:Categorical._from_sequence", "pandas/io/excel/_base.py:BaseExcelReader.get_sheet_by_index", "asv_bench/benchmarks/frame_methods.py:Dtypes.setup", "pandas/io/formats/info.py:TableBuilderAbstract.non_null_counts", "asv_bench/benchmarks/frame_methods.py:Dtypes.time_frame_dtypes", "pandas/core/apply.py:FrameApply.result_index", "asv_bench/benchmarks/timeseries.py:DatetimeAccessor.time_dt_accessor_day_name", "pandas/plotting/_matplotlib/core.py:MPLPlot._make_plot", "pandas/core/internals/array_manager.py:ArrayManager.is_mixed_type", "asv_bench/benchmarks/timeseries.py:DatetimeAccessor.time_dt_accessor_year", "asv_bench/benchmarks/groupby.py:GroupByMethods.time_dtype_as_field", "asv_bench/benchmarks/frame_methods.py:Equals.time_frame_float_unequal", "pandas/core/groupby/groupby.py:GroupByPlot.__call__.f", "asv_bench/benchmarks/frame_methods.py:Equals.time_frame_nonunique_equal", "pandas/core/indexes/category.py:CategoricalIndex._maybe_cast_indexer", "pandas/core/aggregation.py:transform", "pandas/core/internals/array_manager.py:ArrayManager.copy.copy_func", "asv_bench/benchmarks/groupby.py:Categories.time_groupby_nosort", "pandas/io/formats/info.py:DataFrameTableBuilderNonVerbose.add_columns_summary_line", "asv_bench/benchmarks/groupby.py:Categories.time_groupby_ordered_sort", "pandas/core/indexes/base.py:Index._constructor", "pandas/core/indexes/category.py:CategoricalIndex._is_comparable_dtype", "asv_bench/benchmarks/groupby.py:Categories.time_groupby_extra_cat_sort", "asv_bench/benchmarks/groupby.py:Categories.time_groupby_extra_cat_nosort", "pandas/core/aggregation.py:transform_str_or_callable", "asv_bench/benchmarks/frame_methods.py:Equals.time_frame_object_unequal", "pandas/core/apply.py:FrameApply.values", "pandas/core/apply.py:FrameApply.agg_axis", "pandas/io/formats/info.py:DataFrameTableBuilder._fill_non_empty_info", "pandas/core/arrays/sparse/array.py:SparseArray.__setitem__", "asv_bench/benchmarks/groupby.py:Float32.time_sum", "pandas/io/excel/_openpyxl.py:OpenpyxlReader.sheet_names", "pandas/core/frame.py:DataFrame._constructor", "pandas/core/arrays/period.py:PeriodArray._box_func", "asv_bench/benchmarks/frame_methods.py:Interpolate.time_interpolate", "asv_bench/benchmarks/frame_methods.py:Interpolate.time_interpolate_some_good", "pandas/core/frame.py:DataFrame._constructor_expanddim.constructor", "pandas/core/computation/ops.py:isnumeric", "asv_bench/benchmarks/frame_methods.py:Shift.setup", "pandas/core/aggregation.py:transform_dict_like", "pandas/core/generic.py:NDFrame._check_inplace_and_allows_duplicate_labels", "pandas/core/indexes/base.py:Index._join_non_unique", "pandas/core/generic.py:NDFrame._is_view", "pandas/core/frame.py:DataFrame._series", "pandas/core/generic.py:NDFrame.reindex_like", "pandas/core/series.py:Series.apply"]], "raw_output_loc": ["```\npandas/core/series.py\nline: 3974-3977\nclass: Series\nfunction: transform\n\npandas/core/aggregation.py\nline: 404-467\nfunction: transform\n\npandas/core/aggregation.py\nline: 514-532\nfunction: transform_str_or_callable\n\npandas/core/aggregation.py\nline: 470-511\nfunction: transform_dict_like\n\npandas/core/series.py\nline: 3979-4083\nclass: Series\nfunction: apply\n```"], "meta_data": {"repo": "pandas-dev/pandas", "base_commit": "d558bce8e9d5d4adfb0ab587be20b8a231dd1eea", "problem_statement": "BUG: ValueError on \".transform\" method applied to an empty DataFrame\n\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the latest version of pandas.\r\n\r\n- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.\r\n\r\n---\r\n\r\n#### Code Sample, a copy-pastable example\r\n\r\nOutput on version 1.1.5:\r\n```python\r\nIn [5]: import pandas as pd\r\n   ...: df = pd.DataFrame([], columns=[\"id\", \"field\"])\r\n   ...: df[\"id\"].transform(lambda x: x + 10)\r\nOut[5]: Series([], Name: id, dtype: object)\r\n```\r\n\r\nOutput on version 1.2.x:\r\n```python\r\nIn [4]: import pandas as pd\r\n   ...: df = pd.DataFrame([], columns=[\"id\", \"field\"])\r\n   ...: df[\"id\"].transform(lambda x: x + 10)\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-3-d1e6cad57091> in <module>\r\n----> 1 df[\"id\"].transform(lambda x: x + 10)\r\n\r\n~/.pyenv/versions/3.9.1/envs/odds-data-3.9.1/lib/python3.9/site-packages/pandas/core/series.py in transform(self, func, axis, *args, **kwargs)\r\n   3975         self, func: AggFuncType, axis: Axis = 0, *args, **kwargs\r\n   3976     ) -> FrameOrSeriesUnion:\r\n-> 3977         return transform(self, func, axis, *args, **kwargs)\r\n   3978 \r\n   3979     def apply(self, func, convert_dtype=True, args=(), **kwds):\r\n\r\n~/.pyenv/versions/3.9.1/envs/odds-data-3.9.1/lib/python3.9/site-packages/pandas/core/aggregation.py in transform(obj, func, axis, *args, **kwargs)\r\n    458     # when the dtype is not appropriate\r\n    459     if isinstance(result, (ABCSeries, ABCDataFrame)) and result.empty:\r\n--> 460         raise ValueError(\"Transform function failed\")\r\n    461     if not isinstance(result, (ABCSeries, ABCDataFrame)) or not result.index.equals(\r\n    462         obj.index\r\n\r\nValueError: Transform function failed\r\n```\r\n\r\n#### Problem description\r\n\r\nApplying `.transform` on an empty DataFrame raises a `ValueError` on version 1.2.x. This is a change on the behavior of 1.1.5 version that returns the same empty DataFrame (as `.apply` is still doing).\r\n\r\nThe change that added this error apparently is related to this commit https://github.com/pandas-dev/pandas/pull/35964/commits/7b6ab94720024d6696b19867f5f8f59f79587ff0 \r\n\r\n#### Expected Output\r\n\r\n```python\r\nIn [5]: import pandas as pd\r\n   ...: df = pd.DataFrame([], columns=[\"id\", \"field\"])\r\n   ...: df[\"id\"].transform(lambda x: x + 10)\r\nOut[5]: Series([], Name: id, dtype: object)\r\n```\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : 9d598a5e1eee26df95b3910e3f2934890d062caa\r\npython           : 3.9.1.final.0\r\npython-bits      : 64\r\nOS               : Linux\r\nOS-release       : 5.4.0-65-generic\r\nVersion          : #73-Ubuntu SMP Mon Jan 18 17:25:17 UTC 2021\r\nmachine          : x86_64\r\nprocessor        : x86_64\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : en_US.UTF-8\r\nLOCALE           : en_US.UTF-8\r\n\r\npandas           : 1.2.1\r\nnumpy            : 1.20.0\r\npytz             : 2021.1\r\ndateutil         : 2.8.1\r\npip              : 20.2.3\r\nsetuptools       : 49.2.1\r\nCython           : None\r\npytest           : 6.2.2\r\nhypothesis       : None\r\nsphinx           : None\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : None\r\nlxml.etree       : 4.6.2\r\nhtml5lib         : None\r\npymysql          : None\r\npsycopg2         : None\r\njinja2           : None\r\nIPython          : 7.20.0\r\npandas_datareader: None\r\nbs4              : None\r\nbottleneck       : None\r\nfsspec           : None\r\nfastparquet      : None\r\ngcsfs            : None\r\nmatplotlib       : None\r\nnumexpr          : None\r\nodfpy            : None\r\nopenpyxl         : None\r\npandas_gbq       : None\r\npyarrow          : None\r\npyxlsb           : None\r\ns3fs             : None\r\nscipy            : 1.6.0\r\nsqlalchemy       : 1.3.23\r\ntables           : None\r\ntabulate         : None\r\nxarray           : None\r\nxlrd             : None\r\nxlwt             : None\r\nnumba            : None\r\n\r\n</details>", "patch": ""}}
{"instance_id": "pandas-dev__pandas-33238", "found_files": [[]], "found_modules": [[]], "found_entities": [[]], "raw_output_loc": [], "meta_data": {"repo": "pandas-dev/pandas", "base_commit": "9572a2e00ddadb9fc7e2125c3e723b8a3b54be05", "problem_statement": "CI/COMPAT: Linux py37_np_dev pipeline timeouts\n\n#### Problem description\r\n\r\nLinux py37_np_dev pipeline appears to timeout for everyone after 60 minutes.\r\nThere are a couple hundred thousand errors like this:\r\n```\r\nException ignored in: 'pandas.io.sas._sas.Parser.process_byte_array_with_data'\r\nDeprecationWarning: tostring() is deprecated. Use tobytes() instead.\r\nDeprecationWarning: tostring() is deprecated. Use tobytes() instead.\r\n```\r\nHere is a [link](https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=32212&view=logs&j=3a03f79d-0b41-5610-1aa4-b4a014d0bc70&t=4d05ed0e-1ed3-5bff-dd63-1e957f2766a9&l=792078) to it failing for me.", "patch": ""}}
{"instance_id": "scikit-learn__scikit-learn-10251", "found_files": [[]], "found_modules": [[]], "found_entities": [[]], "raw_output_loc": [], "meta_data": {"repo": "scikit-learn/scikit-learn", "base_commit": "61e722aa126207efcdbc1ddcd4453854ad44ea09", "problem_statement": "Extending Criterion\n\nUnless I'm missing something, it's not completely trivial how one can use a custom `sklearn.tree._criterion.Criterion` for a decision tree. See my use case [here](https://stats.stackexchange.com/q/316954/98500).\r\n\r\nThings I have tried include:\r\n\r\n- Import the `ClassificationCriterion` in Python and subclass it. It seems that `node_impurity` and `children_impurity` do not get called, the impurity is always 0 (perhaps because they are `cdef` and not `cpdef`?). I'm also unsure what the parameters to `__new__` / `__cinit__` should be (e.g. `1` and `np.array([2], dtype='intp')` for a binary classification problem?), or how to pass them properly: I have to create the `Criterion` object from outside the tree to circumvent [the check on the `criterion` argument](https://github.com/scikit-learn/scikit-learn/blob/a24c8b464d094d2c468a16ea9f8bf8d42d949f84/sklearn/tree/tree.py#L324).\r\n\r\n- Extend `ClassificationCriterion` in a Cython file. This seems to work, but (a) it requires exporting `ClassificationCriterion` from `_criterion.pxd` and (b) it would be nice if it would be documented more extensively what should be done in `node_impurity` and `children_impurity`. I will post my code below once it seems to work correctly.\r\n\r\nMay I propose one of the following to make this easier?\r\n\r\n- Document what should be done to extend the class in Cython or Python - if Python should be allowed: I am aware of the performance issue with that, but in some cases it may be OK to do this in Python - I don't know.\r\n- Make it possible to pass a function or other object not extending `Criterion` to the tree, similar to how it is very easy to implement a custom scorer for validation functions. That would require changing the checks [here](https://github.com/scikit-learn/scikit-learn/blob/a24c8b464d094d2c468a16ea9f8bf8d42d949f84/sklearn/tree/tree.py#L324).", "patch": ""}}
{"instance_id": "scikit-learn__scikit-learn-27682", "found_files": [[]], "found_modules": [[]], "found_entities": [[]], "raw_output_loc": [], "meta_data": {"repo": "scikit-learn/scikit-learn", "base_commit": "3d19272be75fe32edd4cf01cb2eeac2281305e42", "problem_statement": "MAINT Directly `cimport` interfaces from `std::algorithm`\n\nSome Cython implementations use interfaces from the standard library of C++, namely `std::algorithm::move` and `std::algorithm::fill` from [`std::algorithm`](https://en.cppreference.com/w/cpp/algorithm/).\r\n\r\nBefore Cython 3, those interfaces had to be imported directly using the verbose syntax from Cython:\r\n - https://github.com/scikit-learn/scikit-learn/blob/5fc67aeb092d636895b599921283221a68c7a2ad/sklearn/metrics/_pairwise_distances_reduction/_radius_neighbors.pyx.tp#L22-L26\r\n - https://github.com/scikit-learn/scikit-learn/blob/5fc67aeb092d636895b599921283221a68c7a2ad/sklearn/metrics/_pairwise_distances_reduction/_middle_term_computer.pyx.tp#L28-L33\r\n\r\nCython 3 introduced the following line natively, for those interfaces. Those interfaces should now be `cimported` directly. That is one can replace the line shown above respectively with:\r\n\r\n```cython\r\nfrom libcpp.algorithm cimport move\r\nfrom libcpp.algorithm cimport fill\r\n```\r\n\r\nI believe this is a good first Cython issue.\r\n\r\nAny reader should feel free to pick it up. It might be possible that there is some context missing.\r\n\r\nPlease let me know if you need help. :slightly_smiling_face:", "patch": ""}}
{"instance_id": "huggingface__transformers-9954", "found_files": [["src/transformers/models/lxmert/modeling_tf_lxmert.py", "src/transformers/models/lxmert/configuration_lxmert.py", "src/transformers/utils/dummy_tf_objects.py"]], "found_modules": [[]], "found_entities": [[]], "raw_output_loc": ["```\ntests/test_modeling_tf_lxmert.py\n# (Should contain new integration tests for TF-LXMERT)\n\nsrc/transformers/models/lxmert/modeling_tf_lxmert.py\n# (Contains the TensorFlow implementation of LXMERT. Entry points for model execution will be referenced from new tests)\n\nsrc/transformers/models/lxmert/configuration_lxmert.py\n# (Defines the configuration for LXMERT, typically used when creating or loading model checkpoints in integration tests)\n\nsrc/transformers/utils/dummy_tf_objects.py\n# (For enabling/disabling/marking availability of TensorFlow objects in tests)\n\nsrc/transformers/testing_utils.py\n# (Contains @slow and @require_tf decorators. Used to annotate integration test functions for proper behavior)\n\n```\n"], "meta_data": {"repo": "huggingface/transformers", "base_commit": "626a0a01471accc32ded29ccca3ed93c4995fcd6", "problem_statement": "[Good first issue] LXMERT TensorFlow Integration tests\n\nThe TensorFlow implementation of the LXMERT model currently has no integration tests. This is problematic as the behavior can diverge without being noticed.\r\n\r\nThe [test_modeling_tf_lxmert.py](https://github.com/huggingface/transformers/blob/master/tests/test_modeling_tf_lxmert.py) file should be updated to include integration testing.\r\n\r\nAn example of a good modeling integration test is visible in the [test_modeling_tf_bert.py#L365-L387](https://github.com/huggingface/transformers/blob/1809de5165804666ba6c6a02a9d177f6683869cc/tests/test_modeling_tf_bert.py#L365-L387) file:\r\n\r\nhttps://github.com/huggingface/transformers/blob/1809de5165804666ba6c6a02a9d177f6683869cc/tests/test_modeling_tf_bert.py#L365-L387\r\n\r\nSome additional tips:\r\n- The test must be marked as slow using the `@slow` decorator, so as to be run *daily*, and not on every commit of every branch/pull request of this repository.\r\n- The test must be decorated with the `@require_tf` decorator so as to only be run in environments using PyTorch.\r\n- A single test is necessary. If you feel like implementing multiple of these, then sharing the same checkpoint would be ideal so as to reduce download time.", "patch": ""}}
{"instance_id": "pandas-dev__pandas-24115", "found_files": [["pandas/core/arrays/datetimes.py", "pandas/core/arrays/timedeltas.py", "pandas/core/arrays/period.py", "pandas/core/indexes/base.py", "pandas/core/generic.py"]], "found_modules": [["pandas/core/indexes/base.py:Index", "pandas/core/generic.py:NDFrame"]], "found_entities": [["pandas/core/indexes/base.py:Index._update_inplace", "pandas/core/generic.py:NDFrame._update_inplace"]], "raw_output_loc": ["```\npandas/core/arrays/datetimes.py\nclass: DatetimeArray\nfunction: (Constructors: __init__, _from_sequence; Mutators: tz_localize, tz_convert, normalize, round, floor, ceil)\n\npandas/core/arrays/timedeltas.py\nclass: TimedeltaArray\nfunction: (Constructors: __init__, _from_sequence; Mutators: round, floor, ceil)\n\npandas/core/arrays/period.py\nclass: PeriodArray\nfunction: (Constructors: __init__, _from_sequence; Mutators: asfreq, to_timestamp)\n\npandas/core/indexes/base.py\nclass: Index\nfunction: _update_inplace\n\npandas/core/generic.py\nclass: NDFrame\nfunction: _update_inplace\n```\n\nThese files, classes, and functions are central to the problem where DTA (DatetimeArray), TDA (TimedeltaArray), and PA (PeriodArray) inplace methods should actually operate in place, rather than returning new objects as currently implemented following Index subclass behavior. Key change zones include the array class constructors and mutators handling inplace operations, as well as the underlying update logic in NDFrame and Index."], "meta_data": {"repo": "pandas-dev/pandas", "base_commit": "710df2140555030e4d86e669d6df2deb852bcaf5", "problem_statement": "DTA/TDA/PA inplace methods should actually be inplace\n\nAt the moment we are using the implementations designed for Index subclasses, which return new objects.", "patch": ""}}
{"instance_id": "huggingface__transformers-26809", "found_files": [["src/transformers/models/mistral/modeling_mistral.py", "src/transformers/models/mistral/configuration_mistral.py", "src/transformers/models/mistral/convert_mistral_weights_to_hf.py", "src/transformers/models/llama/modeling_llama.py"]], "found_modules": [["src/transformers/models/xglm/modeling_flax_xglm.py:FlaxXGLMAttention"]], "found_entities": [["src/transformers/models/xglm/modeling_flax_xglm.py:FlaxXGLMAttention._split_heads"]], "raw_output_loc": ["```\nsrc/transformers/models/mistral/modeling_mistral.py\nclass: MistralMLP\nline: 164\n\nsrc/transformers/models/mistral/configuration_mistral.py\n\nsrc/transformers/models/mistral/convert_mistral_weights_to_hf.py\n\nsrc/transformers/models/llama/modeling_llama.py\n\nexamples/flax\n```\n"], "meta_data": {"repo": "huggingface/transformers", "base_commit": "da1d0d404f05523d37b37207a4c1ff419cc1f47f", "problem_statement": "Add Mistral Models to Flax\n\n### Feature request\r\n\r\nI would like to implement the ~~Llama~~ Mistral model in flax\r\n\r\n### Motivation\r\n\r\nI've been trying to get familiar with jax and as such I started migrating the llama model, and I think I am at a point where both models are quite comparable in outcome\r\n\r\n### Your contribution\r\n\r\nYes I could submit a PR with the model implementation", "patch": ""}}
{"instance_id": "pandas-dev__pandas-6403", "found_files": [["pandas/io/excel.py", "pandas/io/parsers.py"]], "found_modules": [["pandas/io/html.py:_BeautifulSoupHtml5LibFrameParser", "pandas/core/groupby.py:GroupBy", "pandas/tseries/period.py:PeriodIndex", "pandas/io/pytables.py:HDFStore", "pandas/stats/plm.py:PanelOLS", "pandas/computation/expr.py:BaseExprVisitor", "pandas/io/excel.py:ExcelFile", "pandas/sparse/series.py:SparseSeries", "pandas/core/categorical.py:Categorical", "pandas/tseries/offsets.py:DateOffset", "pandas/stats/ols.py:OLS", "pandas/tseries/frequencies.py:get_period_alias", "pandas/tseries/tdi.py:TimedeltaIndex", "setup.py:CheckSDist", "pandas/core/generic.py:NDFrame", "pandas/compat/__init__.py:_OrderedDict", "pandas/tseries/tdi.py:TimedeltaIndex.components", "pandas/core/series.py:Series", "setup.py:DummyBuildSrc", "pandas/tseries/base.py:DatetimeIndexOpsMixin._add_datetimelike_methods", "pandas/core/internals.py:Block", "doc/sphinxext/numpydoc/compiler_unparse.py:UnparseCompilerAst", "pandas/io/ga.py:_get_col_names", "pandas/stats/var.py:VAR", "pandas/io/excel.py:_trim_excel_header", "pandas/core/series.py:Series.quantile", "pandas/core/groupby.py:BaseGrouper", "pandas/core/internals.py:FloatBlock", "pandas/core/strings.py:StringMethods", "pandas/core/internals.py:ComplexBlock", "pandas/tools/plotting.py:MPLPlot", "pandas/core/index.py:Index", "pandas/core/internals.py:IntBlock", "pandas/io/parsers.py:TextParser"]], "found_entities": [["pandas/io/html.py:_BeautifulSoupHtml5LibFrameParser._parse_thead", "pandas/core/groupby.py:GroupBy.ngroups", "pandas/io/html.py:_BeautifulSoupHtml5LibFrameParser._parse_tbody", "pandas/tseries/period.py:PeriodIndex.to_datetime", "pandas/io/html.py:_BeautifulSoupHtml5LibFrameParser._parse_tfoot", "pandas/io/pytables.py:HDFStore.filename", "pandas/stats/plm.py:PanelOLS.y_fitted", "pandas/computation/expr.py:BaseExprVisitor.visit_NameConstant", "pandas/io/pytables.py:HDFStore.__setitem__", "pandas/io/excel.py:ExcelFile._parse_excel", "pandas/computation/expr.py:BaseExprVisitor.visit_Expr", "pandas/stats/plm.py:PanelOLS._df_model_raw", "pandas/sparse/series.py:SparseSeries.__getstate__", "pandas/core/categorical.py:Categorical.size", "pandas/stats/plm.py:PanelOLS._df_resid_raw", "pandas/tseries/offsets.py:DateOffset.__mul__", "pandas/stats/ols.py:OLS.std_err", "pandas/tseries/frequencies.py:get_period_alias", "pandas/tseries/offsets.py:DateOffset.__neg__", "pandas/tseries/tdi.py:TimedeltaIndex.days", "setup.py:CheckSDist.run", "pandas/core/generic.py:NDFrame._get_index_resolvers", "pandas/compat/__init__.py:_OrderedDict.items", "pandas/core/generic.py:NDFrame._info_axis", "pandas/compat/__init__.py:_OrderedDict.iterkeys", "pandas/core/generic.py:NDFrame._stat_axis", "pandas/stats/plm.py:PanelOLS.resid", "pandas/core/generic.py:NDFrame.shape", "pandas/tseries/tdi.py:TimedeltaIndex.components.f", "pandas/core/series.py:Series.view", "pandas/sparse/series.py:SparseSeries._set_with_engine", "setup.py:DummyBuildSrc.initialize_options", "pandas/core/generic.py:NDFrame.size", "pandas/core/groupby.py:GroupBy.__len__", "pandas/tseries/base.py:DatetimeIndexOpsMixin._add_datetimelike_methods.__rsub__", "pandas/core/groupby.py:GroupBy.__unicode__", "pandas/computation/expr.py:BaseExprVisitor.visit_Num", "pandas/core/series.py:Series.real", "pandas/computation/expr.py:BaseExprVisitor.visit_Str", "pandas/stats/plm.py:PanelOLS._unstack_y", "pandas/core/internals.py:Block._can_hold_element", "pandas/stats/plm.py:PanelOLS._time_obs_count", "doc/sphinxext/numpydoc/compiler_unparse.py:UnparseCompilerAst._Sub", "pandas/tseries/period.py:PeriodIndex._mpl_repr", "pandas/io/ga.py:_get_col_names", "pandas/stats/var.py:VAR._lagged_data", "pandas/stats/plm.py:PanelOLS._nobs", "pandas/core/categorical.py:Categorical._get_categories", "pandas/io/excel.py:_trim_excel_header", "pandas/core/series.py:Series.quantile.multi", "pandas/core/groupby.py:BaseGrouper._get_splitter", "pandas/core/internals.py:FloatBlock.should_store", "pandas/core/strings.py:StringMethods.center", "pandas/core/internals.py:ComplexBlock._can_hold_element", "pandas/core/strings.py:StringMethods.ljust", "pandas/core/categorical.py:Categorical.__len__", "pandas/core/strings.py:StringMethods.rjust", "pandas/core/internals.py:ComplexBlock._try_cast", "pandas/core/categorical.py:Categorical.__iter__", "pandas/tools/plotting.py:MPLPlot._maybe_add_color", "pandas/core/index.py:Index.sort", "pandas/core/internals.py:IntBlock.should_store", "pandas/core/strings.py:StringMethods.slice_replace", "pandas/core/strings.py:StringMethods.decode", "pandas/io/parsers.py:TextParser"]], "raw_output_loc": ["```\npandas/io/excel.py\nline: 322-440\nclass: ExcelFile\nfunction: ExcelFile._parse_excel\n\npandas/io/excel.py\nline: 459-464\nfunction: _trim_excel_header\n\npandas/io/excel.py\nline: ExcelFile.parse (location inferred from execution trace and structure)\n\npandas/io/parsers.py\nline: 1255-1310\nfunction: TextParser\n\npandas/io/excel.py\nline: 407-421\nfunction: ExcelFile._parse_excel (data initialization and header handling)\n```"], "meta_data": {"repo": "pandas-dev/pandas", "base_commit": "0b74c72e1c7fe320440fa97a3d256107ea329307", "problem_statement": "ExcelFile parse of empty sheet fails with \"IndexError: list index out of range\"\n\nUsing pandas 0.13.1 on OS X Mavericks to parse a blank Excel spreadsheet causes \"IndexError: list index out of range\". Apparently the default header=0 in `_parse_excel` causes the execution of `_trim_excel_header(data[header])`. Perhaps when nrows==0 this should not be executed.\n\n``` python\nimport pandas as pd\nxl_file = pd.ExcelFile('blank.xlsx')\nxl_file.parse('Sheet1') #Sheet1 has no data\n```\n\nSTDERR:\n\n```\nTraceback (most recent call last):\n  File \"/Users/myourshaw/lab/pypeline/python2/excel_example.py\", line 10, in <module>\n    xl_file.parse('Sheet1')\n  File \"/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/io/excel.py\", line 208, in parse\n    **kwds)\n  File \"/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/io/excel.py\", line 291, in _parse_excel\n    data[header] = _trim_excel_header(data[header])\nIndexError: list index out of range\n```", "patch": ""}}
{"instance_id": "localstack__localstack-4987", "found_files": [["localstack/services/s3/s3_listener.py", "localstack/services/s3/s3_utils.py", "localstack/services/edge.py", "localstack/utils/common.py", "localstack/services/s3/multipart_content.py"]], "found_modules": [["localstack/utils/common.py:md5"]], "found_entities": [["localstack/utils/common.py:md5"]], "raw_output_loc": ["```\nlocalstack/services/s3/s3_listener.py\nfunction: (Likely at entry points for S3 'PUT Object', handling Content-MD5 validation)\n\nlocalstack/services/s3/s3_utils.py\n(functions for S3 utilities managing payloads, MD5 computation, or chunked upload validation)\n\nlocalstack/services/edge.py\n(functions/classes controlling the edge routing layer for S3 requests, potentially passing headers like Content-MD5)\n\nlocalstack/utils/common.py\nline: 577-580\nfunction: md5\n\nlocalstack/services/s3/multipart_content.py\n(function: functions/classes handling multipart upload flow and validation, possibly implicated in content matching)\n```\n"], "meta_data": {"repo": "localstack/localstack", "base_commit": "2fe8440b619329891db150e45910e8aaad97b7ce", "problem_statement": "bug: The Content-MD5 you specified did not match what we received\n\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues\n\n### Current Behavior\n\nI started getting the following exception\r\n\r\n```\r\ncom.amazonaws.services.s3.model.AmazonS3Exception: The Content-MD5 you specified did not match what we received. \r\n(Service: Amazon S3; Status Code: 400; Error Code: BadDigest; Request ID: null; S3 Extended Request ID: null; Proxy: null)\r\n```\r\n\r\nafter upgrade to `localstack/localstack-light:latest`, reverting back to `localstack/localstack-light:0.13.0` fixes it for me.\r\n\n\n### Expected Behavior\n\nNo exception.\n\n### How are you starting LocalStack?\n\nCustom (please describe below)\n\n### Steps To Reproduce\n\n#### How are you starting localstack (e.g., `bin/localstack` command, arguments, or `docker-compose.yml`)\r\n\r\n    Using https://www.testcontainers.org/ to start the test.\r\n\r\n#### Client commands (e.g., AWS SDK code snippet, or sequence of \"awslocal\" commands)\r\n\r\n```\r\n@Bean\r\npublic AmazonS3 createAmazonS3() {\r\n    final DockerImageName diName = DockerImageName.parse(\"localstack/localstack-light:latest\").asCompatibleSubstituteFor(\"localstack/localstack\");\r\n    final LocalStackContainer localstack = new LocalStackContainer(diName)\r\n        .withServices(S3);\r\n    localstack.addEnv(\"AWS_ACCESS_KEY\", \"test\");\r\n    localstack.addEnv(\"AWS_SECRET_ACCESS_KEY\", \"567\");\r\n    localstack.addEnv(\"AWS_REGION\", \"us-east-1\");\r\n    localstack.addEnv(\"LS_LOG\", \"trace\");\r\n    localstack.start();\r\n    return AmazonS3ClientBuilder\r\n        .standard()\r\n        .withEndpointConfiguration(localstack.getEndpointConfiguration(S3))\r\n        .withCredentials(localstack.getDefaultCredentialsProvider())\r\n        .build();\r\n  }\r\n```\r\n\r\nthen calling `store` on `org.springframework.core.io.Resource` which is `SimpleStorageResource`.\r\n\n\n### Environment\n\n```markdown\n- OS: macOS Catalina 10.15.7\r\n- LocalStack: latest\n```\n\n\n### Anything else?\n\n`LS_LOG=trace` with `localstack/localstack-light:0.13.0`\r\n\r\n```\r\n2021-11-22T19:12:03:DEBUG:localstack.services.edge: IN(s3): \"GET /test-bucket-name/test-runtime.properties\" - headers: {'Remote-Addr': '172.17.0.1', 'Host': '127.0.0.1:52476', 'Amz-Sdk-Invocation-Id': '307eaac4-b1b6-d23e-96b1-a6dcff7d5414', 'Amz-Sdk-Request': 'attempt=1;max=4', 'Amz-Sdk-Retry': '0/0/500', 'Authorization': 'AWS4-HMAC-SHA256 Credential=accesskey/20211122/us-east-1/s3/aws4_request, SignedHeaders=amz-sdk-invocation-id;amz-sdk-request;amz-sdk-retry;content-type;host;user-agent;x-amz-content-sha256;x-amz-date, Signature=72f59f88e302656e9e4c77308f1de7925f5b63aec3efec93dd9d5f32ae6a2b6d', 'Content-Type': 'application/octet-stream', 'User-Agent': 'aws-sdk-java/1.11.951 Mac_OS_X/10.15.7 OpenJDK_64-Bit_Server_VM/11.0.11+9-LTS java/11.0.11 scala/2.13.6 kotlin/1.5.31 vendor/Amazon.com_Inc.', 'X-Amz-Content-Sha256': 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855', 'X-Amz-Date': '20211122T191203Z', 'Content-Length': '0', 'Connection': 'Keep-Alive', 'X-Forwarded-For': '172.17.0.1, 127.0.0.1:52476', 'x-localstack-edge': 'http://127.0.0.1:52476'} - data: b''\r\n2021-11-22T19:12:03:DEBUG:localstack.services.edge: OUT(s3): \"GET /test-bucket-name/test-runtime.properties\" - status: 404 - response headers: {'x-amzn-requestid': 'UJFL1535CHVAFPN2JLH2ACBUQX026PCCCTNN0RSBF4PJHULNR1AR', 'Content-Type': 'application/xml; charset=utf-8', 'Content-Length': '207', 'Access-Control-Allow-Origin': '*', 'Server': 'Werkzeug/2.0.2 Python/3.8.12', 'Date': 'Mon, 22 Nov 2021 19:12:03 GMT', 'Last-Modified': 'Mon, 22 Nov 2021 19:12:03 GMT', 'x-amz-request-id': '3DAD4B54E96B3CA1', 'x-amz-id-2': 'MzRISOwyjmnup3DAD4B54E96B3CA17/JypPGXLh0OVFGcJaaO3KW/hRAqKOpIEEp', 'accept-ranges': 'bytes', 'content-language': 'en-US'} - response: b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<Error>\\n    <Code>NoSuchKey</Code>\\n    <Message>The specified key does not exist.</Message>\\n    \\n    <RequestID>7a62c49f-347e-4fc4-9331-6e8eEXAMPLE</RequestID>\\n</Error>'\r\n2021-11-22T19:12:03:DEBUG:localstack.services.edge: OUT(s3): \"GET /test-bucket-name/test-runtime.properties\" - status: 404 - response headers: {'x-amzn-requestid': 'UJFL1535CHVAFPN2JLH2ACBUQX026PCCCTNN0RSBF4PJHULNR1AR', 'Content-Type': 'application/xml; charset=utf-8', 'Content-Length': '207', 'Access-Control-Allow-Origin': '*', 'Server': 'Werkzeug/2.0.2 Python/3.8.12', 'Date': 'Mon, 22 Nov 2021 19:12:03 GMT', 'Last-Modified': 'Mon, 22 Nov 2021 19:12:03 GMT', 'x-amz-request-id': '3DAD4B54E96B3CA1', 'x-amz-id-2': 'MzRISOwyjmnup3DAD4B54E96B3CA17/JypPGXLh0OVFGcJaaO3KW/hRAqKOpIEEp', 'accept-ranges': 'bytes', 'content-language': 'en-US'} - response: b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<Error>\\n    <Code>NoSuchKey</Code>\\n    <Message>The specified key does not exist.</Message>\\n    \\n    <RequestID>7a62c49f-347e-4fc4-9331-6e8eEXAMPLE</RequestID>\\n</Error>'\r\n2021-11-22T19:12:03:DEBUG:localstack.services.edge: IN(s3): \"PUT /test-bucket-name/test-runtime.properties\" - headers: {'Remote-Addr': '172.17.0.1', 'Host': '127.0.0.1:52476', 'Amz-Sdk-Invocation-Id': '8a6682d3-1481-f538-4ed4-4ac03c4e4ec3', 'Amz-Sdk-Request': 'attempt=1;max=4', 'Amz-Sdk-Retry': '0/0/500', 'Authorization': 'AWS4-HMAC-SHA256 Credential=accesskey/20211122/us-east-1/s3/aws4_request, SignedHeaders=amz-sdk-invocation-id;amz-sdk-request;amz-sdk-retry;content-length;content-md5;content-type;host;user-agent;x-amz-content-sha256;x-amz-date;x-amz-decoded-content-length, Signature=282e9062c19a5a575d49902c3c642928039a210c8d5eb54de069655f10ef94ea', 'Content-Md5': 'pX8KKuGXS1f2VTcuJpqjkw==', 'Content-Type': 'application/octet-stream', 'User-Agent': 'aws-sdk-java/1.11.951 Mac_OS_X/10.15.7 OpenJDK_64-Bit_Server_VM/11.0.11+9-LTS java/11.0.11 scala/2.13.6 kotlin/1.5.31 vendor/Amazon.com_Inc.', 'X-Amz-Content-Sha256': 'STREAMING-AWS4-HMAC-SHA256-PAYLOAD', 'X-Amz-Date': '20211122T191203Z', 'X-Amz-Decoded-Content-Length': '147', 'Content-Length': '320', 'Connection': 'Keep-Alive', 'Expect': '100-continue', 'X-Forwarded-For': '172.17.0.1, 127.0.0.1:52476', 'x-localstack-edge': 'http://127.0.0.1:52476'} - data: b'93;chunk-signature=68bf4c0366a3d4c963efb7eaf3426c439ac26f9ca077b6c71e1bd60de69f0259\\r\\n#20211122+0100\\n#Mon Nov 22 20:12:03 CET 2021\\nlast.sync.url.test-space-key=2822a50f-4992-425a-b8fb-923735a9ddff317e3479-5907-46cf-b33a-60da9709274f\\n\\r\\n0;chunk-signature=bf3a6ecc9d3913d2ad6618d420c1db6abefb4f452469693ffc5bbd038ad2f2f0\\r\\n\\r\\n'\r\n2021-11-22T19:12:03:DEBUG:localstack.services.edge: OUT(s3): \"PUT /test-bucket-name/test-runtime.properties\" - status: 200 - response headers: {'ETag': '\"a57f0a2ae1974b57f655372e269aa393\"', 'last-modified': 'Mon, 22 Nov 2021 19:12:03 GMT', 'Content-Length': '0', 'x-amzn-requestid': '1EYVT7AJ5TJ3JH1SK3ZVTHBBB860EIC4FTOP9VPHCSHR967AFFAP', 'Content-Type': 'text/html; charset=utf-8', 'Access-Control-Allow-Origin': '*', 'Server': 'Werkzeug/2.0.2 Python/3.8.12', 'Date': 'Mon, 22 Nov 2021 19:12:03 GMT', 'Location': '/test-bucket-name', 'x-amz-request-id': '5BC855D1EAAEFD00', 'x-amz-id-2': 'MzRISOwyjmnup5BC855D1EAAEFD007/JypPGXLh0OVFGcJaaO3KW/hRAqKOpIEEp'} - response: b''\r\n2021-11-22T19:12:03:DEBUG:localstack.services.edge: OUT(s3): \"PUT /test-bucket-name/test-runtime.properties\" - status: 200 - response headers: {'ETag': '\"a57f0a2ae1974b57f655372e269aa393\"', 'last-modified': 'Mon, 22 Nov 2021 19:12:03 GMT', 'Content-Length': '0', 'x-amzn-requestid': '1EYVT7AJ5TJ3JH1SK3ZVTHBBB860EIC4FTOP9VPHCSHR967AFFAP', 'Content-Type': 'text/html; charset=utf-8', 'Access-Control-Allow-Origin': '*', 'Server': 'Werkzeug/2.0.2 Python/3.8.12', 'Date': 'Mon, 22 Nov 2021 19:12:03 GMT', 'Location': '/test-bucket-name', 'x-amz-request-id': '5BC855D1EAAEFD00', 'x-amz-id-2': 'MzRISOwyjmnup5BC855D1EAAEFD007/JypPGXLh0OVFGcJaaO3KW/hRAqKOpIEEp'} - response: b''\r\n```\r\n\r\n----\r\n\r\n`LS_LOG=trace` with `localstack/localstack-light:latest`\r\n\r\n```\r\n2021-11-22T19:10:42.097:DEBUG:localstack.services.edge: IN(s3): \"GET /test-bucket-name/test-runtime.properties\" - headers: {'Remote-Addr': '172.17.0.1', 'Host': '127.0.0.1:52438', 'Amz-Sdk-Invocation-Id': '3f452c53-2a97-15f7-8f44-96c3b3d4aa27', 'Amz-Sdk-Request': 'attempt=1;max=4', 'Amz-Sdk-Retry': '0/0/500', 'Authorization': 'AWS4-HMAC-SHA256 Credential=accesskey/20211122/us-east-1/s3/aws4_request, SignedHeaders=amz-sdk-invocation-id;amz-sdk-request;amz-sdk-retry;content-type;host;user-agent;x-amz-content-sha256;x-amz-date, Signature=a8c7d475d338c92c01eca9638e858e8f0e84ae73498435a55520ee04ff655476', 'Content-Type': 'application/octet-stream', 'User-Agent': 'aws-sdk-java/1.11.951 Mac_OS_X/10.15.7 OpenJDK_64-Bit_Server_VM/11.0.11+9-LTS java/11.0.11 scala/2.13.6 kotlin/1.5.31 vendor/Amazon.com_Inc.', 'X-Amz-Content-Sha256': 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855', 'X-Amz-Date': '20211122T191042Z', 'Content-Length': '0', 'Connection': 'Keep-Alive', 'X-Forwarded-For': '172.17.0.1, 127.0.0.1:52438', 'x-localstack-edge': 'http://127.0.0.1:52438'} - data: b''\r\n2021-11-22T19:10:42.118:DEBUG:localstack.services.edge: OUT(s3): \"GET /test-bucket-name/test-runtime.properties\" - status: 404 - response headers: {'x-amzn-requestid': 'RMJVBYKAH478ETR8T1G9DQ4TUHEIKKB96892NRKM3PYQYRVUPI8M', 'Content-Type': 'application/xml; charset=utf-8', 'Content-Length': '207', 'Access-Control-Allow-Origin': '*', 'Server': 'Werkzeug/2.0.2 Python/3.8.12', 'Date': 'Mon, 22 Nov 2021 19:10:42 GMT', 'Last-Modified': 'Mon, 22 Nov 2021 19:10:42 GMT', 'x-amz-request-id': '7D83EFCB204B6EC9', 'x-amz-id-2': 'MzRISOwyjmnup7D83EFCB204B6EC97/JypPGXLh0OVFGcJaaO3KW/hRAqKOpIEEp', 'accept-ranges': 'bytes', 'content-language': 'en-US'} - response: b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<Error>\\n    <Code>NoSuchKey</Code>\\n    <Message>The specified key does not exist.</Message>\\n    \\n    <RequestID>7a62c49f-347e-4fc4-9331-6e8eEXAMPLE</RequestID>\\n</Error>'\r\n2021-11-22T19:10:42.119:DEBUG:localstack.services.edge: OUT(s3): \"GET /test-bucket-name/test-runtime.properties\" - status: 404 - response headers: {'x-amzn-requestid': 'RMJVBYKAH478ETR8T1G9DQ4TUHEIKKB96892NRKM3PYQYRVUPI8M', 'Content-Type': 'application/xml; charset=utf-8', 'Content-Length': '207', 'Access-Control-Allow-Origin': '*', 'Server': 'Werkzeug/2.0.2 Python/3.8.12', 'Date': 'Mon, 22 Nov 2021 19:10:42 GMT', 'Last-Modified': 'Mon, 22 Nov 2021 19:10:42 GMT', 'x-amz-request-id': '7D83EFCB204B6EC9', 'x-amz-id-2': 'MzRISOwyjmnup7D83EFCB204B6EC97/JypPGXLh0OVFGcJaaO3KW/hRAqKOpIEEp', 'accept-ranges': 'bytes', 'content-language': 'en-US'} - response: b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<Error>\\n    <Code>NoSuchKey</Code>\\n    <Message>The specified key does not exist.</Message>\\n    \\n    <RequestID>7a62c49f-347e-4fc4-9331-6e8eEXAMPLE</RequestID>\\n</Error>'\r\n2021-11-22T19:10:45.164:DEBUG:localstack.services.edge: IN(s3): \"PUT /test-bucket-name/test-runtime.properties\" - headers: {'Remote-Addr': '172.17.0.1', 'Host': '127.0.0.1:52438', 'Amz-Sdk-Invocation-Id': '3446d18f-08a6-2432-a4dc-f79846c9655e', 'Amz-Sdk-Request': 'attempt=1;max=4', 'Amz-Sdk-Retry': '0/0/500', 'Authorization': 'AWS4-HMAC-SHA256 Credential=accesskey/20211122/us-east-1/s3/aws4_request, SignedHeaders=amz-sdk-invocation-id;amz-sdk-request;amz-sdk-retry;content-length;content-md5;content-type;host;user-agent;x-amz-content-sha256;x-amz-date;x-amz-decoded-content-length, Signature=56f95a44e31918932bc863893064a1fcafbf4066d44bc44c8d078cf420316011', 'Content-Md5': 'Xi4HEV9K00jfK4+6lHxpDA==', 'Content-Type': 'application/octet-stream', 'User-Agent': 'aws-sdk-java/1.11.951 Mac_OS_X/10.15.7 OpenJDK_64-Bit_Server_VM/11.0.11+9-LTS java/11.0.11 scala/2.13.6 kotlin/1.5.31 vendor/Amazon.com_Inc.', 'X-Amz-Content-Sha256': 'STREAMING-AWS4-HMAC-SHA256-PAYLOAD', 'X-Amz-Date': '20211122T191045Z', 'X-Amz-Decoded-Content-Length': '147', 'Content-Length': '320', 'Connection': 'Keep-Alive', 'Expect': '100-continue', 'X-Forwarded-For': '172.17.0.1, 127.0.0.1:52438', 'x-localstack-edge': 'http://127.0.0.1:52438'} - data: b'93;chunk-signature=5be6b2d473e96bb9f297444da60bdf0ff8f5d2e211e1d551b3cf3646c0946641\\r\\n#20211122+0100\\n#Mon Nov 22 20:10:44 CET 2021\\nlast.sync.url.test-space-key=2822a50f-4992-425a-b8fb-923735a9ddff317e3479-5907-46cf-b33a-60da9709274f\\n\\r\\n0;chunk-signature=bd5c830b94346b57ddc8805ba26c44a122256c207014433bf6579b0985f21df7\\r\\n\\r\\n'\r\n2021-11-22T19:10:45.167:DEBUG:localstack.services.edge: OUT(s3): \"PUT /test-bucket-name/test-runtime.properties\" - status: 400 - response headers: {'Content-Type': 'application/xml', 'Location': '/test-bucket-name', 'Last-Modified': 'Mon, 22 Nov 2021 19:10:45 GMT', 'x-amz-request-id': '20278550A22502FB', 'x-amz-id-2': 'MzRISOwyjmnup20278550A22502FB7/JypPGXLh0OVFGcJaaO3KW/hRAqKOpIEEp', 'Content-Length': '156'} - response: <?xml version=\"1.0\" encoding=\"utf-8\"?>\r\n<Error><Code>BadDigest</Code><Message>The Content-MD5 you specified did not match what we received.</Message></Error>\r\n2021-11-22T19:10:45.168:DEBUG:localstack.services.edge: OUT(s3): \"PUT /test-bucket-name/test-runtime.properties\" - status: 400 - response headers: {'Content-Type': 'application/xml', 'Location': '/test-bucket-name', 'Last-Modified': 'Mon, 22 Nov 2021 19:10:45 GMT', 'x-amz-request-id': '20278550A22502FB', 'x-amz-id-2': 'MzRISOwyjmnup20278550A22502FB7/JypPGXLh0OVFGcJaaO3KW/hRAqKOpIEEp', 'Content-Length': '156'} - response: <?xml version=\"1.0\" encoding=\"utf-8\"?>\r\n<Error><Code>BadDigest</Code><Message>The Content-MD5 you specified did not match what we received.</Message></Error>\r\n```", "patch": ""}}
{"instance_id": "localstack__localstack-11696", "found_files": [["localstack-core/localstack/services/apigateway/next_gen/execute_api/gateway.py", "localstack-core/localstack/services/apigateway/next_gen/provider.py", "localstack-core/localstack/state/snapshot.py", "localstack-core/localstack/state/pickle.py", "localstack-core/localstack/services/apigateway/resource_providers/aws_apigateway_restapi_plugin.py"]], "found_modules": [["localstack-core/localstack/services/stepfunctions/asl/component/state/state_execution/state_task/service/state_task_service_ecs.py:StateTaskServiceEcs", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_execution/state_map/tolerated_failure.py:ToleratedFailureCount", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_execution/state_task/service/state_task_service_lambda.py:StateTaskServiceLambda", "localstack-core/localstack/services/stepfunctions/asl/component/common/timeouts/timeout.py:TimeoutSecondsPath", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserListener.py:ASLParserListener", "localstack-core/localstack/services/stepfunctions/legacy/provider_legacy.py:StepFunctionsProvider", "localstack-core/localstack/services/plugins.py:ServiceLifecycleHook", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_execution/state_map/iteration/iteration_worker.py:IterationWorker", "localstack-core/localstack/services/apigateway/legacy/invocations.py:AuthorizationError", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_execution/state_parallel/branches_decl.py:BranchWorkerPool", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_choice/comparison/operator/implementations/is_operator.py:IsPresent", "localstack-core/localstack/services/events/replay.py:ReplayService", "localstack-core/localstack/services/apigateway/legacy/templates.py:MappingTemplates", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_execution/state_task/service/state_task_service_batch.py:StateTaskServiceBatch", "localstack-core/localstack/runtime/server/hypercorn.py:HypercornRuntimeServer", "localstack-core/localstack/services/cloudformation/provider_utils.py:recurse_properties", "localstack-core/localstack/services/route53resolver/utils.py:get_trace_id", "localstack-core/localstack/services/stepfunctions/backend/execution_worker.py:ExecutionWorker", "localstack-core/localstack/services/cloudformation/engine/entities.py:StackIdentifier", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_choice/comparison/operator/implementations/numeric.py:NumericGreaterThanPath", "localstack-core/localstack/services/lambda_/invocation/executor_endpoint.py:ExecutorRouter", "localstack-core/localstack/services/apigateway/next_gen/execute_api/router.py:ApiGatewayEndpoint", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_execution/state_map/iteration/inline_iteration_component.py:InlineIterationComponent", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_choice/comparison/operator/implementations/timestamp_operators.py:TimestampGreaterThanPath", "localstack-core/localstack/services/apigateway/legacy/invocations.py:BadRequestBody", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserVisitor.py:ASLParserVisitor", "localstack-core/localstack/services/stepfunctions/asl/component/eval_component.py:EvalComponent", "localstack-core/localstack/services/apigateway/legacy/templates.py:AttributeDict", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_choice/comparison/operator/implementations/numeric.py:NumericGreaterThanEquals", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_choice/comparison/operator/implementations/is_operator.py:IsTimestamp", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_choice/comparison/operator/implementations/timestamp_operators.py:TimestampGreaterThanEquals", "localstack-core/localstack/services/cloudformation/scaffolding/__main__.py:ResourceName", "localstack-core/localstack/services/lambda_/invocation/execution_environment.py:ExecutionEnvironment", "localstack-core/localstack/services/apigateway/next_gen/execute_api/template_mapping.py:VelocityUtilApiGateway", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_execution/state_map/iteration/distributed_iteration_component.py:DistributedIterationComponent", "localstack-core/localstack/services/ec2/provider.py:Ec2Provider", "localstack-core/localstack/services/apigateway/helpers.py:get_moto_backend", "localstack-core/localstack/services/events/v1/provider.py:EventsProvider", "localstack-core/localstack/services/apigateway/legacy/templates.py:VelocityUtilApiGateway", "localstack-core/localstack/services/sqs/models.py:SqsMessage", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_choice/comparison/operator/implementations/string_operators.py:StringLessThan", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLIntrinsicParser.py:ASLIntrinsicParser.Func_declContext", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_choice/comparison/operator/implementations/numeric.py:NumericLessThan", "localstack-core/localstack/services/plugins.py:Service", "localstack-core/localstack/services/dynamodb/server.py:DynamodbServer", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_choice/comparison/operator/implementations/timestamp_operators.py:TimestampLessThan", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_execution/state_task/service/state_task_service_api_gateway.py:StateTaskServiceApiGateway", "localstack-core/localstack/utils/objects.py:fully_qualified_class_name", "localstack-core/localstack/services/cloudformation/scaffolding/propgen.py:PropertyTypeScaffolding", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_choice/comparison/operator/implementations/numeric.py:NumericLessThanPath", "localstack-core/localstack/utils/analytics/metadata.py:_generate_session_id", "localstack-core/localstack/services/events/models.py:Rule", "localstack-core/localstack/services/apigateway/next_gen/execute_api/handlers/method_request.py:MethodRequestHandler", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_execution/execute_state.py:ExecutionState", "localstack-core/localstack/aws/forwarder.py:HttpFallbackDispatcher", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_choice/comparison/operator/implementations/timestamp_operators.py:TimestampLessThanPath", "localstack-core/localstack/services/events/provider.py:encode_next_token", "localstack-core/localstack/services/stepfunctions/asl/component/state/state.py:CommonStateField", "localstack-core/localstack/services/s3/storage/core.py:S3StoredMultipart", "localstack-core/localstack/services/stepfunctions/asl/eval/callback/callback.py:CallbackEndpoint", "localstack-core/localstack/services/stepfunctions/asl/eval/environment.py:Environment", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_choice/comparison/operator/implementations/timestamp_operators.py:TimestampLessThanEquals", "localstack-core/localstack/services/stepfunctions/backend/execution.py:Execution", "localstack-core/localstack/services/stepfunctions/asl/eval/callback/callback.py:ActivityCallbackEndpoint", "localstack-core/localstack/services/apigateway/next_gen/execute_api/template_mapping.py:VelocityInput", "localstack-core/localstack/services/events/resource_providers/aws_events_rule.py:extract_rule_name", "localstack-core/localstack/services/dynamodbstreams/dynamodbstreams_api.py:get_kinesis_stream_name", "localstack-core/localstack/services/s3/utils.py:ChecksumHash", "localstack-core/localstack/services/apigateway/legacy/templates.py:VelocityInput", "localstack-core/localstack/services/lambda_/invocation/assignment.py:AssignmentService", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_choice/comparison/operator/implementations/timestamp_operators.py:TimestampLessThanEqualsPath", "localstack-core/localstack/services/apigateway/next_gen/execute_api/router.py:ApiGatewayRouter", "localstack-core/localstack/services/events/models.py:Archive", "localstack-core/localstack/services/apigateway/legacy/context.py:ApiInvocationContext", "localstack-core/localstack/services/apigateway/legacy/invocations.py:RequestValidator", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_execution/state_map/iteration/itemprocessor/map_run_record.py:MapRunRecordPoolManager", "localstack-core/localstack/services/stepfunctions/asl/eval/callback/callback.py:CallbackPoolManager", "localstack-core/localstack/services/cloudformation/engine/entities.py:Stack", "localstack-core/localstack/config.py:eval_log_type", "localstack-core/localstack/services/cloudformation/resource_provider.py:ResourceProvider", "localstack-core/localstack/services/s3/storage/core.py:S3ObjectStore", "localstack-core/localstack/services/apigateway/helpers.py:OpenAPISpecificationResolver", "localstack-core/localstack/services/lambda_/resource_providers/aws_lambda_function.py:_runtime_supports_inline_code", "localstack-core/localstack/services/opensearch/cluster_manager.py:ClusterManager", "localstack-core/localstack/utils/aws/aws_responses.py:LambdaResponse", "localstack-core/localstack/http/trace.py:TracingHandler.__call__", "localstack-core/localstack/utils/kinesis/kinesis_connector.py:KinesisProcessorThread", "localstack-core/localstack/aws/spec.py:ServiceCatalog", "localstack-core/localstack/services/apigateway/legacy/helpers.py:make_json_response", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLIntrinsicParser.py:ASLIntrinsicParser.State_fun_nameContext", "localstack-core/localstack/services/opensearch/cluster_manager.py:_get_port_from_url", "localstack-core/localstack/services/s3/legacy/provider.py:S3Provider", "localstack-core/localstack/services/plugins.py:ServiceManager", "localstack-core/localstack/services/sqs/provider.py:CloudwatchPublishWorker.start", "localstack-core/localstack/services/plugins.py:ServicePlugin", "localstack-core/localstack/state/pickle.py:ObjectStateReducer", "localstack-core/localstack/services/stepfunctions/asl/component/common/payload/payloadvalue/payloadbinding/payload_binding.py:PayloadBinding", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_execution/state_map/result_writer/resource_eval/resource_eval.py:ResourceEval", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_execution/state_parallel/branch_worker.py:BranchWorker.BranchWorkerComm", "localstack-core/localstack/services/stepfunctions/asl/component/common/retry/jitter_strategy_decl.py:JitterStrategy", "localstack-core/localstack/services/apigateway/next_gen/execute_api/integrations/core.py:RestApiIntegration", "localstack-core/localstack/services/apigateway/resource_providers/aws_apigateway_restapi_plugin.py:ApiGatewayRestApiProviderPlugin"]], "found_entities": [["localstack-core/localstack/services/stepfunctions/asl/component/state/state_execution/state_task/service/state_task_service_ecs.py:StateTaskServiceEcs._get_supported_parameters", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_execution/state_map/tolerated_failure.py:ToleratedFailureCount._eval_tolerated_failure_count", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_execution/state_task/service/state_task_service_lambda.py:StateTaskServiceLambda._get_supported_parameters", "localstack-core/localstack/services/stepfunctions/asl/component/common/timeouts/timeout.py:TimeoutSecondsPath.from_raw", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserListener.py:ASLParserListener.enterComment_decl", "localstack-core/localstack/services/stepfunctions/asl/component/common/timeouts/timeout.py:TimeoutSecondsPath.is_default_value", "localstack-core/localstack/services/stepfunctions/legacy/provider_legacy.py:StepFunctionsProvider.on_before_state_load", "localstack-core/localstack/services/plugins.py:ServiceLifecycleHook.on_before_start", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_execution/state_map/iteration/iteration_worker.py:IterationWorker._eval_input", "localstack-core/localstack/services/apigateway/legacy/invocations.py:AuthorizationError.to_response", "localstack-core/localstack/services/stepfunctions/legacy/provider_legacy.py:StepFunctionsProvider.on_before_stop", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_execution/state_parallel/branches_decl.py:BranchWorkerPool.get_exit_event_details", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_choice/comparison/operator/implementations/is_operator.py:IsPresent.impl_name", "localstack-core/localstack/services/events/replay.py:ReplayService.set_state", "localstack-core/localstack/services/apigateway/legacy/templates.py:MappingTemplates.get_passthrough_behavior", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserListener.py:ASLParserListener.exitVersion_decl", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_execution/state_task/service/state_task_service_batch.py:StateTaskServiceBatch._get_supported_parameters", "localstack-core/localstack/runtime/server/hypercorn.py:HypercornRuntimeServer._set_closed", "localstack-core/localstack/services/cloudformation/provider_utils.py:recurse_properties", "localstack-core/localstack/services/route53resolver/utils.py:get_trace_id", "localstack-core/localstack/services/stepfunctions/backend/execution_worker.py:ExecutionWorker._get_evaluation_entrypoint", "localstack-core/localstack/services/cloudformation/engine/entities.py:StackIdentifier.generate", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_choice/comparison/operator/implementations/numeric.py:NumericGreaterThanPath.impl_name", "localstack-core/localstack/services/lambda_/invocation/executor_endpoint.py:ExecutorRouter.register_endpoint", "localstack-core/localstack/services/apigateway/next_gen/execute_api/router.py:ApiGatewayEndpoint.is_rest_api", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_execution/state_map/iteration/inline_iteration_component.py:InlineIterationComponent._create_worker", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_choice/comparison/operator/implementations/timestamp_operators.py:TimestampGreaterThanPath.impl_name", "localstack-core/localstack/services/apigateway/legacy/invocations.py:BadRequestBody.to_response", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserVisitor.py:ASLParserVisitor.visitResource_decl", "localstack-core/localstack/services/stepfunctions/asl/component/eval_component.py:EvalComponent._eval_body", "localstack-core/localstack/services/apigateway/legacy/templates.py:AttributeDict.__setattr__", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_choice/comparison/operator/implementations/numeric.py:NumericGreaterThanEquals.impl_name", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_choice/comparison/operator/implementations/is_operator.py:IsTimestamp.impl_name", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserListener.py:ASLParserListener.exitState_name", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_choice/comparison/operator/implementations/timestamp_operators.py:TimestampGreaterThanEquals.impl_name", "localstack-core/localstack/services/cloudformation/scaffolding/__main__.py:ResourceName.schema_filename", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserListener.py:ASLParserListener.enterState_decl", "localstack-core/localstack/services/cloudformation/scaffolding/__main__.py:ResourceName.path_compatible_full_name", "localstack-core/localstack/services/lambda_/invocation/execution_environment.py:ExecutionEnvironment.get_log_stream_name", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_choice/comparison/operator/implementations/is_operator.py:IsTimestamp.is_timestamp", "localstack-core/localstack/services/apigateway/next_gen/execute_api/template_mapping.py:VelocityUtilApiGateway.toJson", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_execution/state_map/iteration/distributed_iteration_component.py:DistributedIterationComponent._create_worker", "localstack-core/localstack/services/apigateway/next_gen/execute_api/template_mapping.py:VelocityUtilApiGateway.urlEncode", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserVisitor.py:ASLParserVisitor.visitOutput_path_decl_path_context_object", "localstack-core/localstack/services/ec2/provider.py:Ec2Provider.on_after_init", "localstack-core/localstack/services/apigateway/next_gen/execute_api/template_mapping.py:VelocityUtilApiGateway.urlDecode", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserListener.py:ASLParserListener.exitState_decl_body", "localstack-core/localstack/services/apigateway/helpers.py:get_moto_backend", "localstack-core/localstack/services/events/v1/provider.py:EventsProvider.get_store", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserListener.py:ASLParserListener.enterType_decl", "localstack-core/localstack/services/apigateway/legacy/templates.py:VelocityUtilApiGateway.urlEncode", "localstack-core/localstack/services/sqs/models.py:SqsMessage.dead_letter_queue_source_arn", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_choice/comparison/operator/implementations/string_operators.py:StringLessThan.impl_name", "localstack-core/localstack/services/apigateway/legacy/templates.py:VelocityUtilApiGateway.urlDecode", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLIntrinsicParser.py:ASLIntrinsicParser.Func_declContext.states_func_decl", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_choice/comparison/operator/implementations/numeric.py:NumericLessThan.impl_name", "localstack-core/localstack/services/plugins.py:Service.name", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserListener.py:ASLParserListener.enterNext_decl", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLIntrinsicParser.py:ASLIntrinsicParser.Func_declContext.EOF", "localstack-core/localstack/services/dynamodb/server.py:DynamodbServer.in_memory", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserListener.py:ASLParserListener.exitNext_decl", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_choice/comparison/operator/implementations/timestamp_operators.py:TimestampLessThan.impl_name", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserVisitor.py:ASLParserVisitor.visitError_path_decl_path", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserListener.py:ASLParserListener.enterResource_decl", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_execution/state_task/service/state_task_service_api_gateway.py:StateTaskServiceApiGateway._get_supported_parameters", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserListener.py:ASLParserListener.exitResource_decl", "localstack-core/localstack/utils/objects.py:fully_qualified_class_name", "localstack-core/localstack/services/cloudformation/scaffolding/propgen.py:PropertyTypeScaffolding.get_ref_definition", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_choice/comparison/operator/implementations/numeric.py:NumericLessThanPath.impl_name", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserListener.py:ASLParserListener.enterInput_path_decl_path_context_object", "localstack-core/localstack/utils/analytics/metadata.py:_generate_session_id", "localstack-core/localstack/services/events/models.py:Rule.arn", "localstack-core/localstack/services/apigateway/next_gen/execute_api/handlers/method_request.py:MethodRequestHandler.should_validate_body", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_execution/execute_state.py:ExecutionState._eval_execution", "localstack-core/localstack/aws/forwarder.py:HttpFallbackDispatcher", "localstack-core/localstack/services/apigateway/next_gen/execute_api/handlers/method_request.py:MethodRequestHandler.should_validate_request", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_choice/comparison/operator/implementations/timestamp_operators.py:TimestampLessThanPath.impl_name", "localstack-core/localstack/services/events/provider.py:encode_next_token", "localstack-core/localstack/services/stepfunctions/asl/component/state/state.py:CommonStateField._eval_state", "localstack-core/localstack/services/s3/storage/core.py:S3StoredMultipart.open", "localstack-core/localstack/services/stepfunctions/asl/eval/callback/callback.py:CallbackEndpoint.report", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserListener.py:ASLParserListener.enterResult_decl", "localstack-core/localstack/services/stepfunctions/asl/eval/environment.py:Environment.is_running", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserListener.py:ASLParserListener.exitResult_decl", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_choice/comparison/operator/implementations/timestamp_operators.py:TimestampLessThanEquals.impl_name", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserVisitor.py:ASLParserVisitor.visitTimestamp_decl", "localstack-core/localstack/services/stepfunctions/backend/execution.py:Execution.to_start_output", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_execution/execute_state.py:ExecutionState._terminate_with_event", "localstack-core/localstack/services/stepfunctions/asl/eval/callback/callback.py:ActivityCallbackEndpoint.get_activity_input", "localstack-core/localstack/services/apigateway/next_gen/execute_api/template_mapping.py:VelocityInput.__repr__", "localstack-core/localstack/services/events/resource_providers/aws_events_rule.py:extract_rule_name", "localstack-core/localstack/services/stepfunctions/asl/eval/callback/callback.py:ActivityCallbackEndpoint.notify_activity_task_start", "localstack-core/localstack/services/dynamodbstreams/dynamodbstreams_api.py:get_kinesis_stream_name", "localstack-core/localstack/services/s3/utils.py:ChecksumHash.digest", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserListener.py:ASLParserListener.exitOutput_path_decl_path_context_object", "localstack-core/localstack/services/apigateway/legacy/templates.py:VelocityInput.__repr__", "localstack-core/localstack/services/lambda_/invocation/assignment.py:AssignmentService.stop", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserListener.py:ASLParserListener.enterOutput_path_decl_path", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_choice/comparison/operator/implementations/timestamp_operators.py:TimestampLessThanEqualsPath.impl_name", "localstack-core/localstack/services/apigateway/next_gen/execute_api/router.py:ApiGatewayRouter.unregister_routes", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserListener.py:ASLParserListener.exitOutput_path_decl_path", "localstack-core/localstack/services/sqs/models.py:SqsMessage.__repr__", "localstack-core/localstack/services/events/models.py:Archive.event_count", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserVisitor.py:ASLParserVisitor.visitParameters_decl", "localstack-core/localstack/services/apigateway/legacy/context.py:ApiInvocationContext.domain_name", "localstack-core/localstack/services/apigateway/legacy/invocations.py:RequestValidator.should_validate_request", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_execution/state_map/iteration/itemprocessor/map_run_record.py:MapRunRecordPoolManager.add", "localstack-core/localstack/services/stepfunctions/asl/eval/callback/callback.py:CallbackPoolManager.get", "localstack-core/localstack/services/cloudformation/engine/entities.py:Stack.set_time_attribute", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_execution/state_map/iteration/itemprocessor/map_run_record.py:MapRunRecordPoolManager.get", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserVisitor.py:ASLParserVisitor.visitTimeout_seconds_path_decl", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserListener.py:ASLParserListener.exitDefault_decl", "localstack-core/localstack/config.py:eval_log_type", "localstack-core/localstack/services/cloudformation/resource_provider.py:ResourceProvider.update", "localstack-core/localstack/services/s3/storage/core.py:S3ObjectStore.create_bucket", "localstack-core/localstack/services/apigateway/helpers.py:OpenAPISpecificationResolver.resolve_references", "localstack-core/localstack/services/s3/storage/core.py:S3ObjectStore.delete_bucket", "localstack-core/localstack/services/lambda_/resource_providers/aws_lambda_function.py:_runtime_supports_inline_code", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserVisitor.py:ASLParserVisitor.visitPayload_tmpl_decl", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserListener.py:ASLParserListener.enterError_path_decl_path", "localstack-core/localstack/services/stepfunctions/asl/eval/environment.py:Environment.is_frame", "localstack-core/localstack/services/opensearch/cluster_manager.py:ClusterManager.shutdown_all", "localstack-core/localstack/utils/aws/aws_responses.py:LambdaResponse.content", "localstack-core/localstack/services/stepfunctions/asl/eval/callback/callback.py:CallbackPoolManager.generate", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserListener.py:ASLParserListener.enterError_path_decl_intrinsic", "localstack-core/localstack/http/trace.py:TracingHandler.__call__._call", "localstack-core/localstack/utils/kinesis/kinesis_connector.py:KinesisProcessorThread.wait_subprocesses_initialized", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserVisitor.py:ASLParserVisitor.visitPayload_binding_intrinsic_func", "localstack-core/localstack/aws/spec.py:ServiceCatalog.service_names", "localstack-core/localstack/services/apigateway/legacy/helpers.py:make_json_response", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserVisitor.py:ASLParserVisitor.visitPayload_binding_value", "localstack-core/localstack/aws/spec.py:ServiceCatalog.target_prefix_index", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserListener.py:ASLParserListener.exitCause_decl", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLIntrinsicParser.py:ASLIntrinsicParser.State_fun_nameContext.Array", "localstack-core/localstack/services/opensearch/cluster_manager.py:_get_port_from_url", "localstack-core/localstack/aws/spec.py:ServiceCatalog.operations_index", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLIntrinsicParser.py:ASLIntrinsicParser.State_fun_nameContext.ArrayContains", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserListener.py:ASLParserListener.exitCause_path_decl_path", "localstack-core/localstack/aws/spec.py:ServiceCatalog.endpoint_prefix_index", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLIntrinsicParser.py:ASLIntrinsicParser.State_fun_nameContext.ArrayRange", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserVisitor.py:ASLParserVisitor.visitPayload_value_decl", "localstack-core/localstack/services/s3/legacy/provider.py:S3Provider.on_before_stop", "localstack-core/localstack/services/plugins.py:ServiceManager.exists", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLIntrinsicParser.py:ASLIntrinsicParser.State_fun_nameContext.ArrayLength", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserListener.py:ASLParserListener.exitCause_path_decl_intrinsic", "localstack-core/localstack/services/plugins.py:ServiceManager.is_running", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLIntrinsicParser.py:ASLIntrinsicParser.State_fun_nameContext.ArrayUnique", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserVisitor.py:ASLParserVisitor.visitPayload_value_int", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserListener.py:ASLParserListener.exitParameters_decl", "localstack-core/localstack/services/cloudformation/engine/entities.py:Stack.resource", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserVisitor.py:ASLParserVisitor.visitItem_processor_decl", "localstack-core/localstack/services/stepfunctions/asl/antlr/runtime/ASLParserListener.py:ASLParserListener.enterTimeout_seconds_decl", "localstack-core/localstack/services/sqs/provider.py:CloudwatchPublishWorker.start._run", "localstack-core/localstack/services/plugins.py:ServicePlugin.load", "localstack-core/localstack/state/pickle.py:ObjectStateReducer.restore", "localstack-core/localstack/services/stepfunctions/asl/component/common/payload/payloadvalue/payloadbinding/payload_binding.py:PayloadBinding._eval_val", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_execution/state_map/result_writer/resource_eval/resource_eval.py:ResourceEval.eval_resource", "localstack-core/localstack/services/stepfunctions/asl/component/state/state_execution/state_parallel/branch_worker.py:BranchWorker.BranchWorkerComm.on_terminated", "localstack-core/localstack/services/stepfunctions/asl/component/common/retry/jitter_strategy_decl.py:JitterStrategy.__repr__", "localstack-core/localstack/services/apigateway/next_gen/execute_api/integrations/core.py:RestApiIntegration.invoke", "localstack-core/localstack/services/apigateway/resource_providers/aws_apigateway_restapi_plugin.py:ApiGatewayRestApiProviderPlugin.load"]], "raw_output_loc": ["```\nlocalstack-core/localstack/services/apigateway/next_gen/execute_api/gateway.py\nclass: RestApiGateway\n\nlocalstack-core/localstack/services/apigateway/next_gen/provider.py\nline: 43-265\nclass: ApigatewayNextGenProvider\n\nlocalstack-core/localstack/state/snapshot.py\nclass: SnapshotPersistencePlugin\n\nlocalstack-core/localstack/state/pickle.py\nline: 341-348\nfunction: ObjectStateReducer.restore\n\nlocalstack-core/localstack/services/apigateway/resource_providers/aws_apigateway_restapi_plugin.py\nline: 15-20\nfunction: ApiGatewayRestApiProviderPlugin.load\n```"], "meta_data": {"repo": "localstack/localstack", "base_commit": "8c9d9b0475247f667a0f184f2fbc6d66b955749f", "problem_statement": "bug: API Gateway does not persist correctly when you restart the localstack docker container\n\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues\r\n\r\n### Current Behavior\r\n\r\nI have a working api gateway created with localstack. When I restart the container and try to query the same url, I get this message:\r\n`{\"message\": \"The API id '0e0cf92f' does not correspond to a deployed API Gateway API\"}`.\r\n\r\n# Details:\r\nFirst I create my API and confirm it works:\r\n```\r\n$ awslocal apigatewayv2 get-apis\r\n{\r\n    \"Items\": [\r\n        {\r\n            \"ApiEndpoint\": \"http://0e0cf92f.execute-api.localhost.localstack.cloud:4566\",\r\n            \"ApiId\": \"0e0cf92f\",\r\n            \"ApiKeySelectionExpression\": \"$request.header.x-api-key\",\r\n            \"CorsConfiguration\": {\r\n                \"AllowHeaders\": [\r\n                    \"*\"\r\n                ],\r\n                \"AllowMethods\": [\r\n                    \"*\"\r\n                ],\r\n                \"AllowOrigins\": [\r\n                    \"*\"\r\n                ],\r\n                \"ExposeHeaders\": [\r\n                    \"*\"\r\n                ]\r\n            },\r\n            \"CreatedDate\": \"2024-10-16T05:24:49.452000+00:00\",\r\n            \"DisableExecuteApiEndpoint\": false,\r\n            \"Name\": \"XpedigoAPI_v2\",\r\n            \"ProtocolType\": \"HTTP\",\r\n            \"RouteSelectionExpression\": \"$request.method $request.path\",\r\n            \"Tags\": {},\r\n            \"Version\": \"2024-09-25 01:18:37UTC\"\r\n        }\r\n    ]\r\n}\r\n```\r\n```\r\n$ awslocal apigatewayv2 get-stages --api-id=0e0cf92f\r\n{\r\n    \"Items\": [\r\n        {\r\n            \"CreatedDate\": \"2024-10-16T05:24:49.524619+00:00\",\r\n            \"DefaultRouteSettings\": {\r\n                \"DetailedMetricsEnabled\": false\r\n            },\r\n            \"DeploymentId\": \"4d3d207f\",\r\n            \"LastUpdatedDate\": \"2024-10-16T05:24:49.524619+00:00\",\r\n            \"RouteSettings\": {},\r\n            \"StageName\": \"local\",\r\n            \"StageVariables\": {\r\n                \"baseurl\": \"alb-localstack-bdowson.ngrok.io\",\r\n                \"env\": \"local\"\r\n            },\r\n            \"Tags\": {}\r\n        }\r\n    ]\r\n}\r\n```\r\n```\r\n$ awslocal apigatewayv2 get-deployments --api-id=0e0cf92f\r\n{\r\n    \"Items\": [\r\n        {\r\n            \"AutoDeployed\": false,\r\n            \"CreatedDate\": \"2024-10-16T05:24:49.529068+00:00\",\r\n            \"DeploymentId\": \"4d3d207f\",\r\n            \"DeploymentStatus\": \"DEPLOYED\"\r\n        }\r\n    ]\r\n}\r\n```\r\nConfirm it works:\r\n```\r\n$ curl -v https://0e0cf92f.execute-api.localhost.localstack.cloud:4566/local/accounts/health\r\n* Trying 127.0.0.1:4566...\r\n* TCP_NODELAY set\r\n* Connected to 0e0cf92f.execute-api.localhost.localstack.cloud (127.0.0.1) port 4566 (#0)\r\n* ALPN, offering h2\r\n* ALPN, offering http/1.1\r\n* successfully set certificate verify locations:\r\n*   CAfile: /etc/ssl/certs/ca-certificates.crt\r\n  CApath: /etc/ssl/certs\r\n* TLSv1.3 (OUT), TLS handshake, Client hello (1):\r\n* TLSv1.3 (IN), TLS handshake, Server hello (2):\r\n* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):\r\n* TLSv1.3 (IN), TLS handshake, Certificate (11):\r\n* TLSv1.3 (IN), TLS handshake, CERT verify (15):\r\n* TLSv1.3 (IN), TLS handshake, Finished (20):\r\n* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):\r\n* TLSv1.3 (OUT), TLS handshake, Finished (20):\r\n* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384\r\n* ALPN, server accepted to use h2\r\n* Server certificate:\r\n*  subject: CN=localhost.localstack.cloud\r\n*  start date: Sep  6 00:00:00 2024 GMT\r\n*  expire date: Dec  5 23:59:59 2024 GMT\r\n*  subjectAltName: host \"0e0cf92f.execute-api.localhost.localstack.cloud\" matched cert's \"*.execute-api.localhost.localstack.cloud\"\r\n*  issuer: C=AT; O=ZeroSSL; CN=ZeroSSL RSA Domain Secure Site CA\r\n*  SSL certificate verify ok.\r\n* Using HTTP2, server supports multi-use\r\n* Connection state changed (HTTP/2 confirmed)\r\n* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0\r\n* Using Stream ID: 1 (easy handle 0x5b8d78082650)\r\n> GET /local/accounts/health HTTP/2\r\n> Host: 0e0cf92f.execute-api.localhost.localstack.cloud:4566\r\n> user-agent: curl/7.68.0\r\n> accept: */*\r\n> \r\n* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\r\n* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\r\n* old SSL session ID is stale, removing\r\n* Connection state changed (MAX_CONCURRENT_STREAMS == 100)!\r\n< HTTP/2 200 \r\n< server: TwistedWeb/24.3.0\r\n< date: Wed, 16 Oct 2024 05:25:16 GMT\r\n< content-type: text/html; charset=UTF-8\r\n< cache-control: private, must-revalidate\r\n< expires: -1\r\n< pragma: no-cache\r\n< x-powered-by: PHP/8.1.9RC1\r\n< content-length: 2\r\n< apigw-requestid: 5f9a3aa7\r\n< \r\n* Connection #0 to host 0e0cf92f.execute-api.localhost.localstack.cloud left intact\r\nOK\r\n```\r\n\r\nNow I stop localstack, and restart it with `docker-compose up`. The api gateway no longer works correctly:\r\n```\r\n$ curl -v https://0e0cf92f.execute-api.localhost.localstack.cloud:4566/local/accounts/health\r\n*   Trying 127.0.0.1:4566...\r\n* TCP_NODELAY set\r\n* Connected to 0e0cf92f.execute-api.localhost.localstack.cloud (127.0.0.1) port 4566 (#0)\r\n* ALPN, offering h2\r\n* ALPN, offering http/1.1\r\n* successfully set certificate verify locations:\r\n*   CAfile: /etc/ssl/certs/ca-certificates.crt\r\n  CApath: /etc/ssl/certs\r\n* TLSv1.3 (OUT), TLS handshake, Client hello (1):\r\n* TLSv1.3 (IN), TLS handshake, Server hello (2):\r\n* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):\r\n* TLSv1.3 (IN), TLS handshake, Certificate (11):\r\n* TLSv1.3 (IN), TLS handshake, CERT verify (15):\r\n* TLSv1.3 (IN), TLS handshake, Finished (20):\r\n* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):\r\n* TLSv1.3 (OUT), TLS handshake, Finished (20):\r\n* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384\r\n* ALPN, server accepted to use h2\r\n* Server certificate:\r\n*  subject: CN=localhost.localstack.cloud\r\n*  start date: Sep  6 00:00:00 2024 GMT\r\n*  expire date: Dec  5 23:59:59 2024 GMT\r\n*  subjectAltName: host \"0e0cf92f.execute-api.localhost.localstack.cloud\" matched cert's \"*.execute-api.localhost.localstack.cloud\"\r\n*  issuer: C=AT; O=ZeroSSL; CN=ZeroSSL RSA Domain Secure Site CA\r\n*  SSL certificate verify ok.\r\n* Using HTTP2, server supports multi-use\r\n* Connection state changed (HTTP/2 confirmed)\r\n* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0\r\n* Using Stream ID: 1 (easy handle 0x6550ac6c5650)\r\n> GET /local/accounts/health HTTP/2\r\n> Host: 0e0cf92f.execute-api.localhost.localstack.cloud:4566\r\n> user-agent: curl/7.68.0\r\n> accept: */*\r\n> \r\n* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\r\n* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\r\n* old SSL session ID is stale, removing\r\n* Connection state changed (MAX_CONCURRENT_STREAMS == 100)!\r\n< HTTP/2 404 \r\n< server: TwistedWeb/24.3.0\r\n< date: Wed, 16 Oct 2024 05:29:09 GMT\r\n< content-type: application/json\r\n< content-length: 86\r\n< \r\n* Connection #0 to host 0e0cf92f.execute-api.localhost.localstack.cloud left intact\r\n{\"message\": \"The API id '0e0cf92f' does not correspond to a deployed API Gateway API\"}\r\n```\r\n\r\nBut the configurations are all the same as before:\r\n```\r\n$ awslocal apigatewayv2 get-apis\r\n{\r\n    \"Items\": [\r\n        {\r\n            \"ApiEndpoint\": \"http://0e0cf92f.execute-api.localhost.localstack.cloud:4566\",\r\n            \"ApiId\": \"0e0cf92f\",\r\n            \"ApiKeySelectionExpression\": \"$request.header.x-api-key\",\r\n            \"CorsConfiguration\": {\r\n                \"AllowHeaders\": [\r\n                    \"*\"\r\n                ],\r\n                \"AllowMethods\": [\r\n                    \"*\"\r\n                ],\r\n                \"AllowOrigins\": [\r\n                    \"*\"\r\n                ],\r\n                \"ExposeHeaders\": [\r\n                    \"*\"\r\n                ]\r\n            },\r\n            \"CreatedDate\": \"2024-10-16T05:24:49.452000+00:00\",\r\n            \"DisableExecuteApiEndpoint\": false,\r\n            \"Name\": \"XpedigoAPI_v2\",\r\n            \"ProtocolType\": \"HTTP\",\r\n            \"RouteSelectionExpression\": \"$request.method $request.path\",\r\n            \"Tags\": {},\r\n            \"Version\": \"2024-09-25 01:18:37UTC\"\r\n        }\r\n    ]\r\n}\r\n\r\n$ awslocal apigatewayv2 get-deployments --api-id=0e0cf92f\r\n{\r\n    \"Items\": [\r\n        {\r\n            \"AutoDeployed\": false,\r\n            \"CreatedDate\": \"2024-10-16T05:24:49.529068+00:00\",\r\n            \"DeploymentId\": \"4d3d207f\",\r\n            \"DeploymentStatus\": \"DEPLOYED\"\r\n        }\r\n    ]\r\n}\r\n\r\n$ awslocal apigatewayv2 get-deployments --api-id=0e0cf92f\r\n{\r\n    \"Items\": [\r\n        {\r\n            \"CreatedDate\": \"2024-10-16T05:24:49.524619+00:00\",\r\n            \"DefaultRouteSettings\": {\r\n                \"DetailedMetricsEnabled\": false\r\n            },\r\n            \"DeploymentId\": \"4d3d207f\",\r\n            \"LastUpdatedDate\": \"2024-10-16T05:24:49.524619+00:00\",\r\n            \"RouteSettings\": {},\r\n            \"StageName\": \"local\",\r\n            \"StageVariables\": {\r\n                \"baseurl\": \"alb-localstack-bdowson.ngrok.io\",\r\n                \"env\": \"local\"\r\n            },\r\n            \"Tags\": {}\r\n        }\r\n    ]\r\n}\r\n```\r\n\r\n\r\n### Expected Behavior\r\n\r\nAPI gateway should work correctly even after a localstack container restart.\r\n\r\n### How are you starting LocalStack?\r\n\r\nWith a docker-compose file\r\n\r\n### Steps To Reproduce\r\n\r\ndocker-compose.yml:\r\n```\r\nlocalstack:\r\n    container_name: localstack\r\n    image: localstack/localstack-pro:latest\r\n    ports:\r\n      - 4566:4566\r\n      - 4510-4559:4510-4559\r\n    environment:\r\n      - DOCKER_HOST=unix:///var/run/docker.sock\r\n      - DEBUG=1\r\n      - PERSISTENCE=1\r\n      - SNAPSHOT_LOAD_STRATEGY=ON_STARTUP\r\n      - LOCALSTACK_API_KEY=${LOCALSTACK_API_KEY}\r\n      - PROVIDER_OVERRIDE_APIGATEWAY=next_gen\r\n    networks:\r\n      app_network:\r\n        ipv4_address: 10.0.2.20\r\n    volumes:\r\n      - \"/var/run/docker.sock:/var/run/docker.sock\"\r\n      - \"/localstack-data:/var/lib/localstack\"\r\n```\r\n\r\n1. `docker-compose up localstack`\r\n2. Import API Gateway with `awslocal apigatewayv2 import-api --body file://t.json`\r\n3. Create stage with `awslocal apigatewayv2 create-stage --api-id 54ae753d --stage-name local --auto-deploy`\r\n4. Confirm it works with `curl -v https://[gateway url]/local/whatever`\r\n5. Stop localstack\r\n6. Run `docker-compose up localstack` again\r\n7. Try and curl the api again and you will get an error\r\n\r\n### Environment\r\n\r\n```markdown\r\n- OS: Ubuntu 20.04.5 LTS\r\n- LocalStack: \r\n  LocalStack version: 3.8.2.dev33\r\n  LocalStack Docker image sha: localstack/localstack-pro@sha256:b533e1bcfbe8f5462483725276a0e7f8fbd9ded32b1be2dac5ec9cee5e822023\r\n  LocalStack build date: 2024-10-15\r\n  LocalStack build git hash: 318e1adc\r\n```\r\n\r\n\r\n### Anything else?\r\n\r\nAfter this error appears, even if I delete the API and recreate it I still get the message `{\"message\": \"The API id 'xxxx' does not correspond to a deployed API Gateway API\"}`. The only way for me to resolve it is to delete my local locastack snapshot folder and rebuild everything.", "patch": ""}}
{"instance_id": "pandas-dev__pandas-23814", "found_files": [["pandas/core/series.py", "pandas/core/arrays/categorical.py", "pandas/core/ops.py", "pandas/core/categorical.py"]], "found_modules": [["pandas/core/arrays/categorical.py:CategoricalAccessor", "pandas/core/arrays/categorical.py:Categorical", "pandas/core/ops.py:_comp_method_SERIES", "pandas/core/ops.py:dispatch_to_extension_op"]], "found_entities": [["pandas/core/arrays/categorical.py:CategoricalAccessor.codes", "pandas/core/arrays/categorical.py:Categorical.categories", "pandas/core/ops.py:_comp_method_SERIES", "pandas/core/ops.py:dispatch_to_extension_op"]], "raw_output_loc": ["```\npandas/core/series.py\nclass: Series\nfunction: __eq__\n\npandas/core/arrays/categorical.py\nclass: CategoricalAccessor\nfunction: codes\n\npandas/core/arrays/categorical.py\nclass: Categorical\nfunction: categories\n\npandas/core/ops.py\nfunction: _comp_method_SERIES\nfunction: dispatch_to_extension_op\n\npandas/core/categorical.py\nfunction: __eq__\n```\n"], "meta_data": {"repo": "pandas-dev/pandas", "base_commit": "d865e5213515cef6344f16f4c77386be9ce8f223", "problem_statement": "equality comparison with a scalar is slow for category (performance regression)\n\nAre the following 2 ways to compare a series to a scalar equivalent (ignore missing values)? I have to write the hard way in order to take advantage of the category properties.\r\n\r\n    ```python\r\n    x = pd.Series(list('abcd') * 1000000).astype('category')\r\n    %timeit x == 'a'\r\n    # 10 loops, best of 3: 25.2 ms per loop\r\n    %timeit x.cat.codes == x.cat.categories.get_loc('a')\r\n    # 1000 loops, best of 3: 750 \u00b5s per loop\r\n    ```", "patch": ""}}
{"instance_id": "pandas-dev__pandas-25828", "found_files": [[]], "found_modules": [[]], "found_entities": [[]], "raw_output_loc": [], "meta_data": {"repo": "pandas-dev/pandas", "base_commit": "923ac2bdee409e4fa8c47414b07f52e036bb21bc", "problem_statement": "Use Substitution Decorator for CustomBusinessMonthEnd\n\nThis is a follow up to https://github.com/pandas-dev/pandas/pull/21093/files#r188805397 which wasn't working with Py27. Now that that is a thing of the past we should be able to use the more idiomatic Substitution approach to generating this docstring", "patch": ""}}
{"instance_id": "scikit-learn__scikit-learn-4744", "found_files": [["sklearn/ensemble/forest.py", "sklearn/utils/validation.py"]], "found_modules": [["sklearn/multiclass.py:_ConstantPredictor", "sklearn/ensemble/forest.py:ExtraTreesClassifier", "sklearn/utils/validation.py:check_array", "sklearn/ensemble/forest.py:ForestClassifier"]], "found_entities": [["sklearn/multiclass.py:_ConstantPredictor.decision_function", "sklearn/utils/validation.py:check_array", "sklearn/ensemble/forest.py:ForestClassifier.predict_proba"]], "raw_output_loc": ["```\nsklearn/ensemble/forest.py\nline: 195\nfunction: check_array (change accept_sparse=\"csc\" handling; location of change to csc)\n\nsklearn/ensemble/forest.py\nline: 369\nfunction: ExtraTreesClassifier (change in invocation of predict_proba(X[mask_indices, :], check_input=False); line of bug)\n\nsklearn/utils/validation.py\nfunction: check_array (controls matrix format conversion that affects csc/csr logic)\n\nsklearn/ensemble/forest.py\nclass: ForestClassifier\nfunction: predict_proba (uses check_input=False, needs review for sparse/csr compatibility)\n\nsklearn/ensemble/forest.py\nclass: ExtraTreesClassifier\nfunction: __init__ (field: oob_score)\n```\n"], "meta_data": {"repo": "scikit-learn/scikit-learn", "base_commit": "abb31d0a7ca769a1e6406553a58a7fb0bd3b259a", "problem_statement": "Bug with using TreeClassifier with OOB score and sparse matrices\n\nWhen using the ExtraTreesClassifier (and likely other classes that are derived from BaseTreeClassifier), there is a problem when using sparsematrices: `ValueError: X should be in csr_matrix format, got <class 'scipy.sparse.csc.csc_matrix'>`.\n\nI tracked the issue down to the following lines:\n\nOn line 195 of forest.py the sparse matrix is changed to a csc matrix:\n`X = check_array(X, dtype=DTYPE, accept_sparse=\"csc\")`\n\nHowever on line 369 of forest.py, the following is call is made with `check_input=false`:\n`p_estimator = estimator.predict_proba(X[mask_indices, :], check_input=False)`\n\nThis leads to a ValueError in predict `ValueError: X should be in csr_matrix format, got <class 'scipy.sparse.csc.csc_matrix'>`.\n\nChanging check_input to True seems to fix the issue. It's probably best to also include a test case for this problem, I just made a quick PR with only the False -> True fix.", "patch": ""}}
{"instance_id": "pandas-dev__pandas-22046", "found_files": [["pandas/core/indexing.py", "pandas/core/frame.py"]], "found_modules": [["pandas/core/indexing.py:_iLocIndexer", "asv_bench/benchmarks/groupby.py:TransformEngine", "pandas/core/indexing.py:_LocationIndexer", "pandas/core/frame.py:DataFrame", "pandas/core/indexing.py:IndexingMixin"]], "found_entities": [["pandas/core/indexing.py:_iLocIndexer._setitem_with_indexer", "asv_bench/benchmarks/groupby.py:TransformEngine.time_series_cython", "pandas/core/indexing.py:_LocationIndexer.__setitem__", "pandas/core/frame.py:DataFrame.__setitem__", "pandas/core/indexing.py:IndexingMixin.iloc"]], "raw_output_loc": ["```\npandas/core/indexing.py\nline: 1322\nclass: _iLocIndexer\nfunction: _setitem_with_indexer\n\npandas/core/indexing.py\nline: 675\nclass: _LocationIndexer\nfunction: __setitem__\n\npandas/core/indexing.py\nline: 1717\nclass: _iLocIndexer\nfunction: _setitem_with_indexer_frame_value\n\npandas/core/frame.py\nline: 3090\nclass: DataFrame\nfunction: __setitem__\n\npandas/core/indexing.py\nline: 103\nclass: IndexingMixin\nfunction: iloc\n```"], "meta_data": {"repo": "pandas-dev/pandas", "base_commit": "9b4dfa195e3f23d81389745c26bff8e0087e74b0", "problem_statement": "Replacing multiple columns (or just one) with iloc does not work\n\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas\r\n\r\ncolumns = pandas.DataFrame({'a2': [11, 12, 13], 'b2': [14, 15, 16]})\r\ninputs = pandas.DataFrame({'a1': [1, 2, 3], 'b1': [4, 5, 6], 'c1': [7, 8, 9]})\r\n\r\ninputs.iloc[:, [1]] = columns.iloc[:, [0]]\r\n\r\nprint(inputs)\r\n```\r\n\r\n#### Problem description\r\n\r\nI have a code which is replacing a set of columns with another set of columns, based on column indices. To make things done without a special case, I assumes I could just use `iloc` to both select and set columns in a DataFrame. But it seems that this not work and fails in strange ways.\r\n\r\n#### Expected Output\r\n\r\n```\r\n   a1  b1  c1\r\n0   1  11   7\r\n1   2  12   8\r\n2   3  13   9\r\n```\r\n\r\nBut in reality, you get:\r\n\r\n```\r\n    a1  b1   c1\r\n0  1.0 NaN  7.0\r\n1  2.0 NaN  8.0\r\n2  3.0 NaN  9.0\r\n```\r\n\r\nSee how values converted to float and how column is `NaN`s?\r\n\r\nBut, if I do the following I get expected results:\r\n\r\n```\r\ninputs.iloc[:, [1]] = [[11], [12], [13]]\r\n```\r\n\r\nThis also works:\r\n\r\n```\r\ninputs.iloc[:, [1]] = columns.iloc[:, [0]].values\r\n```\r\n\r\nSo if it works with lists and ndarrays, one would assume it would also work with DataFrames themselves. But it does not.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.13.0-46-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.23.3\r\npytest: None\r\npip: 18.0\r\nsetuptools: 40.0.0\r\nCython: None\r\nnumpy: 1.15.0\r\nscipy: None\r\npyarrow: None\r\nxarray: None\r\nIPython: None\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.7.3\r\npytz: 2018.5\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>", "patch": ""}}
{"instance_id": "pandas-dev__pandas-41423", "found_files": [["pandas/core/series.py", "pandas/core/generic.py", "pandas/core/internals/managers.py", "pandas/core/internals/array_manager.py"]], "found_modules": [["pandas/core/generic.py:NDFrame", "pandas/core/internals/managers.py:BaseBlockManager", "pandas/core/internals/array_manager.py:BaseArrayManager"]], "found_entities": [["pandas/core/generic.py:NDFrame.copy", "pandas/core/internals/managers.py:BaseBlockManager.copy", "pandas/core/internals/array_manager.py:BaseArrayManager.copy"]], "raw_output_loc": ["```\npandas/core/series.py\nclass: Series\nfunction: __init__\n\npandas/core/series.py\nclass: Series\n# (Document at class-level and constructor docstring)\n\npandas/core/generic.py\nclass: NDFrame\nfunction: copy\n\npandas/core/internals/managers.py\nclass: BaseBlockManager\nfunction: copy\n\npandas/core/internals/array_manager.py\nclass: BaseArrayManager\nfunction: copy\n```\nThese files and locations are most relevant for clarification and documentation of the `copy` argument for `pandas.Series`. Additionally, documentation changes should be made to the class-level docstring and constructor documentation, and may need referencing the internal managers dealing with `copy` behavior."], "meta_data": {"repo": "pandas-dev/pandas", "base_commit": "896256ee02273bebf723428ee41cab31930a69f4", "problem_statement": "DOC: pandas.Series(data=None, index=None, dtype=None, name=None, copy=False, fastpath=False)\n\nNo proper information on \"copy\" is present under [Documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html)", "patch": ""}}
{"instance_id": "pandas-dev__pandas-49647", "found_files": [[]], "found_modules": [[]], "found_entities": [[]], "raw_output_loc": [], "meta_data": {"repo": "pandas-dev/pandas", "base_commit": "fa78ea801392f4f0d37ea7ddbbfe44e9c8c102bd", "problem_statement": "STYLE place standard library imports at top of file\n\nImports should typically be placed at the top of files. Sometimes, imports are placed inside functions to:\r\n- avoid circular imports\r\n- avoid `ImportError` if it's an optional dependency\r\n\r\nStandard library imports should really always be at the top of files.\r\n\r\nNoticed in https://github.com/pandas-dev/pandas/pull/49645 that this is often not the case\r\n\r\nI've made a script to automate detecting when this is the case. So the task is:\r\n```\r\ngit checkout -b standard-library-imports main\r\ngit pull git@github.com:MarcoGorelli/pandas.git standard-library-imports\r\ngit reset --hard FETCH_HEAD\r\npre-commit run stdlib-imports --all-files\r\n```\r\nThen, fixup any errors that are reported. Finally, stage your changes, commit them, push them to your fork, and open a pull request\r\n\r\nFeel free to reach out if you into any issues along the way\r\n\r\nIf any wants to take this, it would be a nice and welcome clean up!\r\n\r\n---\r\n\r\nEDIT: after going through a PR, I'm not sure it's worth introducing a check for this - but we can still take some of the cleanups it found", "patch": ""}}
{"instance_id": "pallets__flask-3555", "found_files": [["src/flask/json/__init__.py", "src/flask/wrappers.py", "src/flask/json/tag.py", "src/flask/sessions.py", "src/flask/helpers.py"]], "found_modules": [["src/flask/json/__init__.py:dump", "src/flask/json/__init__.py:htmlsafe_dumps", "src/flask/json/__init__.py:htmlsafe_dump", "src/flask/json/__init__.py:jsonify", "src/flask/signals.py:Namespace", "src/flask/wrappers.py:JSONMixin"]], "found_entities": [["src/flask/json/__init__.py:dump", "src/flask/json/__init__.py:htmlsafe_dumps", "src/flask/json/__init__.py:htmlsafe_dump", "src/flask/json/__init__.py:jsonify", "src/flask/signals.py:Namespace.signal", "src/flask/wrappers.py:JSONMixin.on_json_loading_failed"]], "raw_output_loc": ["```\nsrc/flask/json/__init__.py\nfunction: dump\nfunction: htmlsafe_dumps\nfunction: htmlsafe_dump\nfunction: jsonify\n\nsrc/flask/wrappers.py\nclass: JSONMixin\nline: 10\nfunction: on_json_loading_failed\n\nsrc/flask/json/tag.py\n\nsrc/flask/sessions.py\n\nsrc/flask/helpers.py\n```\n"], "meta_data": {"repo": "pallets/flask", "base_commit": "024f0d384cf5bb65c76ac59f8ddce464b2dc2ca1", "problem_statement": "Remove simplejson\n\nIn modern Python it's unlikely to be significantly better than the built-in `json`. The module used by `JSONMixin` is overridable, so users can plug it in again if they want.\r\n\r\nSee pallets/itsdangerous#146 and pallets/werkzeug#1766.", "patch": ""}}
{"instance_id": "pandas-dev__pandas-5420", "found_files": [[]], "found_modules": [[]], "found_entities": [[]], "raw_output_loc": [], "meta_data": {"repo": "pandas-dev/pandas", "base_commit": "324208eaa66a528f1e88f938c71c2d8efb8304f3", "problem_statement": "BUG: loc should not fallback for integer indexing for multi-index\n\nhttps://groups.google.com/forum/m/#!topic/pydata/W0e3l0UvNwI", "patch": ""}}
{"instance_id": "pandas-dev__pandas-3561", "found_files": [[]], "found_modules": [[]], "found_entities": [[]], "raw_output_loc": [], "meta_data": {"repo": "pandas-dev/pandas", "base_commit": "6d2c57fa010c12f21f700034b5651519670b9b9d", "problem_statement": "DataFrame.ix losing row ordering when index has duplicates\n\n``` python\nimport pandas as pd\n\nind = ['A', 'A', 'B', 'C']i\ndf = pd.DataFrame({'test':range(len(ind))}, index=ind)\n\nrows = ['C', 'B']\nres = df.ix[rows]\nassert rows == list(res.index) # fails\n```\n\nThe problem is that the resulting DataFrame keeps the ordering of the `df.index` and not the `rows` key. You'll notice that the `rows` key doesn't reference a duplicate value.", "patch": ""}}
{"instance_id": "pandas-dev__pandas-18734", "found_files": [["pandas/core/apply.py", "pandas/core/frame.py", "pandas/core/nanops.py", "pandas/core/generic.py"]], "found_modules": [["pandas/core/frame.py:DataFrame", "pandas/core/generic.py:NDFrame", "asv_bench/benchmarks/strings.py:Methods", "pandas/io/parquet.py:BaseImpl", "pandas/core/strings/base.py:BaseStringArrayMethods", "pandas/core/internals/array_manager.py:BaseArrayManager", "pandas/core/window/expanding.py:Expanding", "pandas/core/apply.py:Apply", "pandas/io/sas/sas_xport.py:XportReader", "pandas/tseries/frequencies.py:_FrequencyInferer", "asv_bench/benchmarks/categoricals.py:Indexing", "asv_bench/benchmarks/algos/isin.py:IsInLongSeriesValuesDominate", "pandas/io/clipboard/__init__.py:CheckedCall", "asv_bench/benchmarks/arithmetic.py:IndexArithmetic", "pandas/core/indexes/range.py:RangeIndex", "pandas/core/indexes/accessors.py:DatetimeProperties", "asv_bench/benchmarks/frame_methods.py:MaskBool", "asv_bench/benchmarks/rolling.py:table_method_func", "asv_bench/benchmarks/groupby.py:MultiColumn", "pandas/core/computation/ops.py:is_term", "pandas/plotting/_matplotlib/hist.py:_grouped_hist", "asv_bench/benchmarks/indexing.py:SortedAndUnsortedDatetimeIndexLoc", "pandas/core/arrays/interval.py:IntervalArray", "pandas/core/arrays/period.py:PeriodArray", "pandas/io/html.py:_LxmlFrameParser", "pandas/core/computation/expr.py:BaseExprVisitor", "asv_bench/benchmarks/groupby.py:Categories", "pandas/core/reshape/concat.py:_Concatenator", "pandas/core/apply.py:FrameApply", "pandas/core/groupby/ops.py:BaseGrouper", "pandas/io/parsers/python_parser.py:PythonParser", "pandas/core/window/ewm.py:OnlineExponentialMovingWindow", "pandas/io/stata.py:StataMissingValue", "pandas/io/json/_json.py:JsonReader", "pandas/io/formats/format.py:DataFrameFormatter._get_formatted_column_labels", "pandas/core/nanops.py:nanstd"]], "found_entities": [["pandas/core/frame.py:DataFrame.aggregate", "pandas/core/generic.py:NDFrame._stat_function_ddof", "asv_bench/benchmarks/strings.py:Methods.time_isdecimal", "pandas/io/parquet.py:BaseImpl.read", "pandas/core/strings/base.py:BaseStringArrayMethods._str_wrap", "pandas/core/strings/base.py:BaseStringArrayMethods._str_get_dummies", "pandas/core/internals/array_manager.py:BaseArrayManager.get_dtypes", "pandas/core/window/expanding.py:Expanding.aggregate", "pandas/core/apply.py:Apply.agg", "pandas/io/sas/sas_xport.py:XportReader._get_row", "pandas/tseries/frequencies.py:_FrequencyInferer.fields", "asv_bench/benchmarks/categoricals.py:Indexing.time_unique", "asv_bench/benchmarks/algos/isin.py:IsInLongSeriesValuesDominate.time_isin", "pandas/io/clipboard/__init__.py:CheckedCall.__setattr__", "asv_bench/benchmarks/arithmetic.py:IndexArithmetic.time_multiply", "pandas/core/indexes/range.py:RangeIndex.inferred_type", "pandas/core/indexes/accessors.py:DatetimeProperties.freq", "asv_bench/benchmarks/frame_methods.py:MaskBool.time_frame_mask_floats", "asv_bench/benchmarks/rolling.py:table_method_func", "asv_bench/benchmarks/groupby.py:MultiColumn.time_col_select_numpy_sum", "pandas/core/generic.py:NDFrame.attrs", "pandas/core/computation/ops.py:is_term", "pandas/plotting/_matplotlib/hist.py:_grouped_hist.plot_group", "asv_bench/benchmarks/indexing.py:SortedAndUnsortedDatetimeIndexLoc.time_loc_unsorted", "pandas/core/apply.py:Apply.agg_list_like", "pandas/core/arrays/interval.py:IntervalArray.__len__", "pandas/core/arrays/period.py:PeriodArray._add_offset", "pandas/io/html.py:_LxmlFrameParser._text_getter", "pandas/core/computation/expr.py:BaseExprVisitor.translate_In", "asv_bench/benchmarks/groupby.py:Categories.time_groupby_sort", "pandas/core/reshape/concat.py:_Concatenator._maybe_check_integrity", "asv_bench/benchmarks/groupby.py:Categories.time_groupby_nosort", "asv_bench/benchmarks/groupby.py:Categories.time_groupby_ordered_sort", "pandas/core/apply.py:FrameApply.agg", "pandas/core/groupby/ops.py:BaseGrouper.names", "pandas/core/indexes/range.py:RangeIndex.__len__", "pandas/io/parsers/python_parser.py:PythonParser._clear_buffer", "pandas/core/window/ewm.py:OnlineExponentialMovingWindow.cov", "pandas/core/window/ewm.py:OnlineExponentialMovingWindow.var", "pandas/io/stata.py:StataMissingValue.__str__", "pandas/io/json/_json.py:JsonReader._combine_lines", "pandas/io/stata.py:StataMissingValue.__repr__", "pandas/io/formats/format.py:DataFrameFormatter._get_formatted_column_labels.space_format", "pandas/core/nanops.py:nanstd"]], "raw_output_loc": ["```\npandas/core/apply.py\nline: 145-174\nfunction: Apply.agg\n\npandas/core/apply.py\nline: 289-372\nfunction: Apply.agg_list_like\n\npandas/core/apply.py\nline: 695-716\nfunction: FrameApply.agg\n\npandas/core/frame.py\nline: 9568-9590\nfunction: DataFrame.aggregate\n\npandas/core/nanops.py\nline: 894-936\nfunction: nanstd\n\npandas/core/generic.py\nline: 11284-11301\nfunction: NDFrame._stat_function_ddof\n```\n"], "meta_data": {"repo": "pandas-dev/pandas", "base_commit": "bcc5160b3a5b0fc9c531da194c6bb83619045434", "problem_statement": "ddof for np.std in df.agg changes depending on how given & lambda expression does not work correctly in a list of functions\n\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nIn [31]: import numpy as np\r\n\r\nIn [32]: import pandas as pd\r\n\r\nIn [33]: df = pd.DataFrame(np.arange(6).reshape(3, 2), columns=['A', 'B'])\r\n\r\nIn [34]: df\r\nOut[34]:\r\n   A  B\r\n0  0  1\r\n1  2  3\r\n2  4  5\r\n\r\nIn [35]: df.agg(np.std)  # Behavior of ddof=0\r\nOut[35]:\r\nA    1.632993\r\nB    1.632993\r\ndtype: float64\r\n\r\nIn [36]: df.agg([np.std])  # Behavior of ddof=1\r\nOut[36]:\r\n       A    B\r\nstd  2.0  2.0\r\n\r\nIn [37]: # So how to get the ddof=0 behavior when giving a list of functions?\r\n\r\nIn [39]: df.agg([lambda x: np.std(x)])  # This gives a numerically unexpected result.\r\nOut[39]:\r\n         A        B\r\n  <lambda> <lambda>\r\n0      0.0      0.0\r\n1      0.0      0.0\r\n2      0.0      0.0\r\n\r\nIn [40]: df.agg([np.mean, lambda x: np.std(x)])  # This gives an error.\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-40-52f4ec4195b5> in <module>()\r\n----> 1 df.agg([np.mean, lambda x: np.std(x)])\r\n\r\n/Users/ikeda/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/pandas/core/frame.py in aggregate(self, func, axis, *args, **kwargs)\r\n   4740         if axis == 0:\r\n   4741             try:\r\n-> 4742                 result, how = self._aggregate(func, axis=0, *args, **kwargs)\r\n   4743             except TypeError:\r\n   4744                 pass\r\n\r\n/Users/ikeda/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/pandas/core/base.py in _aggregate(self, arg, *args, **kwargs)\r\n    537             return self._aggregate_multiple_funcs(arg,\r\n    538                                                   _level=_level,\r\n--> 539                                                   _axis=_axis), None\r\n    540         else:\r\n    541             result = None\r\n\r\n/Users/ikeda/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/pandas/core/base.py in _aggregate_multiple_funcs(self, arg, _level, _axis)\r\n    594         # if we are empty\r\n    595         if not len(results):\r\n--> 596             raise ValueError(\"no results\")\r\n    597\r\n    598         try:\r\n\r\nValueError: no results\r\n\r\n```\r\n#### Problem description\r\n\r\nWhen using, e.g., `df.agg`, the `ddof` (degrees of freedom) value for the function `np.std` changes depending on how the function is given (single function or a list of functions), which may be so confusing for many people. I believe the behavior should be unified in some way.\r\n\r\nFurthermore, I could not find the way to obtain to the `np.std` result with `ddof=0` by supplying it as one of the members of a list of functions. The `lambda` expression does not work well in a list of functions (this gives numerically unexpected results or even gives errors). This prohibits us to use many useful methods like `df.agg`, `df.apply`, and `df.describe` when we hope the `ddof=0` behavior. \r\n\r\nFrom https://github.com/pandas-dev/pandas/issues/13344, I guess Developers prefer the `ddof=1` behavior in pandas. So the expected behavior should be as below.\r\n\r\n#### Expected Output\r\n```\r\nIn [35]: df.agg(np.std)  # Behavior of ddof=1\r\nOut[35]:\r\nA    2.0\r\nB    2.0\r\ndtype: float64\r\n\r\nIn [38]: df.agg([lambda x: np.std(x)])  # To obtain the ddof=0 results\r\nOut[38]:\r\n                     A             B\r\n<lambda>      1.632993      1.632993\r\n\r\nIn [41]: df.agg([np.mean, lambda x: np.std(x)])\r\n                     A             B\r\nmean          2.0           3.0\r\n<lambda>      1.632993      1.632993\r\n```\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\n\r\npandas: 0.21.0\r\npytest: 3.0.7\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.25.2\r\nnumpy: 1.13.3\r\nscipy: 0.19.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 5.3.0\r\nsphinx: 1.5.6\r\npatsy: 0.4.1\r\ndateutil: 2.6.1\r\npytz: 2017.3\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.3.0\r\nnumexpr: 2.6.2\r\nfeather: None\r\nmatplotlib: 2.0.2\r\nopenpyxl: 2.4.7\r\nxlrd: 1.0.0\r\nxlwt: 1.2.0\r\nxlsxwriter: 0.9.6\r\nlxml: 3.7.3\r\nbs4: 4.6.0\r\nhtml5lib: 0.999\r\nsqlalchemy: 1.1.9\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>", "patch": ""}}
{"instance_id": "scikit-learn__scikit-learn-29742", "found_files": [[]], "found_modules": [[]], "found_entities": [[]], "raw_output_loc": [], "meta_data": {"repo": "scikit-learn/scikit-learn", "base_commit": "c13703c8dfb7324a05a82e8befe9b203a6590257", "problem_statement": "spin docs --no-plot runs the examples\n\nSeen at the EuroScipy sprint\r\n\r\nCommands run by spin:\r\n```\r\n$ export SPHINXOPTS=-W -D plot_gallery=0 -j auto\r\n$ cd doc\r\n$ make html\r\n```\r\n\r\nLooks like our Makefile does not use SPHINXOPTS the same way as expected:\r\nProbably we have a slightly different way of building the doc\r\n\r\n```\r\n\u276f make html-noplot -n\r\nsphinx-build -D plot_gallery=0 -b html -d _build/doctrees  -T  . -jauto \\\r\n    _build/html/stable\r\necho\r\necho \"Build finished. The HTML pages are in _build/html/stable.\"\r\n```", "patch": ""}}
{"instance_id": "huggingface__transformers-15640", "found_files": [[]], "found_modules": [[]], "found_entities": [[]], "raw_output_loc": [], "meta_data": {"repo": "huggingface/transformers", "base_commit": "147c8166852db64de12b851b8307f44c9e8fe0dd", "problem_statement": "Add support for ONNX-TensorRT conversion for GPT-J6B (and possible bug in rotary embedding)\n\n### Who can help\r\n@patil-suraj \r\n\r\n## Information\r\n\r\nModel I am using: GPT-J\r\n\r\nThe problem arises when using:\r\n* [x] the official example scripts: (give details below)\r\n* [x] my own modified scripts: (give details below)\r\n\r\n## Description\r\nI opened this issue for two reasons:\r\n1. This is not strictly a bug report, rather a change that enables converting this model to ONNX and then parsing it using the current TensorRT ONNX parser.\r\n2. Possible implementation bug in GPT-J.\r\n\r\n## Details\r\n1. When exporting GPT-J to ONNX using the latest version (v4.16.2), one of the ops that is exported is [SplitToSequence](https://github.com/onnx/onnx/blob/main/docs/Operators.md#SplitToSequence) (along with more Sequence* ops) that is currently not supported in the [TensorRT ONNX parser](https://github.com/onnx/onnx-tensorrt/blob/master/docs/operators.md).\r\nThis is entirely due to just 1 line of code that uses `torch.repeat_interleave`. ([relevant line](https://github.com/huggingface/transformers/blob/52d2e6f6e904ef9b75c78716ce77b98196ed837a/src/transformers/models/gptj/modeling_gptj.py#L67))\r\n```\r\nsin, cos = map(lambda t: t[None, offset : x.shape[1] + offset, None, :].repeat_interleave(2, 3), sincos)\r\n```\r\nBy replacing `lambda t` with this:\r\n```\r\nlambda t: t.view(-1, 1).repeat(1, 2).view(seq_len, -1)[None, offset : x.shape[1] + offset, None, :]\r\n```\r\nwe get the exact same output tensors but now exporting to ONNX doesn't include any Sequence* ops, and TensorRT can parse it successfully.\r\nThe suggested function is even faster, although probably not critical in this huge model (benched only on CPU):\r\n```\r\noriginal: 106 \u00b5s \u00b1 20.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\nsuggested: 32.4 \u00b5s \u00b1 6.55 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n```\r\n\r\n2. I was following the implementation in EleutherAI for rotary positional embeddings and I'm trying to understand if this is a bug or I'm simply missing something (would love an explanation if you can spare the time) but there (EleutherAI) they implement this function (rotary positional embedding) using `torch.cat` instead of `torch.repeat_interleave`, as can be seen [here](https://github.com/EleutherAI/gpt-neox/blob/b30afd1d0a1d06220be9b5f2c9c9c1523defba96/megatron/model/positional_embeddings.py#L41).\r\n\r\nIf I'm not missing something, the EleutherAI version transforms a tensor from\r\n```\r\n[[1,2,3],\r\n [4,5,6]]\r\n```\r\nto \r\n```\r\n[[1,2,3,1,2,3],\r\n [4,5,6,4,5,6]]\r\n```\r\nand HF version (using repeat_interleave):\r\n```\r\n[[1,2,3],\r\n [4,5,6]]\r\n```\r\nto \r\n```\r\n[[1,1,2,2,3,3],\r\n [4,4,5,5,6,6]]\r\n```\r\nCan anyone confirm the current implementation is indeed correct? Because otherwise `cat` and `repeat_interleave` are very different, and the rest of the implementation doesn't take it into account.", "patch": ""}}
{"instance_id": "scikit-learn__scikit-learn-2185", "found_files": [["sklearn/cluster/k_means_.py", "examples/cluster/plot_dict_face_patches.py", "sklearn/utils/linear_assignment_.py"]], "found_modules": [["sklearn/cluster/k_means_.py:MiniBatchKMeans", "sklearn/cluster/k_means_.py:_init_centroids", "sklearn/utils/linear_assignment_.py:linear_assignment", "sklearn/cluster/k_means_.py:_k_init"]], "found_entities": [["sklearn/cluster/k_means_.py:MiniBatchKMeans.partial_fit", "sklearn/cluster/k_means_.py:MiniBatchKMeans._labels_inertia_minibatch", "sklearn/cluster/k_means_.py:_init_centroids", "sklearn/cluster/k_means_.py:MiniBatchKMeans.fit", "sklearn/utils/linear_assignment_.py:linear_assignment", "sklearn/cluster/k_means_.py:_k_init"]], "raw_output_loc": ["```\nsklearn/cluster/k_means_.py\nclass: MiniBatchKMeans\nfunction: MiniBatchKMeans.partial_fit\nfunction: MiniBatchKMeans._labels_inertia_minibatch\nfunction: _init_centroids\n\nexamples/cluster/plot_dict_face_patches.py\nfunction: main (or the code section where MiniBatchKMeans is used)\n\nsklearn/cluster/k_means_.py\nfunction: MiniBatchKMeans.fit\n\nsklearn/utils/linear_assignment_.py\nfunction: linear_assignment\n\nsklearn/cluster/k_means_.py\nfunction: _k_init\nfunction: _init_centroids\n```\n"], "meta_data": {"repo": "scikit-learn/scikit-learn", "base_commit": "14d03f60ed366df942be09ee4bc394a69958e09c", "problem_statement": "MinibatchKMeans bad center reallocation causes duplicate centers\n\nFor instance have a look at:\n\n  http://scikit-learn.org/dev/auto_examples/cluster/plot_dict_face_patches.html\n\nsome of the centroids are duplicated, presumably because of a bug in the bad cluster reallocation heuristic.", "patch": ""}}
{"instance_id": "huggingface__transformers-8171", "found_files": [[]], "found_modules": [[]], "found_entities": [[]], "raw_output_loc": [], "meta_data": {"repo": "huggingface/transformers", "base_commit": "eb3bd73ce35bfef56eeb722d697f2d39a06a8f8d", "problem_statement": "Need suggestion on contributing TFDPR\n\n# \ud83c\udf1f New model addition\r\n\r\n## Model description\r\nHi, I would love to try contributing TFDPR . This is the first time to me, so I need some suggestions.\r\nI have followed @sshleifer 's [great PR on TFBart model](https://github.com/huggingface/transformers/commit/829842159efeb1f920cbbb1daf5ad67e0114d0b9) on 4 files :` __init__.py , convert_pytorch_checkpoint_to_tf2.py , utils/dummy_tf_objects.py` and (newly created) `modeling_tf_dpr.py `\r\n\r\nNow the TF model works properly and can load Pytorch's weights successfully the same output as Pytorch's counterparts **except** small random noise (1e-5) which I suspect of some dtypes different , but I could not find the cause. \r\n\r\nI guess I need to add document on  docs/source/model_doc/dpr.rst , and that's all ? \r\n**My question is do I need to change / fix any other files ? and/or do I need to do some other thing before making PR ?**\r\n\r\n<!-- Important information -->\r\nTo resolve TF vs. Pytorch naming issues, there's one change regarding `TFBertModel` vs. `TFBertMainLayer` as [discussed here](https://discuss.huggingface.co/t/solved-issue-on-translating-dpr-to-tfdpr-on-loading-pytorch-weights-to-tf-model/1764) .\r\nThanks to @sshleifer for his help to solve the issue.\r\n\r\n## Open source status\r\n\r\n* [X] the model implementation is available: (give details)\r\nYou can see all the modified codes with test run at : \r\nhttps://colab.research.google.com/drive/1lU4fx7zkr-Y3CXa3wmHIY8yJhKdiN3DI?usp=sharing\r\n(to easily navigate the changes, please \u201cfind on page\u201d for e.g. `TFDPRContextEncoder` )\r\n\r\n* [X] the model weights are available: (give details)\r\nAt the moment, I use existing Pytorch weights, but will upload TF weights too.\r\n\r\n* [X] who are the authors: (mention them, if possible by @gh-username)\r\n@ratthachat", "patch": ""}}
{"instance_id": "huggingface__transformers-34390", "found_files": [["src/transformers/models/mask2former/modeling_mask2former.py"]], "found_modules": [["src/transformers/models/mask2former/modeling_mask2former.py:Mask2FormerPixelDecoderEncoderOnly", "src/transformers/models/deprecated/gptsan_japanese/modeling_gptsan_japanese.py:GPTSanJapaneseForConditionalGeneration", "src/transformers/models/blenderbot/modeling_blenderbot.py:BlenderbotForCausalLM"]], "found_entities": [["src/transformers/models/mask2former/modeling_mask2former.py:Mask2FormerPixelDecoderEncoderOnly.get_reference_points", "src/transformers/models/mask2former/modeling_mask2former.py:Mask2FormerPixelDecoderEncoderOnly.forward", "src/transformers/models/deprecated/gptsan_japanese/modeling_gptsan_japanese.py:GPTSanJapaneseForConditionalGeneration.set_output_embeddings", "src/transformers/models/blenderbot/modeling_blenderbot.py:BlenderbotForCausalLM.get_decoder"]], "raw_output_loc": ["```\nsrc/transformers/models/mask2former/modeling_mask2former.py\nline: 1106\nfunction: Mask2FormerPixelDecoderEncoderOnly.get_reference_points\n\nsrc/transformers/models/mask2former/modeling_mask2former.py\nline: 1165\nclass: Mask2FormerPixelDecoderEncoderOnly\nfunction: forward\n\nsrc/transformers/models/mask2former/modeling_mask2former.py\nline: 1319\nclass: Mask2FormerPixelDecoder\nfunction: forward\n\nsrc/transformers/models/mask2former/modeling_mask2former.py\nline: 1395\nclass: Mask2FormerPixelLevelModule\nfunction: forward\n\nsrc/transformers/models/mask2former/modeling_mask2former.py\nline: 2499\nclass: Mask2FormerForUniversalSegmentation\nfunction: forward\n```"], "meta_data": {"repo": "huggingface/transformers", "base_commit": "9bee9ff5db6e68fb31065898d7e924d07c1eb9c1", "problem_statement": "[mask2former] torch.export error for Mask2Former\n\n### System Info\r\n\r\n- `transformers` version: 4.46.0.dev0\r\n- Platform: Linux-6.8.0-47-generic-x86_64-with-glibc2.35\r\n- Python version: 3.11.9\r\n- Huggingface_hub version: 0.25.2\r\n- Safetensors version: 0.4.5\r\n- Accelerate version: not installed\r\n- Accelerate config: not found\r\n- PyTorch version (GPU?): 2.4.1+cu121 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n- Using GPU in script?: <fill in>\r\n- GPU type: NVIDIA GeForce RTX 4090\r\n\r\n### Who can help?\r\n\r\n@amyeroberts, @qubvel, @ylacombe\r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [X] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\n```python\r\nimport torch\r\nfrom transformers import Mask2FormerForUniversalSegmentation\r\n\r\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained(\r\n    \"facebook/mask2former-swin-base-coco-panoptic\", torchscript=True\r\n)\r\n\r\nscripted_model = torch.export.export(model, args=(torch.randn(1, 3, 800, 1280),))\r\n```\r\nwhich causes\r\n```\r\nUserError: Could not extract specialized integer from data-dependent expression u0 (unhinted: u0).  (Size-like symbols: none)\r\n\r\nPotential framework code culprit (scroll up for full backtrace):\r\n  File \"/home/philkuz/.pyenv/versions/3.11.9/envs/gml311/lib/python3.11/site-packages/torch/_dynamo/utils.py\", line 2132, in run_node\r\n    return node.target(*args, **kwargs)\r\n\r\nFor more information, run with TORCH_LOGS=\"dynamic\"\r\nFor extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"u0\"\r\nIf you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\r\nFor more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing\r\n\r\nUser Stack (most recent call last):\r\n  (snipped, see stack below for prefix)\r\n  File \"/home/philkuz/dev/transformers/src/transformers/models/mask2former/modeling_mask2former.py\", line 2499, in forward\r\n    outputs = self.model(\r\n  File \"/home/philkuz/.pyenv/versions/3.11.9/envs/gml311/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/philkuz/dev/transformers/src/transformers/models/mask2former/modeling_mask2former.py\", line 2270, in forward\r\n    pixel_level_module_output = self.pixel_level_module(\r\n  File \"/home/philkuz/.pyenv/versions/3.11.9/envs/gml311/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/philkuz/dev/transformers/src/transformers/models/mask2former/modeling_mask2former.py\", line 1395, in forward\r\n    decoder_output = self.decoder(backbone_features, output_hidden_states=output_hidden_states)\r\n  File \"/home/philkuz/.pyenv/versions/3.11.9/envs/gml311/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/philkuz/dev/transformers/src/transformers/models/mask2former/modeling_mask2former.py\", line 1319, in forward\r\n    encoder_outputs = self.encoder(\r\n  File \"/home/philkuz/.pyenv/versions/3.11.9/envs/gml311/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/philkuz/dev/transformers/src/transformers/models/mask2former/modeling_mask2former.py\", line 1165, in forward\r\n    reference_points = self.get_reference_points(spatial_shapes, valid_ratios, device=inputs_embeds.device)\r\n  File \"/home/philkuz/dev/transformers/src/transformers/models/mask2former/modeling_mask2former.py\", line 1106, in get_reference_points\r\n    torch.linspace(0.5, height - 0.5, height, dtype=valid_ratios.dtype, device=device),\r\n\r\nFor C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\r\nFor more information about this error, see: https://pytorch.org/docs/main/generated/exportdb/index.html#constrain-as-size-example\r\n\r\nfrom user code:\r\n   File \"/home/philkuz/dev/transformers/src/transformers/models/mask2former/modeling_mask2former.py\", line 2499, in forward\r\n    outputs = self.model(\r\n  File \"/home/philkuz/.pyenv/versions/3.11.9/envs/gml311/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/philkuz/dev/transformers/src/transformers/models/mask2former/modeling_mask2former.py\", line 2270, in forward\r\n    pixel_level_module_output = self.pixel_level_module(\r\n  File \"/home/philkuz/.pyenv/versions/3.11.9/envs/gml311/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/philkuz/dev/transformers/src/transformers/models/mask2former/modeling_mask2former.py\", line 1395, in forward\r\n    decoder_output = self.decoder(backbone_features, output_hidden_states=output_hidden_states)\r\n  File \"/home/philkuz/.pyenv/versions/3.11.9/envs/gml311/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/philkuz/dev/transformers/src/transformers/models/mask2former/modeling_mask2former.py\", line 1319, in forward\r\n    encoder_outputs = self.encoder(\r\n  File \"/home/philkuz/.pyenv/versions/3.11.9/envs/gml311/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/philkuz/dev/transformers/src/transformers/models/mask2former/modeling_mask2former.py\", line 1165, in forward\r\n    reference_points = self.get_reference_points(spatial_shapes, valid_ratios, device=inputs_embeds.device)\r\n  File \"/home/philkuz/dev/transformers/src/transformers/models/mask2former/modeling_mask2former.py\", line 1106, in get_reference_points\r\n    torch.linspace(0.5, height - 0.5, height, dtype=valid_ratios.dtype, device=device),\r\n ```\r\n\r\n### Expected behavior\r\n\r\ntorch.export works for this model.", "patch": ""}}
{"instance_id": "pandas-dev__pandas-22471", "found_files": [[]], "found_modules": [[]], "found_entities": [[]], "raw_output_loc": [], "meta_data": {"repo": "pandas-dev/pandas", "base_commit": "953757a3e37ffb80570a20a8eca52dae35fc27bb", "problem_statement": "TST/CLN: remove TestData from frame-tests; replace with fixtures\n\nFollowing review in #22236: \r\n> ok, pls open a new issue that refs this, to remove use of `TestData` in favor of fixtures\r\n\r\nStarted the process in that PR by creating a `conftest.py` that translates all the current attributes of `TestData` to fixtures, with the following \"translation guide\":\r\n\r\n* `frame` -> `float_frame`\r\n* `frame2` -> `float_frame2`\r\n* `intframe` -> `int_frame`\r\n* `tsframe` -> `datetime_frame`\r\n* `mixed_frame` -> `float_string_frame`\r\n* `mixed_float` -> `mixed_float_frame`\r\n* `mixed_float2` -> `mixed_float_frame2`\r\n* `mixed_int` -> `mixed_int_frame`\r\n* `all_mixed` -> `mixed_type_frame`\r\n* `tzframe` -> `timezone_frame`\r\n* `empty` -> `empty_frame`\r\n* `ts1` -> `datetime_series`\r\n* `ts2` -> `datetime_series_short`\r\n* `simple` -> `simple_frame`\r\n\r\nNeed to incrementally replace their usages in `pandas/tests/frame/` (example below).\r\n\r\n- [x] Create `conftest.py` and translate `TestData`-attributes into fixtures (#22236)\r\n- [x] `test_alter_axes.py` (#22236)\r\n- [x] `test_analytics.py` (#22733)\r\n- [x] `test_api.py` (#22738)\r\n- [x] `test_apply.py` (#22735)\r\n- [x] `test_arithmetic.py` (#22736)\r\n- [x] `test_asof.py` (#25628)\r\n- [x] `test_axis_select_reindex.py` (#25627)\r\n- [x] `test_block_internals.py` (#22926)\r\n- [x] `test_combine_concat.py` (#25634)\r\n- [ ] `test_constructors.py` (#25635)\r\n- [ ] `test_convert_to.py`\r\n- [ ] `test_dtypes.py` (#25636)\r\n- [x] `test_duplicates.py`\r\n- [x] `test_indexing.py` (#25633)\r\n- [x] `test_join.py` (#25639)\r\n- [x] `test_missing.py` (#25640)\r\n- [x] `test_mutate_columns.py` (#25642)\r\n- [ ] `test_nonunique_indexes.py`\r\n- [x] `test_operators.py` (#25641)\r\n- [ ] `test_period.py`\r\n- [ ] `test_quantile.py`\r\n- [ ] `test_query_eval.py`\r\n- [ ] `test_rank.py`\r\n- [ ] `test_replace.py`\r\n- [ ] `test_repr_info.py`\r\n- [ ] `test_reshape.py`\r\n- [ ] `test_sort_values_level_as_str.py`\r\n- [ ] `test_sorting.py`\r\n- [ ] `test_subclass.py`\r\n- [ ] `test_timeseries.py`\r\n- [ ] `test_timezones.py`\r\n- [ ] `test_to_csv.py`\r\n- [ ] `test_validate.py`\r\n\r\nThings for follow-ups:\r\n- Remove other class-based test-methods\r\n- Turn tests from class- to function-based\r\n\r\nAn example from #22236 - before:\r\n```\r\ndef test_set_columns(self):\r\n    cols = Index(np.arange(len(self.mixed_frame.columns)))\r\n    self.mixed_frame.columns = cols\r\n    with tm.assert_raises_regex(ValueError, 'Length mismatch'):\r\n        self.mixed_frame.columns = cols[::2]\r\n```\r\nAfter:\r\n```\r\ndef test_set_columns(self, float_string_frame):\r\n    cols = Index(np.arange(len(float_string_frame.columns)))\r\n    float_string_frame.columns = cols\r\n    with tm.assert_raises_regex(ValueError, 'Length mismatch'):\r\n        float_string_frame.columns = cols[::2]\r\n```\r\n\r\nBasically, it comes down to replacing all the occurrences of `self.<name>` with `translation_guide[<name>]` (and specifying`<name>` as a parameter to the function).\r\n\r\nPS. Note that some fixtures added by #22236 have now been removed by #24885. Please check #24885 which code was removed, in case you should need it for the fixturisation. Alternatively, you can ping me, @jbrockmendel or @jreback.", "patch": ""}}
{"instance_id": "scikit-learn__scikit-learn-2372", "found_files": [["sklearn/cross_validation.py", "sklearn/grid_search.py"]], "found_modules": [["sklearn/cross_validation.py:cross_val_score", "sklearn/ensemble/gradient_boosting.py:BinomialDeviance", "sklearn/naive_bayes.py:BaseDiscreteNB", "sklearn/hmm.py:_BaseHMM", "sklearn/base.py:BiclusterMixin", "sklearn/grid_search.py:BaseSearchCV", "sklearn/utils/__init__.py:tosequence", "sklearn/externals/six.py:advance_iterator", "sklearn/cross_validation.py:StratifiedKFold", "sklearn/ensemble/gradient_boosting.py:MultinomialDeviance", "sklearn/ensemble/gradient_boosting.py:HuberLossFunction", "sklearn/decomposition/fastica_.py:fastica", "sklearn/base.py:BaseEstimator", "sklearn/multiclass.py:OneVsRestClassifier", "sklearn/covariance/graph_lasso_.py:GraphLasso", "sklearn/feature_selection/univariate_selection.py:_BaseFilter", "sklearn/ensemble/forest.py:BaseForest", "sklearn/ensemble/bagging.py:BaseBagging", "sklearn/multiclass.py:_fit_ovo_binary", "sklearn/grid_search.py:_CVScoreTuple", "sklearn/cross_validation.py:KFold", "sklearn/externals/six.py:exec_", "sklearn/externals/six.py:print_", "sklearn/feature_selection/univariate_selection.py:SelectFwe", "sklearn/decomposition/fastica_.py:FastICA", "sklearn/externals/joblib/parallel.py:Parallel", "sklearn/naive_bayes.py:BernoulliNB", "sklearn/linear_model/ridge.py:RidgeClassifier", "sklearn/multiclass.py:OutputCodeClassifier", "sklearn/linear_model/ridge.py:_RidgeGCV", "sklearn/linear_model/coordinate_descent.py:ElasticNet", "sklearn/mixture/dpgmm.py:VBGMM", "sklearn/feature_extraction/text.py:CountVectorizer", "sklearn/grid_search.py:GridSearchCV"]], "found_entities": [["sklearn/cross_validation.py:cross_val_score", "sklearn/ensemble/gradient_boosting.py:BinomialDeviance.init_estimator", "sklearn/ensemble/gradient_boosting.py:BinomialDeviance.__call__", "sklearn/naive_bayes.py:BaseDiscreteNB._get_coef", "sklearn/hmm.py:_BaseHMM.predict_proba", "sklearn/ensemble/gradient_boosting.py:BinomialDeviance.negative_gradient", "sklearn/naive_bayes.py:BaseDiscreteNB._get_intercept", "sklearn/base.py:BiclusterMixin.biclusters_", "sklearn/grid_search.py:BaseSearchCV.score", "sklearn/utils/__init__.py:tosequence", "sklearn/externals/six.py:advance_iterator", "sklearn/cross_validation.py:StratifiedKFold.__repr__", "sklearn/ensemble/gradient_boosting.py:MultinomialDeviance.__call__", "sklearn/cross_validation.py:StratifiedKFold.__len__", "sklearn/ensemble/gradient_boosting.py:HuberLossFunction.__call__", "sklearn/decomposition/fastica_.py:fastica.g", "sklearn/base.py:BaseEstimator.__repr__", "sklearn/base.py:BaseEstimator.__str__", "sklearn/multiclass.py:OneVsRestClassifier.multilabel_", "sklearn/multiclass.py:OneVsRestClassifier.score", "sklearn/covariance/graph_lasso_.py:GraphLasso.fit", "sklearn/multiclass.py:OneVsRestClassifier.classes_", "sklearn/feature_selection/univariate_selection.py:_BaseFilter.fit", "sklearn/multiclass.py:OneVsRestClassifier.coef_", "sklearn/ensemble/forest.py:BaseForest._set_oob_score", "sklearn/multiclass.py:OneVsRestClassifier.intercept_", "sklearn/ensemble/bagging.py:BaseBagging._set_oob_score", "sklearn/multiclass.py:_fit_ovo_binary", "sklearn/ensemble/bagging.py:BaseBagging._validate_y", "sklearn/grid_search.py:_CVScoreTuple.__repr__", "sklearn/cross_validation.py:KFold.__len__", "sklearn/ensemble/gradient_boosting.py:HuberLossFunction.init_estimator", "sklearn/externals/six.py:exec_", "sklearn/hmm.py:_BaseHMM._do_viterbi_pass", "sklearn/hmm.py:_BaseHMM._do_forward_pass", "sklearn/externals/six.py:print_.write", "sklearn/feature_selection/univariate_selection.py:SelectFwe._get_support_mask", "sklearn/decomposition/fastica_.py:FastICA.sources_", "sklearn/hmm.py:_BaseHMM._do_backward_pass", "sklearn/externals/joblib/parallel.py:Parallel.__repr__", "sklearn/naive_bayes.py:BernoulliNB._count", "sklearn/hmm.py:_BaseHMM._init", "sklearn/naive_bayes.py:BernoulliNB._update_feature_log_prob", "sklearn/linear_model/ridge.py:RidgeClassifier.classes_", "sklearn/naive_bayes.py:BernoulliNB._joint_log_likelihood", "sklearn/hmm.py:_BaseHMM._do_mstep", "sklearn/multiclass.py:OutputCodeClassifier.fit", "sklearn/linear_model/ridge.py:_RidgeGCV._decomp_diag", "sklearn/linear_model/coordinate_descent.py:ElasticNet.sparse_coef_", "sklearn/linear_model/ridge.py:_RidgeGCV._errors", "sklearn/mixture/dpgmm.py:VBGMM.eval", "sklearn/feature_extraction/text.py:CountVectorizer._sort_features", "sklearn/grid_search.py:GridSearchCV.fit"]], "raw_output_loc": ["```\nsklearn/cross_validation.py\nline: 335-403\nclass: StratifiedKFold\n\nsklearn/cross_validation.py\nline: 252-332\nclass: KFold\n\nsklearn/cross_validation.py\nline: 230-249\nclass: _BaseKFold\n\nsklearn/cross_validation.py\nline: 1047-1127\nfunction: cross_val_score\n\nsklearn/grid_search.py\nline: 507-669\nclass: GridSearchCV\n```"], "meta_data": {"repo": "scikit-learn/scikit-learn", "base_commit": "130601e076ec5ca8298b95c3d02122ac5d8cf8eb", "problem_statement": "StratifiedKFold should do its best to preserve the dataset dependency structure\n\nAs highlighted in this [notebook](http://nbviewer.ipython.org/urls/raw.github.com/ogrisel/notebooks/master/Non%2520IID%2520cross-validation.ipynb) the current implementation of `StratifiedKFold` (which is used by default by `cross_val_score` and `GridSearchCV` for classification problems) breaks the dependency structure of the dataset by computing the folds based on the sorted labels.\n\nInstead one should probably do an implementation that performs individual dependency preserving KFold on for each possible label value and aggregate the folds to get the `StratifiedKFold` final folds.\n\nThis might incur a refactoring to get rid of the `_BaseKFold` base class. It might also make it easier to implement a `shuffle=True` option for `StratifiedKFold`.", "patch": ""}}
{"instance_id": "pandas-dev__pandas-26139", "found_files": [[]], "found_modules": [[]], "found_entities": [[]], "raw_output_loc": [], "meta_data": {"repo": "pandas-dev/pandas", "base_commit": "dc86509b44b3fb0cd9a1a6d6ed564b082dc50848", "problem_statement": "Doc for HDFStore compression unclear on what the default value of None does\n\nThe doc for the `HDFStore` class mentions:\r\n\r\n```    \r\ncomplevel : int, 0-9, default None\r\n            Specifies a compression level for data.\r\n            A value of 0 disables compression.\r\n```\r\n\r\nThat doesn't actually answer the question of what compression level is used when the default (None) is used, though. Is None translated further down to 0? it turns out yes, but you have to dig in the code to actually figure that out. And it could as well have been translated eventually to any other value.\r\n\r\nTwo options:\r\n1. Actually change the default in the `complevel` argument to be \"0\". (It's an immutable object, so it's fine as a default value for a function argument.)\r\n2. Just adjust the doc in some way.\r\n\r\nWhen the right solution is decided, I can do a pull request with it. Thanks!", "patch": ""}}
{"instance_id": "localstack__localstack-4517", "found_files": [[]], "found_modules": [[]], "found_entities": [[]], "raw_output_loc": [], "meta_data": {"repo": "localstack/localstack", "base_commit": "9f1b9dbf60f406e8d6205402b8ac078195cd0c01", "problem_statement": "bug: AWS::NoValue produces error when used in IAM policy template\n\n### Is there an existing issue for this?\r\n\r\n- [x] I have searched the existing issues\r\n\r\n### Current Behavior\r\n\r\nWhen I try to create a role with S3 resource and I use `!Ref AWS::NoValue` for its resource, it fails with errors. It is supposed to be removed from array entry, but it looks like it evaluates as `__aws_no_value__`, which then fails to validate because the value is not in acceptable format for ARN.\r\n(Message: `Resource __aws_no_value__ must be in ARN format or \"*\".`)\r\n\r\ntemplate file `test.template` :\r\n```\r\nAWSTemplateFormatVersion: 2010-09-09\r\n\r\nConditions:\r\n  someCondition: false\r\n\r\nResources:\r\n  SomeRole:\r\n    Type: AWS::IAM::Role\r\n    Properties:\r\n      RoleName: SomeRole\r\n      AssumeRolePolicyDocument:\r\n        Version: 2012-10-17\r\n        Statement:\r\n          - Effect: Allow\r\n            Principal:\r\n              Service:\r\n                - lambda.amazonaws.com\r\n            Action:\r\n              - sts:AssumeRole\r\n      Policies:\r\n        - PolicyName: SomePolicy\r\n          PolicyDocument:\r\n            Version: 2012-10-17\r\n            Statement:\r\n              - Effect: Allow\r\n                Action:\r\n                  - s3:GetObject\r\n                  - s3:GetObjectVersion\r\n                Resource:\r\n                  - arn:aws:s3:::some-prefix-*/*\r\n                  - !If\r\n                    - someCondition\r\n                    - !Ref arn:aws:s3:::another-prefix-*/*\r\n                    - !Ref AWS::NoValue\r\n```\r\nExecuted command:\r\n```\r\nawslocal cloudformation deploy \\\r\n  --no-fail-on-empty-changeset \\\r\n  --capabilities CAPABILITY_NAMED_IAM \\\r\n  --template-file test.template \\\r\n  --stack-name \"test-stack\"\r\n```\r\nError log produced:\r\n```\r\n2021-08-30T07:38:54:DEBUG:localstack.utils.cloudformation.template_deployer: Error applying changes for CloudFormation stack \"test-resources-iam\": An error occurred (MalformedPolicyDocument) when calling the PutRolePolicy operation: Resource __aws_no_value__ must be in ARN format or \"*\". Traceback (most recent call last):\r\n  File \"/opt/code/localstack/localstack/utils/cloudformation/template_deployer.py\", line 2083, in _run\r\n    self.do_apply_changes_in_loop(changes, stack, stack_name)\r\n  File \"/opt/code/localstack/localstack/utils/cloudformation/template_deployer.py\", line 2154, in do_apply_changes_in_loop\r\n    self.apply_change(change, stack, new_resources, stack_name=stack_name)\r\n  File \"/opt/code/localstack/localstack/utils/cloudformation/template_deployer.py\", line 2218, in apply_change\r\n    result = deploy_resource(resource_id, new_resources, stack_name)\r\n  File \"/opt/code/localstack/localstack/utils/cloudformation/template_deployer.py\", line 1037, in deploy_resource\r\n    return execute_resource_action(resource_id, resources, stack_name, ACTION_CREATE)\r\n  File \"/opt/code/localstack/localstack/utils/cloudformation/template_deployer.py\", line 1152, in execute_resource_action\r\n    resource_id, resources, resource_type, func, stack_name, action_name\r\n  File \"/opt/code/localstack/localstack/utils/cloudformation/template_deployer.py\", line 1314, in configure_resource_via_sdk\r\n    run_post_create_actions(action_name, resource_id, resources, resource_type, stack_name, result)\r\n  File \"/opt/code/localstack/localstack/utils/cloudformation/template_deployer.py\", line 1414, in run_post_create_actions\r\n    PolicyDocument=doc,\r\n  File \"/opt/code/localstack/.venv/lib/python3.7/site-packages/botocore/client.py\", line 386, in _api_call\r\n    return self._make_api_call(operation_name, kwargs)\r\n  File \"/opt/code/localstack/.venv/lib/python3.7/site-packages/botocore/client.py\", line 705, in _make_api_call\r\n    raise error_class(parsed_response, operation_name)\r\nbotocore.errorfactory.MalformedPolicyDocumentException: An error occurred (MalformedPolicyDocument) when calling the PutRolePolicy operation: Resource __aws_no_value__ must be in ARN format or \"*\".\r\n```\r\n\r\n### Expected Behavior\r\n\r\nCreate stack without failing\r\n\r\n### How are you starting LocalStack?\r\n\r\nWith the `localstack` script\r\n\r\n### Steps To Reproduce\r\n\r\n#### How are you starting localstack (e.g., `bin/localstack` command, arguments, or `docker-compose.yml`)\r\n\r\n```\r\nFORCE_NONINTERACTIVE=1 \\\r\nSERVICES=iam,s3,lambda,cloudformation \\\r\nlocalstack infra start &\r\n```\r\n\r\n#### Client commands (e.g., AWS SDK code snippet, or sequence of \"awslocal\" commands)\r\n\r\n```\r\nawslocal cloudformation deploy \\\r\n  --no-fail-on-empty-changeset \\\r\n  --capabilities CAPABILITY_NAMED_IAM \\\r\n  --template-file test.template \\\r\n  --stack-name \"test-stack\"\r\n```\r\n\r\n\r\n### Environment\r\n\r\n```markdown\r\n- OS:  Ubuntu 20.04\r\n- LocalStack: latest\r\n```\r\n\r\n\r\n### Anything else?\r\n\r\n_No response_", "patch": ""}}
{"instance_id": "scikit-learn__scikit-learn-3689", "found_files": [["sklearn/cross_validation.py", "sklearn/grid_search.py", "sklearn/base.py", "sklearn/random_projection.py"]], "found_modules": [["sklearn/cross_validation.py:_check_is_partition", "sklearn/hmm.py:GMMHMM", "sklearn/cross_validation.py:cross_val_score", "sklearn/grid_search.py:ParameterGrid", "examples/hetero_feature_union.py:SubjectBodyExtractor", "sklearn/externals/six.py:MovedModule", "examples/applications/plot_out_of_core_classification.py:ReutersParser", "sklearn/random_projection.py:sparse_random_matrix", "sklearn/multiclass.py:_ConstantPredictor", "examples/text/document_classification_20newsgroups.py:trim", "sklearn/svm/base.py:BaseLibSVM", "sklearn/pipeline.py:Pipeline", "sklearn/externals/joblib/memory.py:MemorizedResult", "sklearn/feature_extraction/text.py:VectorizerMixin", "sklearn/feature_selection/rfe.py:RFE", "examples/applications/svm_gui.py:View", "sklearn/cross_validation.py:LeavePOut", "doc/conf.py:setup", "sklearn/externals/joblib/memory.py:NotMemorizedResult", "sklearn/ensemble/gradient_boosting.py:LeastSquaresError", "sklearn/externals/joblib/numpy_pickle.py:NumpyPickler", "doc/sphinxext/numpy_ext/docscrape.py:NumpyDocString._parse_index", "sklearn/base.py:BaseEstimator", "sklearn/cluster/affinity_propagation_.py:AffinityPropagation", "sklearn/decomposition/fastica_.py:fastica", "sklearn/ensemble/gradient_boosting.py:LeastAbsoluteError", "sklearn/grid_search.py:_CVScoreTuple", "sklearn/ensemble/forest.py:BaseForest"]], "found_entities": [["sklearn/cross_validation.py:_check_is_partition", "sklearn/hmm.py:GMMHMM._initialize_sufficient_statistics", "sklearn/cross_validation.py:cross_val_score", "sklearn/grid_search.py:ParameterGrid.__len__", "examples/hetero_feature_union.py:SubjectBodyExtractor.fit", "sklearn/externals/six.py:MovedModule._resolve", "examples/applications/plot_out_of_core_classification.py:ReutersParser.start_reuters", "sklearn/random_projection.py:sparse_random_matrix", "sklearn/multiclass.py:_ConstantPredictor.predict_proba", "examples/text/document_classification_20newsgroups.py:trim", "sklearn/svm/base.py:BaseLibSVM._validate_targets", "sklearn/pipeline.py:Pipeline.predict_proba", "sklearn/externals/joblib/memory.py:MemorizedResult.get", "sklearn/feature_extraction/text.py:VectorizerMixin.get_stop_words", "sklearn/feature_selection/rfe.py:RFE.predict_log_proba", "sklearn/externals/joblib/memory.py:MemorizedResult.__reduce__", "examples/applications/svm_gui.py:View.plot_support_vectors", "sklearn/cross_validation.py:LeavePOut.__len__", "doc/conf.py:setup", "sklearn/externals/joblib/memory.py:NotMemorizedResult.clear", "sklearn/externals/joblib/memory.py:NotMemorizedResult.__repr__", "sklearn/ensemble/gradient_boosting.py:LeastSquaresError.__call__", "sklearn/externals/joblib/numpy_pickle.py:NumpyPickler.close", "sklearn/externals/joblib/memory.py:NotMemorizedResult.__getstate__", "sklearn/ensemble/gradient_boosting.py:LeastSquaresError.negative_gradient", "doc/sphinxext/numpy_ext/docscrape.py:NumpyDocString._parse_index.strip_each_in", "sklearn/base.py:BaseEstimator.__repr__", "sklearn/cluster/affinity_propagation_.py:AffinityPropagation._pairwise", "sklearn/ensemble/gradient_boosting.py:LeastSquaresError._update_terminal_region", "sklearn/feature_extraction/text.py:VectorizerMixin.fixed_vocabulary", "sklearn/decomposition/fastica_.py:fastica.g", "sklearn/ensemble/gradient_boosting.py:LeastAbsoluteError.init_estimator", "sklearn/grid_search.py:_CVScoreTuple.__repr__", "sklearn/ensemble/forest.py:BaseForest._set_oob_score"]], "raw_output_loc": ["```\nsklearn/cross_validation.py\nline: 1224-1225\nfunction: _check_is_partition\n\nsklearn/cross_validation.py\nline: 1227\nfunction: cross_val_score\n\nsklearn/grid_search.py\nclass: ParameterGrid\nfunction: __len__\nline: 104-109\n\nsklearn/base.py\n(No direct hit, but relevant for how fit_params and sparse matrices are handled)\n\nsklearn/random_projection.py\nfunction: sparse_random_matrix\nline: 191-280\n```\n"], "meta_data": {"repo": "scikit-learn/scikit-learn", "base_commit": "439c19596a248a31cd1aa8220f54a622a0322160", "problem_statement": "using sparse matrix in fit_params\n\nWhen the value of a fit_params is sparse matrix, it will raise error from the following code.\nsklearn/cross_validation.py\n\n```\n1224                       if hasattr(v, '__len__') and len(v) == n_samples else v)\n1225                       for k, v in fit_params.items()])\n```\n\nIt is because the `__len__` of sparse matrix is defined as\nscipy/sparse/base.py\n\n```\n190    def __len__(self):\n191        # return self.getnnz()\n192        raise TypeError(\"sparse matrix length is ambiguous; use getnnz()\"\n193                         \" or shape[0]\")\n```\n\nIs there anyway to circumpass this issue. I do not want to convert the sparse matrix into a dense one, since it will consume a big memory.", "patch": ""}}
{"instance_id": "scikit-learn__scikit-learn-9174", "found_files": [["sklearn/svm/classes.py", "sklearn/svm/base.py", "sklearn/multiclass.py", "sklearn/utils/multiclass.py"]], "found_modules": [["sklearn/svm/base.py:BaseSVC", "sklearn/svm/base.py:BaseLibSVM", "sklearn/multiclass.py:OneVsOneClassifier", "sklearn/multiclass.py:_predict_binary", "sklearn/utils/multiclass.py:_ovr_decision_function"]], "found_entities": [["sklearn/svm/base.py:BaseSVC.decision_function", "sklearn/svm/base.py:BaseLibSVM._decision_function", "sklearn/svm/base.py:BaseLibSVM._dense_decision_function", "sklearn/svm/base.py:BaseLibSVM._sparse_decision_function", "sklearn/multiclass.py:OneVsOneClassifier.decision_function", "sklearn/multiclass.py:_predict_binary", "sklearn/utils/multiclass.py:_ovr_decision_function", "sklearn/svm/base.py:BaseSVC.predict", "sklearn/svm/base.py:BaseLibSVM.predict"]], "raw_output_loc": ["```\nsklearn/svm/classes.py\nclass: SVC\nfunction: SVC.decision_function\n\nsklearn/svm/base.py\nclass: BaseSVC\nfunction: BaseSVC.decision_function\nfunction: BaseLibSVM._decision_function\nfunction: BaseLibSVM._dense_decision_function\nfunction: BaseLibSVM._sparse_decision_function\n\nsklearn/multiclass.py\nclass: OneVsOneClassifier\nfunction: OneVsOneClassifier.decision_function\nfunction: _predict_binary\n\nsklearn/utils/multiclass.py\nfunction: _ovr_decision_function\n\nsklearn/svm/base.py\nfunction: BaseSVC.predict\nfunction: BaseLibSVM.predict\n```\n"], "meta_data": {"repo": "scikit-learn/scikit-learn", "base_commit": "adc1e590d4dc1e230b49a4c10b4cd7b672bb3d69", "problem_statement": "SVC and OneVsOneClassifier decision_function inconsistent on sub-sample\n\nHi,\r\n\r\nI'm seeing inconsistent numerical results with SVC's decision_function.\r\nWhen estimated over an entire batch of samples ( (n_samples, n_features) matrix ) compared to analyzing sample-by-sample, the results are not the same.\r\nThis is true for both the individual numerical values per sample and the overall distribution of the results.\r\n\r\n**The model is SVC with RBF kernel, for a 3-class classification:**\r\n```\r\nSVC(C=1.0, gamma=0.007, class_weight = new_class_weight, probability = True, random_state = 30, \r\ndecision_function_shape = 'ovr')\r\n```\r\n\r\n**The models are loaded from file:**\r\n\r\n`ML = joblib.load(\"model.pkl\")`\r\n\r\n**Option A, analyze a matrix:**\r\n\r\n`distances = ML.decision_function(X)`\r\n\r\n**Option B, analyze individual samples:**    \r\n```\r\ndistances = numpy.zeros([X.shape[0], 3])\r\nfor i in range(X.shape[0]):     \r\n    distances[i,:]` = ML.decision_function(X[i,:].reshape(1,-1))\r\n```\r\n\r\n**Output for first two samples:**\r\n**Option A:**\r\nsample 1: [ 0.90835588, -0.17305875,  2.26470288]\r\nsample 2: [ 1.10437313, -0.2371539 ,  2.13278077]\r\n\r\n**Option B:**\r\nsample 1: [ 0.82689247, -0.32689247,  2.5       ]\r\nsample 2: [ 1.22005359, -0.5       ,  2.27994641]\r\n\r\nI couldn't find any indication for this behavior in the documentation.\r\n\r\nWindows-10-10.0.15063-SP0\r\nPython 3.5.2 |Anaconda 4.2.0 (64-bit)| (default, Jul  5 2016, 11:41:13) [MSC v.1900 64 bit (AMD64)]\r\nNumPy 1.12.1\r\nSciPy 0.18.1\r\nScikit-Learn 0.18.1\r\n\r\nThanks!", "patch": ""}}
{"instance_id": "scikit-learn__scikit-learn-10059", "found_files": [["sklearn/cluster/k_means_.py"]], "found_modules": [["sklearn/cluster/k_means_.py:KMeans", "sklearn/gaussian_process/kernels.py:Exponentiation", "sklearn/cross_validation.py:ShuffleSplit", "sklearn/cross_decomposition/pls_.py:PLSSVD", "sklearn/preprocessing/data.py:maxabs_scale", "sklearn/discriminant_analysis.py:QuadraticDiscriminantAnalysis", "sklearn/svm/base.py:BaseSVC", "sklearn/model_selection/_split.py:StratifiedKFold", "sklearn/externals/joblib/parallel.py:Parallel", "sklearn/externals/funcsigs.py:Signature", "sklearn/ensemble/gradient_boosting.py:ExponentialLoss", "sklearn/cross_validation.py:LeaveOneLabelOut", "sklearn/pipeline.py:FeatureUnion", "sklearn/feature_selection/univariate_selection.py:SelectFwe", "sklearn/utils/estimator_checks.py:check_transformer_general", "sklearn/mixture/gaussian_mixture.py:GaussianMixture", "sklearn/utils/estimator_checks.py:check_transformer_data_not_an_array", "sklearn/cluster/k_means_.py:_init_centroids", "sklearn/ensemble/bagging.py:BaggingClassifier", "sklearn/externals/joblib/memory.py:MemorizedFunc", "sklearn/naive_bayes.py:BaseDiscreteNB", "sklearn/cluster/k_means_.py:_labels_inertia", "sklearn/feature_extraction/text.py:HashingVectorizer", "sklearn/cluster/hierarchical.py:_average_linkage", "sklearn/externals/joblib/numpy_pickle_utils.py:BinaryZlibFile", "sklearn/grid_search.py:BaseSearchCV", "sklearn/externals/six.py:with_metaclass", "sklearn/pipeline.py:_fit_one_transformer", "sklearn/cluster/k_means_.py:_labels_inertia_precompute_dense"]], "found_entities": [["sklearn/cluster/k_means_.py:KMeans._check_fit_data", "sklearn/gaussian_process/kernels.py:Exponentiation.__eq__", "sklearn/cross_validation.py:ShuffleSplit.__repr__", "sklearn/cross_decomposition/pls_.py:PLSSVD.fit_transform", "sklearn/cross_validation.py:ShuffleSplit.__len__", "sklearn/preprocessing/data.py:maxabs_scale", "sklearn/cluster/k_means_.py:KMeans.fit", "sklearn/discriminant_analysis.py:QuadraticDiscriminantAnalysis.covariances_", "sklearn/svm/base.py:BaseSVC._predict_log_proba", "sklearn/model_selection/_split.py:StratifiedKFold._iter_test_masks", "sklearn/externals/joblib/parallel.py:Parallel._print", "sklearn/externals/funcsigs.py:Signature.__ne__", "sklearn/ensemble/gradient_boosting.py:ExponentialLoss._score_to_proba", "sklearn/cross_validation.py:LeaveOneLabelOut._iter_test_masks", "sklearn/pipeline.py:FeatureUnion._validate_transformers", "sklearn/feature_selection/univariate_selection.py:SelectFwe._get_support_mask", "sklearn/pipeline.py:FeatureUnion._iter", "sklearn/utils/estimator_checks.py:check_transformer_general", "sklearn/mixture/gaussian_mixture.py:GaussianMixture._estimate_log_weights", "sklearn/mixture/gaussian_mixture.py:GaussianMixture._compute_lower_bound", "sklearn/utils/estimator_checks.py:check_transformer_data_not_an_array", "sklearn/discriminant_analysis.py:QuadraticDiscriminantAnalysis._decision_function", "sklearn/cluster/k_means_.py:_init_centroids", "sklearn/ensemble/bagging.py:BaggingClassifier._validate_estimator", "sklearn/externals/funcsigs.py:Signature.parameters", "sklearn/externals/joblib/memory.py:MemorizedFunc._hash_func", "sklearn/ensemble/gradient_boosting.py:ExponentialLoss.init_estimator", "sklearn/ensemble/gradient_boosting.py:ExponentialLoss.__call__", "sklearn/naive_bayes.py:BaseDiscreteNB._get_coef", "sklearn/ensemble/gradient_boosting.py:ExponentialLoss.negative_gradient", "sklearn/naive_bayes.py:BaseDiscreteNB._get_intercept", "sklearn/cluster/k_means_.py:_labels_inertia", "sklearn/feature_extraction/text.py:HashingVectorizer._get_hasher", "sklearn/cluster/hierarchical.py:_average_linkage", "sklearn/externals/joblib/numpy_pickle_utils.py:BinaryZlibFile._rewind", "sklearn/grid_search.py:BaseSearchCV.inverse_transform", "sklearn/externals/joblib/parallel.py:Parallel.__enter__", "sklearn/externals/joblib/memory.py:MemorizedFunc.__call__", "sklearn/externals/six.py:with_metaclass", "sklearn/pipeline.py:_fit_one_transformer", "sklearn/externals/joblib/parallel.py:Parallel._terminate_backend", "sklearn/cluster/k_means_.py:_labels_inertia_precompute_dense"]], "raw_output_loc": ["```\nsklearn/cluster/k_means_.py\nline: 875-897\nfunction: KMeans.fit\n\nsklearn/cluster/k_means_.py\nline: 856-862\nfunction: KMeans._check_fit_data\n\nsklearn/cluster/k_means_.py\nline: 626-704\nfunction: _init_centroids\n\nsklearn/cluster/k_means_.py\nline: 573-623\nfunction: _labels_inertia\n\nsklearn/cluster/k_means_.py\nline: 529-570\nfunction: _labels_inertia_precompute_dense\n```"], "meta_data": {"repo": "scikit-learn/scikit-learn", "base_commit": "effd75dda5f4afa61f988035ff8fe4b3a447464e", "problem_statement": "Duplicated input points silently create duplicated clusters in KMeans\n\n#### Description\r\nWhen there are duplicated input points to Kmeans resulting to number of unique points < number of requested clusters, there is no error thrown. Instead, clustering continues to (seemingly) produce the number of clusters requested, but some of them are exactly the same, so the cluster labels produced for the input points do not go all the way to number of requested clusters.\r\n\r\n#### Steps/Code to Reproduce\r\n```python\r\nfrom sklearn.cluster import KMeans\r\nimport numpy as np\r\n\r\n# some input points here are identical, so that n_total=17, n_unique=9\r\nx2d = np.array([(1086, 348), (1087, 347), (1190, 244), (1190, 244), (1086, 348), (1185, 249), (1193, 241), (1185, 249), (1087, 347), (1188, 247), (1187, 233), (26, 111), (26, 111), (26, 110), (26, 110), (26, 110), (26, 110)])\r\nkmeans = KMeans(n_clusters=10) # n_clusters > n_unique\r\nc_labels = kmeans.fit_predict(x2d)\r\nc_centers = kmeans.cluster_centers_\r\n```\r\n#### Expected Results\r\nEither an error thrown, or the cluster labels produced should match the unique clusters only (i.e. no identical cluster centres)\r\n\r\n#### Actual Results\r\n```python\r\n>>> c_labels  # note there's no entry for cluster 9\r\narray([7, 2, 6, 6, 7, 5, 4, 5, 2, 1, 3, 8, 8, 0, 0, 0, 0], dtype=int32)\r\n>>> c_centers # two of these 10 clusters have identical centers, so only 9 of them are unique\r\narray([[   26.,   110.],\r\n       [ 1188.,   247.],\r\n       [ 1087.,   347.],\r\n       [ 1187.,   233.],\r\n       [ 1193.,   241.],\r\n       [ 1185.,   249.],\r\n       [ 1190.,   244.],\r\n       [ 1086.,   348.],\r\n       [   26.,   111.],\r\n       [   26.,   110.]]) \r\n```\r\n\r\n#### Versions\r\n```python\r\nDarwin-16.7.0-x86_64-i386-64bit\r\nPython 3.6.1 |Continuum Analytics, Inc.| (default, May 11 2017, 13:04:09)\r\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]\r\nNumPy 1.13.1\r\nSciPy 0.19.1\r\nScikit-Learn 0.18.2\r\n```", "patch": ""}}
{"instance_id": "scikit-learn__scikit-learn-18146", "found_files": [["sklearn/decomposition/_kernel_pca.py", "sklearn/utils/validation.py", "sklearn/metrics/pairwise.py"]], "found_modules": [["sklearn/decomposition/_kernel_pca.py:KernelPCA", "sklearn/ensemble/_voting.py:VotingClassifier", "sklearn/utils/_mocking.py:CheckingClassifier", "sklearn/linear_model/_stochastic_gradient.py:BaseSGD", "sklearn/utils/_mocking.py:NoSampleWeightWrapper", "asv_benchmarks/benchmarks/common.py:Predictor", "asv_benchmarks/benchmarks/linear_model.py:ElasticNetBenchmark", "sklearn/compose/_column_transformer.py:ColumnTransformer", "sklearn/multioutput.py:_MultiOutputEstimator", "sklearn/ensemble/_base.py:_BaseHeterogeneousEnsemble", "doc/tutorial/machine_learning_map/pyparsing.py:ParseBaseException", "sklearn/isotonic.py:IsotonicRegression", "asv_benchmarks/benchmarks/common.py:Transformer", "asv_benchmarks/benchmarks/linear_model.py:LassoBenchmark", "sklearn/compose/_target.py:TransformedTargetRegressor", "examples/release_highlights/plot_release_highlights_0_22_0.py:test_sklearn_compatible_estimator", "sklearn/linear_model/_base.py:LinearModel", "sklearn/decomposition/_fastica.py:_cube", "examples/neighbors/approximate_nearest_neighbors.py:AnnoyTransformer", "sklearn/feature_selection/_rfe.py:RFE", "asv_benchmarks/benchmarks/common.py:Estimator", "asv_benchmarks/benchmarks/linear_model.py:SGDRegressorBenchmark", "sklearn/preprocessing/_label.py:LabelEncoder", "sklearn/utils/validation.py:_check_psd_eigenvalues", "doc/tutorial/machine_learning_map/pyparsing.py:ParserElement", "sklearn/decomposition/_dict_learning.py:SparseCoder", "sklearn/linear_model/_ridge.py:_RidgeGCV", "sklearn/datasets/_base.py:_pkl_filepath", "sklearn/feature_extraction/text.py:CountVectorizer", "sklearn/metrics/_ranking.py:_check_dcg_target_type", "sklearn/naive_bayes.py:CategoricalNB", "sklearn/naive_bayes.py:CategoricalNB._count", "sklearn/metrics/pairwise.py:_return_float_dtype", "sklearn/ensemble/_hist_gradient_boosting/loss.py:BaseLoss", "benchmarks/bench_plot_parallel_pairwise.py:rbf_kernels", "sklearn/utils/fixes.py:_object_dtype_isnan", "sklearn/linear_model/_ridge.py:_solve_sparse_cg._get_rescaled_operator", "sklearn/tree/_reingold_tilford.py:DrawTree", "examples/cluster/plot_agglomerative_clustering_metrics.py:sqr", "sklearn/utils/_estimator_html_repr.py:_VisualBlock", "examples/cluster/plot_inductive_clustering.py:InductiveClusterer", "sklearn/utils/_show_versions.py:_get_deps_info", "examples/cluster/plot_adjusted_for_chance_measures.py:ami_score"]], "found_entities": [["sklearn/decomposition/_kernel_pca.py:KernelPCA.fit_transform", "sklearn/ensemble/_voting.py:VotingClassifier._collect_probas", "sklearn/utils/_mocking.py:CheckingClassifier._more_tags", "sklearn/linear_model/_stochastic_gradient.py:BaseSGD.average_coef_", "sklearn/utils/_mocking.py:NoSampleWeightWrapper.fit", "sklearn/decomposition/_kernel_pca.py:KernelPCA._fit_transform", "asv_benchmarks/benchmarks/common.py:Predictor.time_predict", "asv_benchmarks/benchmarks/linear_model.py:ElasticNetBenchmark.make_scorers", "sklearn/compose/_column_transformer.py:ColumnTransformer._transformers", "asv_benchmarks/benchmarks/common.py:Predictor.params", "sklearn/multioutput.py:_MultiOutputEstimator._more_tags", "sklearn/ensemble/_base.py:_BaseHeterogeneousEnsemble.named_estimators", "doc/tutorial/machine_learning_map/pyparsing.py:ParseBaseException.__repr__", "sklearn/isotonic.py:IsotonicRegression._check_input_data_shape", "asv_benchmarks/benchmarks/common.py:Transformer.params", "doc/tutorial/machine_learning_map/pyparsing.py:ParseBaseException.__dir__", "asv_benchmarks/benchmarks/linear_model.py:LassoBenchmark.make_estimator", "sklearn/compose/_target.py:TransformedTargetRegressor._more_tags", "asv_benchmarks/benchmarks/linear_model.py:LassoBenchmark.make_scorers", "examples/release_highlights/plot_release_highlights_0_22_0.py:test_sklearn_compatible_estimator", "sklearn/linear_model/_base.py:LinearModel._more_tags", "sklearn/decomposition/_fastica.py:_cube", "examples/neighbors/approximate_nearest_neighbors.py:AnnoyTransformer.transform", "examples/neighbors/approximate_nearest_neighbors.py:AnnoyTransformer.fit_transform", "sklearn/feature_selection/_rfe.py:RFE.classes_", "asv_benchmarks/benchmarks/common.py:Estimator.time_fit", "asv_benchmarks/benchmarks/linear_model.py:SGDRegressorBenchmark.make_scorers", "asv_benchmarks/benchmarks/common.py:Estimator.peakmem_fit", "sklearn/preprocessing/_label.py:LabelEncoder._more_tags", "sklearn/utils/validation.py:_check_psd_eigenvalues", "doc/tutorial/machine_learning_map/pyparsing.py:ParserElement.setDefaultWhitespaceChars", "sklearn/decomposition/_dict_learning.py:SparseCoder.n_features_in_", "sklearn/linear_model/_ridge.py:_RidgeGCV._decomp_diag", "sklearn/datasets/_base.py:_pkl_filepath", "sklearn/feature_extraction/text.py:CountVectorizer.fit", "sklearn/metrics/_ranking.py:_check_dcg_target_type", "sklearn/naive_bayes.py:CategoricalNB._more_tags", "sklearn/naive_bayes.py:CategoricalNB._check_X", "sklearn/naive_bayes.py:CategoricalNB._check_X_y", "sklearn/naive_bayes.py:CategoricalNB._init_counters", "sklearn/naive_bayes.py:CategoricalNB._count._update_cat_count", "sklearn/naive_bayes.py:CategoricalNB._count", "sklearn/naive_bayes.py:CategoricalNB._update_feature_log_prob", "sklearn/metrics/pairwise.py:_return_float_dtype", "sklearn/ensemble/_hist_gradient_boosting/loss.py:BaseLoss.pointwise_loss", "benchmarks/bench_plot_parallel_pairwise.py:rbf_kernels", "sklearn/utils/fixes.py:_object_dtype_isnan", "sklearn/linear_model/_ridge.py:_solve_sparse_cg._get_rescaled_operator.matvec", "sklearn/tree/_reingold_tilford.py:DrawTree.__str__", "examples/cluster/plot_agglomerative_clustering_metrics.py:sqr", "sklearn/utils/_estimator_html_repr.py:_VisualBlock._sk_visual_block_", "examples/cluster/plot_inductive_clustering.py:InductiveClusterer.decision_function", "sklearn/utils/_show_versions.py:_get_deps_info.get_version", "examples/cluster/plot_adjusted_for_chance_measures.py:ami_score"]], "raw_output_loc": ["```\nsklearn/decomposition/_kernel_pca.py\nclass: KernelPCA\nfunction: KernelPCA.fit_transform\nline: 294-315\n\nsklearn/decomposition/_kernel_pca.py\nclass: KernelPCA\nfunction: KernelPCA._fit_transform\nline: 184-254\n\nsklearn/decomposition/_kernel_pca.py\nclass: KernelPCA\nfunction: KernelPCA.__init__\nline: 145-168\n\nsklearn/utils/validation.py\nfunction: _check_psd_eigenvalues\nline: 1093-1253\n\nsklearn/metrics/pairwise.py\nfunction: _return_float_dtype\nline: 39-60\n```\n"], "meta_data": {"repo": "scikit-learn/scikit-learn", "base_commit": "e217b68fd00bb7c54b81a492ee6f9db6498517fa", "problem_statement": "Something goes wrong with KernelPCA with 32 bits input data\n\nWhen given 32 bits input, KernelPCA succeed to transform the data into a 17-dimensional feature space while the original space was 3 features. I did not debug yet but this seems really unlikely.\r\n\r\n```python\r\n# %%\r\nfrom sklearn.datasets import make_blobs\r\nfrom sklearn.preprocessing import StandardScaler\r\n\r\nX, y = make_blobs(\r\n    n_samples=30,\r\n    centers=[[0, 0, 0], [1, 1, 1]],\r\n    random_state=0,\r\n    cluster_std=0.1\r\n)\r\nX = StandardScaler().fit_transform(X)\r\nX -= X.min()\r\n\r\n# %%\r\nimport numpy as np\r\nfrom sklearn.decomposition import KernelPCA\r\n\r\nkpca = KernelPCA()\r\nprint(kpca.fit_transform(X).shape)\r\nprint(kpca.fit_transform(X.astype(np.float32)).shape)\r\n```", "patch": ""}}
{"instance_id": "localstack__localstack-544", "found_files": [["localstack/services/s3/s3_listener.py", "localstack/services/s3/__init__.py", "localstack/services/s3/s3_starter.py", "localstack/utils/persistence.py", "localstack/services/generic_proxy.py"]], "found_modules": [["localstack/utils/persistence.py:record", "localstack/services/generic_proxy.py:GenericProxyHandler"]], "found_entities": [["localstack/utils/persistence.py:record", "localstack/services/generic_proxy.py:GenericProxyHandler.do_HEAD"]], "raw_output_loc": ["```\nlocalstack/services/s3/s3_listener.py\nfunction: (Potentially) handle_s3_put or PUT handler logic\nfunction: (Potentially) handle_s3_head or HEAD handler logic\n\nlocalstack/services/s3/__init__.py\nfunction: (Re-exports or S3 entrypoint functions, for mapping handlers)\n\nlocalstack/services/s3/s3_starter.py\nfunction: (Entry-point and server bootstrap for S3 service events)\n\nlocalstack/utils/persistence.py\nfunction: record\n\nlocalstack/services/generic_proxy.py\nfunction: GenericProxyHandler.do_HEAD\n```\n"], "meta_data": {"repo": "localstack/localstack", "base_commit": "6c8f52d42563c1207a8cb3fbbfccb6d4af2a0670", "problem_statement": "S3 object metadata not saved when uploaded with presigned url\n\nUse case:\r\nI'm enabling users to directly upload to s3 using presigned url. S3 is configured to add event to SQS on Put. Queue consumer, reads the queue and makes HEAD requests with object keys to get the metadata and save information to database (generic image upload, so I know where to add file).\r\n\r\nTest script in node js - some ugly code here (to install deps run `npm install aws-sdk request`):\r\n```js\r\nconst AWS = require(\"aws-sdk\");\r\nconst request = require(\"request\");\r\n\r\nlet s3 = new AWS.S3({\r\n    endpoint: \"http://localhost:4572\",\r\n    s3ForcePathStyle: true,\r\n    accessKeyId: \"\",\r\n    secretAccessKey: \"\",\r\n    region: \"us-west-1\"\r\n});\r\n\r\nvar bucket = \"bucketest\";\r\nvar key = \"test.txt\";\r\n\r\ns3.createBucket({Bucket: bucket}, function (err, data) {\r\n    if (err) {\r\n        console.error(err.message);\r\n        // ignore, probably there is bucket already\r\n    }\r\n\r\n    var params = {\r\n        Bucket: bucket,\r\n        Key: key,\r\n        Metadata: {\r\n            venue: \"123\"\r\n        }\r\n    };\r\n\r\n    s3.getSignedUrl('putObject', params, function (err, url) {\r\n        if (err) {\r\n            console.error('Presigning post data encountered an error', err);\r\n        } else {\r\n            console.log('==== URL: ', url);\r\n\r\n            var body = new Buffer('Test data.');\r\n            request.put({ url, body, method: \"PUT\" }, function(err, resp, body) {\r\n                if (err) {\r\n                    console.log('======= error:', error); \r\n                    return;\r\n                }\r\n\r\n                console.log(body);\r\n\r\n                s3.headObject({Bucket: bucket, Key: key}, function (err, data) {\r\n                    if (err) console.log(\"====== error1:\", err, err.stack); \r\n                    else console.log(\"==== HEAD RESPONSE\", data); \r\n                });\r\n            })\r\n        }\r\n    });\r\n});\r\n```\r\n\r\nOutput:\r\n```\r\n==== URL:  http://localhost:4572/heaps-test/test.txt?AWSAccessKeyId=somekey&Expires=1515503310&Signature=TgK3B33p2kwCWs5F5KtaZ3fxgXA%3D&x-amz-meta-venue=123\r\n<PutObjectResponse xmlns=\"http://s3.amazonaws.com/doc/2006-03-01\"><PutObjectResponse><ETag>&#34;56dd8a439abf97fda051f88f09f00d65&#34;</ETag><LastModified>2018-01-09T12:53:30.637Z</LastModified></PutObjectResponse></PutObjectResponse>\r\n==== HEAD RESPONSE { LastModified: 2018-01-09T12:53:30.000Z,\r\n  ContentLength: 10,\r\n  ETag: '\"56dd8a439abf97fda051f88f09f00d65\"',\r\n  ContentType: 'text/html; charset=utf-8',\r\n  Metadata: {} }\r\n\r\n```\r\n\r\nExpected Output (tested with live AWS): \r\n```\r\n==== URL:  https://heaps-test.s3.eu-west-1.amazonaws.com/test.txt?AWSAccessKeyId=somekey&Expires=1515503234&Signature=enc17C6glTsVtOiGobugz5NELIc%3D&x-amz-meta-venue=123\r\n\r\n==== HEAD RESPONSE { AcceptRanges: 'bytes',\r\n  LastModified: 2018-01-09T12:52:15.000Z,\r\n  ContentLength: 10,\r\n  ETag: '\"56dd8a439abf97fda051f88f09f00d65\"',\r\n  ContentType: 'binary/octet-stream',\r\n  Metadata: { venue: '123' } }\r\n```\r\n\r\nAs you can see Metadata is empty when using localstack", "patch": ""}}
