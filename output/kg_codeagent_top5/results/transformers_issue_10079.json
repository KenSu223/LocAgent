{
  "issue_url": "https://github.com/huggingface/transformers/issues/10079",
  "repo_name": "transformers",
  "title": "Unclear error \"NotImplementedError:  \"while saving tokenizer. How fix it?",
  "base_commit": "322037e842e5e89080918c824998c17722df6f19",
  "ground_truth_files": [
    "src/transformers/tokenization_utils_fast.py"
  ],
  "predicted_files": [
    "src/transformers/tokenization_utils_base.py",
    "src/transformers/models/bert/tokenization_bert_fast.py",
    "src/transformers/tokenization_utils_fast.py"
  ],
  "success": true,
  "raw_output": "{'files_to_modify': ['src/transformers/tokenization_utils_fast.py', 'src/transformers/tokenization_utils_base.py', 'docs/source/tokenizer_summary.rst', 'docs/source/serialization.rst', 'src/transformers/models/bert/tokenization_bert_fast.py'], 'reasoning': '1. src/transformers/tokenization_utils_fast.py: PreTrainedTokenizerFast should implement save_vocabulary to enable saving for user-trained/custom fast tokenizers and avoid NotImplementedError.\\n2. src/transformers/tokenization_utils_base.py: This file raises the error, and its docstrings should clarify requirements for tokenizers to implement save_vocabulary; may need minor tweaks for compatibility/documentation.\\n3. docs/source/tokenizer_summary.rst: Documentation should explain that user-trained fast tokenizers require save_vocabulary implementation, providing example usage.\\n4. docs/source/serialization.rst: Serialization docs should be updated to warn users and guide proper saving of custom tokenizers.\\n5. src/transformers/models/bert/tokenization_bert_fast.py: For completeness, check/extend this file if supporting BertTokenizerFast specifically for custom vocab save (if deployed in user workflows). Most model-specific tokenizers already override this, but may require clarity or pattern fixes.'}",
  "error": null
}