{
  "issue_url": "https://github.com/huggingface/transformers/issues/28435",
  "repo_name": "transformers",
  "title": "Skip some weights for load_in_8bit and keep them as fp16/32?",
  "base_commit": "cef2e40e0f8eaad13b8d32817a48fdddc32eb2a5",
  "ground_truth_files": [
    "src/transformers/modeling_utils.py",
    "src/transformers/utils/quantization_config.py"
  ],
  "predicted_files": [
    "src/transformers/integrations/bitsandbytes.py"
  ],
  "success": true,
  "raw_output": "{'files_to_modify': ['src/transformers/modeling_utils.py', 'src/transformers/utils/quantization_config.py', 'src/transformers/integrations/bitsandbytes.py', 'src/transformers/models/blip_2/modeling_blip_2.py', 'src/transformers/models/clip/modeling_clip.py'], 'reasoning': \"1. src/transformers/modeling_utils.py: The main logic for loading models, controlling dtype/quantization (load_in_8bit, load_in_4bit), and mixing FP16/FP32 weights is here. Modifications will allow skipping certain modules from quantization.\\n2. src/transformers/utils/quantization_config.py: Handles quantization options and arguments, including parsing and passing module/dtype info for custom quantization.\\n3. src/transformers/integrations/bitsandbytes.py: Integration point for bitsandbytes; modification is needed to support partial/mixed quantization when 'skip' modules or lists are provided.\\n4. src/transformers/models/blip_2/modeling_blip_2.py: Vision-language models like BLIP2 require adjusting their loading logic to keep image encoders in FP16/FP32, while LLM parts are quantized.\\n5. src/transformers/models/clip/modeling_clip.py: The CLIP/image encoder module must avoid quantization; adding support here ensures correct dtype usage and avoids performance loss.\"}",
  "error": null
}